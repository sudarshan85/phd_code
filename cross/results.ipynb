{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Models using only MIMIC Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:51:49.343081Z",
     "start_time": "2019-12-01T16:51:47.573573Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from utils.metrics import BinaryAvgMetrics\n",
    "from utils.plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:51:49.374125Z",
     "start_time": "2019-12-01T16:51:49.345849Z"
    }
   },
   "outputs": [],
   "source": [
    "from lr.args import args as lr_args\n",
    "from rf.args import args as rf_args\n",
    "from gbm.args import args as gbm_args\n",
    "\n",
    "\n",
    "transfer_thresholds = {\n",
    "  'mimic_mlh': {\n",
    "    'lr': lr_args.mimic_src_thresh,\n",
    "    'rf': rf_args.mimic_src_thresh,\n",
    "    'gbm': gbm_args.mimic_src_thresh,    \n",
    "  },\n",
    "  'mlh_mimic': {\n",
    "    'lr': lr_args.mlh_src_thresh,\n",
    "    'rf': rf_args.mlh_src_thresh,\n",
    "    'gbm': gbm_args.mlh_src_thresh,    \n",
    "  },\n",
    "}\n",
    "\n",
    "test_thresholds = {\n",
    "  'lr': lr_args.mlh_src_test_thresh,\n",
    "  'rf': rf_args.mlh_src_test_thresh,\n",
    "  'gbm': gbm_args.mlh_src_test_thresh,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:51:49.397564Z",
     "start_time": "2019-12-01T16:51:49.375998Z"
    }
   },
   "outputs": [],
   "source": [
    "path = Path('data')\n",
    "workdir = path/f'workdir'\n",
    "figdir = workdir/'figdir'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:51:49.423411Z",
     "start_time": "2019-12-01T16:51:49.398992Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_ensemble(ensembles, thresh, bams):  \n",
    "  outputs = {}\n",
    "  for ens_model in ensembles:\n",
    "    key = '-'.join(ens_model)\n",
    "    targs = bams[ens_model[0]].targs\n",
    "    avg_thresh = np.array([thresh[model] for model in ens_model]).mean()\n",
    "    max_thresh = max([thresh[model] for model in ens_model])\n",
    "    probs = []\n",
    "    for i in range(len(targs)):\n",
    "      prob = []\n",
    "      for model in ens_model:\n",
    "        prob.append(bams[model].pos_probs[i])\n",
    "      probs.append(np.stack(prob))\n",
    "\n",
    "    avg_probs = [probs.mean(axis=0) for probs in probs]\n",
    "    max_probs = [probs.max(axis=0) for probs in probs]\n",
    "\n",
    "    avg_preds = [(probs > avg_thresh).astype(np.int64) for probs in avg_probs]\n",
    "    max_preds = [(probs > max_thresh).astype(np.int64) for probs in max_probs]\n",
    "    outputs[f'avg-{key}'] = (targs, avg_preds, avg_probs, avg_thresh)\n",
    "    outputs[f'max-{key}'] = (targs, max_preds, max_probs, max_thresh)\n",
    "    \n",
    "  return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:51:49.446204Z",
     "start_time": "2019-12-01T16:51:49.424637Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_ttest(bams, model1, model2, metric):  \n",
    "  if metric == 'sensitivity':\n",
    "    x1 = bams[model1].sensitivities()\n",
    "    x2 = bams[model2].sensitivities()\n",
    "  elif metric == 'specificity':\n",
    "    x1 = bams[model1].specificities()\n",
    "    x2 = bams[model2].specificities()\n",
    "  elif metric == 'ppv':\n",
    "    x1 = bams[model1].ppvs()\n",
    "    x2 = bams[model2].ppvs()\n",
    "  elif metric == 'auroc':\n",
    "    x1 = bams[model1].aurocs()\n",
    "    x2 = bams[model2].aurocs()\n",
    "  elif metric == 'npv':\n",
    "    x1 = bams[model1].npvs()\n",
    "    x2 = bams[model2].npvs()\n",
    "  elif metric == 'f1':    \n",
    "    x1 = bams[model1].f1s()\n",
    "    x2 = bams[model2].f1s()\n",
    "\n",
    "  t, p = stats.ttest_ind(x1, x2)\n",
    "  return np.round(t, 2), max(np.round(p, 2), 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:46:47.244957Z",
     "start_time": "2019-12-01T16:46:45.585504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((116400, 60000), (116400,), (38112, 60000), (38112,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(workdir/f'vectordir/mlh2mimic.pkl', 'rb') as f:\n",
    "  mlh2mimic_vec = pickle.load(f)\n",
    "  x_train_mlh = pickle.load(f)\n",
    "  x_test_mimic = pickle.load(f)\n",
    "  y_train_mlh = pickle.load(f)\n",
    "  y_test_mimic = pickle.load(f)\n",
    "  \n",
    "x_train_mlh.shape, y_train_mlh.shape, x_test_mimic.shape, y_test_mimic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:47:12.918427Z",
     "start_time": "2019-12-01T16:47:12.689884Z"
    }
   },
   "outputs": [],
   "source": [
    "model = 'gbm'\n",
    "clf = pickle.load(open(workdir/model/'models/mlh_full.pkl', 'rb'))\n",
    "\n",
    "prob = clf.predict_proba(x_test_mimic)\n",
    "pos_prob = prob[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:47:13.671343Z",
     "start_time": "2019-12-01T16:47:13.537777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sensitivity</th>\n",
       "      <td>0.673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>specificity</th>\n",
       "      <td>0.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppv</th>\n",
       "      <td>0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npv</th>\n",
       "      <td>0.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auroc</th>\n",
       "      <td>0.631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <td>0.460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Value\n",
       "sensitivity  0.673\n",
       "specificity  0.521\n",
       "ppv          0.316\n",
       "npv          0.829\n",
       "f1           0.430\n",
       "auroc        0.631\n",
       "threshold    0.460"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = thresholds[model]\n",
    "pred = (pos_prob > threshold).astype(np.int64)\n",
    "cm = confusion_matrix(y_test_mimic, pred)\n",
    "tn,fp,fn,tp = cm[0][0],cm[0][1],cm[1][0],cm[1][1]\n",
    "sensitivity = tp/(tp+fn)\n",
    "specificity = tn/(tn+fp)\n",
    "ppv = tp/(tp+fp)\n",
    "npv = tn/(tn+fn)\n",
    "f1 = (2*ppv*sensitivity)/(ppv+sensitivity)\n",
    "auroc = roc_auc_score(y_test_mimic, pos_prob)\n",
    "\n",
    "d = {\n",
    "  'sensitivity': np.round(sensitivity, 3),\n",
    "  'specificity': np.round(specificity, 3),\n",
    "  'ppv': np.round(ppv, 3),\n",
    "  'npv': np.round(npv, 3),\n",
    "  'f1': np.round(f1, 3),\n",
    "  'auroc': np.round(auroc, 3),\n",
    "  'threshold': threshold,\n",
    "}\n",
    "metrics = pd.DataFrame(d.values(), index=d.keys(), columns=['Value'])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:47:14.507748Z",
     "start_time": "2019-12-01T16:47:14.440921Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(workdir/model/'mlh_mimic_test_preds.pkl', 'wb') as f:\n",
    "  pickle.dump(y_test_mimic, f)\n",
    "  pickle.dump(prob, f)\n",
    "  pickle.dump(pred, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Average Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:51:54.643750Z",
     "start_time": "2019-12-01T16:51:54.464818Z"
    }
   },
   "outputs": [],
   "source": [
    "models = ['lr', 'rf', 'gbm']\n",
    "bams = {}\n",
    "\n",
    "for model in models:\n",
    "  with open(workdir/model/f'mlh_mimic_test_preds.pkl', 'rb') as f:\n",
    "    targs = pickle.load(f)\n",
    "    probs = pickle.load(f)\n",
    "    preds = pickle.load(f)\n",
    "  bams[model] = BinaryAvgMetrics([targs], [preds], [probs[:, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:51:57.012873Z",
     "start_time": "2019-12-01T16:51:56.965774Z"
    }
   },
   "outputs": [],
   "source": [
    "# ens_models = [\n",
    "#   ['lr', 'rf'],\n",
    "#   ['lr', 'gbm'],\n",
    "#   ['rf', 'gbm'],  \n",
    "#   ['lr', 'rf', 'gbm'],\n",
    "# ]\n",
    "\n",
    "ens_models = [m for m in sum([list(map(list, combinations(models, i))) for i in range(len(models) + 1)], []) if len(m) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:52:16.921443Z",
     "start_time": "2019-12-01T16:52:16.389611Z"
    }
   },
   "outputs": [],
   "source": [
    "ensembles = get_ensemble(ens_models, test_thresholds, bams)\n",
    "\n",
    "for model, vals in ensembles.items():\n",
    "  bams[model] = BinaryAvgMetrics(*vals[:-1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:52:31.576786Z",
     "start_time": "2019-12-01T16:52:30.789298Z"
    }
   },
   "outputs": [],
   "source": [
    "final_metrics = {}\n",
    "\n",
    "for key in bams.keys():\n",
    "  final_metrics[key] = []\n",
    "  for i in range(len(bams[key].get_avg_metrics())):\n",
    "    final_metrics[key].append(bams[key].get_avg_metrics().iloc[i]['Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:52:39.344409Z",
     "start_time": "2019-12-01T16:52:39.282843Z"
    }
   },
   "outputs": [],
   "source": [
    "final_metrics = pd.DataFrame(final_metrics, index=['sensitivity', 'specificity', 'ppv', 'auroc', 'npv', 'f1']).transpose()\n",
    "\n",
    "best_models = pd.DataFrame([(final_metrics[metric].idxmax(), final_metrics[metric].max()) for metric in final_metrics], columns=['model', 'value'], index=['sensitivity', 'specificity', 'ppv', 'auroc', 'npv', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:52:42.768174Z",
     "start_time": "2019-12-01T16:52:42.707415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>ppv</th>\n",
       "      <th>auroc</th>\n",
       "      <th>npv</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>0.543</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>0.665</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbm</th>\n",
       "      <td>0.673</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg-lr-rf</th>\n",
       "      <td>0.571</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max-lr-rf</th>\n",
       "      <td>0.555</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg-lr-gbm</th>\n",
       "      <td>0.604</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max-lr-gbm</th>\n",
       "      <td>0.689</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg-rf-gbm</th>\n",
       "      <td>0.682</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max-rf-gbm</th>\n",
       "      <td>0.673</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg-lr-rf-gbm</th>\n",
       "      <td>0.622</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max-lr-rf-gbm</th>\n",
       "      <td>0.689</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               sensitivity  specificity    ppv  auroc    npv     f1\n",
       "lr             0.543        0.615        0.316  0.610  0.804  0.400\n",
       "rf             0.665        0.465        0.290  0.581  0.809  0.404\n",
       "gbm            0.673        0.521        0.316  0.631  0.829  0.430\n",
       "avg-lr-rf      0.571        0.587        0.312  0.614  0.806  0.404\n",
       "max-lr-rf      0.555        0.597        0.311  0.606  0.803  0.399\n",
       "avg-lr-gbm     0.604        0.591        0.326  0.634  0.820  0.424\n",
       "max-lr-gbm     0.689        0.505        0.314  0.632  0.832  0.431\n",
       "avg-rf-gbm     0.682        0.509        0.313  0.625  0.830  0.429\n",
       "max-rf-gbm     0.673        0.521        0.316  0.631  0.829  0.430\n",
       "avg-lr-rf-gbm  0.622        0.569        0.321  0.632  0.821  0.424\n",
       "max-lr-rf-gbm  0.689        0.505        0.314  0.632  0.832  0.431"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:52:55.053155Z",
     "start_time": "2019-12-01T16:52:54.999057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sensitivity</th>\n",
       "      <td>max-lr-gbm</td>\n",
       "      <td>0.689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>specificity</th>\n",
       "      <td>lr</td>\n",
       "      <td>0.615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppv</th>\n",
       "      <td>avg-lr-gbm</td>\n",
       "      <td>0.326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auroc</th>\n",
       "      <td>avg-lr-gbm</td>\n",
       "      <td>0.634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npv</th>\n",
       "      <td>max-lr-gbm</td>\n",
       "      <td>0.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>max-lr-gbm</td>\n",
       "      <td>0.431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  value\n",
       "sensitivity  max-lr-gbm  0.689\n",
       "specificity  lr          0.615\n",
       "ppv          avg-lr-gbm  0.326\n",
       "auroc        avg-lr-gbm  0.634\n",
       "npv          max-lr-gbm  0.832\n",
       "f1           max-lr-gbm  0.431"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T17:27:18.804066Z",
     "start_time": "2019-12-01T17:27:18.591939Z"
    }
   },
   "outputs": [],
   "source": [
    "cte = [61, 58.1, 63.1, 61.4, 61.4, 60.6, 63.2, 62.5, 63.1, 63.2, 63.2]\n",
    "ctr = [74.1, 73.7, 73.2, 74.4, 74.2, 74.4, 74.1, 74.2, 73.7, 74.6, 74.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T17:27:18.827252Z",
     "start_time": "2019-12-01T17:27:18.805748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21.48, 26.85, 16.01, 21.17, 20.85, 22.77, 17.25, 18.72, 16.8, 18.04, 17.41]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.round(100 * (b - a) / a, 2) for a, b in zip(cte, ctr)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Cross Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Average Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T16:11:42.401061Z",
     "start_time": "2019-11-25T16:11:35.355215Z"
    }
   },
   "outputs": [],
   "source": [
    "transfer = 'mlh_mimic'\n",
    "thresholds = transfer_thresholds[transfer]\n",
    "models = ['lr', 'rf', 'gbm']\n",
    "bams = {}\n",
    "\n",
    "for model in models:\n",
    "  with open(workdir/model/f'{transfer}_preds.pkl', 'rb') as f:\n",
    "    targs = pickle.load(f)\n",
    "    probs = pickle.load(f)\n",
    "    preds = pickle.load(f)\n",
    "  bams[model] = BinaryAvgMetrics(targs, preds, [prob[:, 1] for prob in probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T16:11:42.422240Z",
     "start_time": "2019-11-25T16:11:42.402659Z"
    }
   },
   "outputs": [],
   "source": [
    "# ens_models = [\n",
    "#   ['lr', 'rf'],\n",
    "#   ['lr', 'gbm'],\n",
    "#   ['rf', 'gbm'],  \n",
    "#   ['lr', 'rf', 'gbm'],\n",
    "# ]\n",
    "\n",
    "ens_models = [m for m in sum([list(map(list, combinations(models, i))) for i in range(len(models) + 1)], []) if len(m) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T16:12:05.045886Z",
     "start_time": "2019-11-25T16:11:48.464766Z"
    }
   },
   "outputs": [],
   "source": [
    "ensembles = get_ensemble(ens_models, thresholds, bams)\n",
    "\n",
    "for model, vals in ensembles.items():\n",
    "  bams[model] = BinaryAvgMetrics(*vals[:-1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T16:13:00.284082Z",
     "start_time": "2019-11-25T16:12:05.047933Z"
    }
   },
   "outputs": [],
   "source": [
    "final_metrics = {}\n",
    "\n",
    "for key in bams.keys():\n",
    "  final_metrics[key] = []\n",
    "  for i in range(len(bams[key].get_avg_metrics())):\n",
    "    final_metrics[key].append(bams[key].get_avg_metrics().iloc[i]['Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T16:13:00.509521Z",
     "start_time": "2019-11-25T16:13:00.286303Z"
    }
   },
   "outputs": [],
   "source": [
    "final_metrics = pd.DataFrame(final_metrics, index=['sensitivity', 'specificity', 'ppv', 'auroc', 'npv', 'f1']).transpose()\n",
    "\n",
    "best_models = pd.DataFrame([(final_metrics[metric].idxmax(), final_metrics[metric].max()) for metric in final_metrics], columns=['model', 'value'], index=['sensitivity', 'specificity', 'ppv', 'auroc', 'npv', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Student-t Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T16:13:00.529713Z",
     "start_time": "2019-11-25T16:13:00.510876Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "models = list(final_metrics.index)\n",
    "metrics = list(final_metrics.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T16:14:27.330381Z",
     "start_time": "2019-11-25T16:13:00.530745Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ttests = {}\n",
    "\n",
    "for m1, m2 in combinations(models, 2):  \n",
    "  ttests[f'{m1}:{m2}'] = {}\n",
    "  for metric in metrics:\n",
    "    ttests[f'{m1}:{m2}'][metric] = do_ttest(bams, m1, m2, metric)\n",
    "\n",
    "ttests = pd.DataFrame(ttests).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T16:14:38.490750Z",
     "start_time": "2019-11-25T16:14:27.332377Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(bams, open(workdir/f'{transfer}_bams.pkl', 'wb'))\n",
    "final_metrics.to_csv(workdir/f'{transfer}_final_metrics.csv', float_format='%.3f')\n",
    "best_models.to_csv(workdir/f'{transfer}_best_models.csv', float_format='%.3f')\n",
    "ttests.to_csv(workdir/f'{transfer}_ttests.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:35:14.229376Z",
     "start_time": "2019-12-01T16:35:14.133610Z"
    }
   },
   "outputs": [],
   "source": [
    "bams = BinaryAvgMetrics([y_test_mimic], [pred], [pos_prob])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bams = {}\n",
    "\n",
    "for model in models:\n",
    "  with open(workdir/model/f'{transfer}_preds.pkl', 'rb') as f:\n",
    "    targs = pickle.load(f)\n",
    "    probs = pickle.load(f)\n",
    "    preds = pickle.load(f)\n",
    "  bams[model] = BinaryAvgMetrics(targs, preds, [prob[:, 1] for prob in probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T16:11:42.401061Z",
     "start_time": "2019-11-25T16:11:35.355215Z"
    }
   },
   "outputs": [],
   "source": [
    "thresholds = transfer_thresholds[transfer]\n",
    "models = ['lr', 'rf', 'gbm']\n",
    "bams = {}\n",
    "\n",
    "for model in models:\n",
    "  with open(workdir/model/f'{transfer}_preds.pkl', 'rb') as f:\n",
    "    targs = pickle.load(f)\n",
    "    probs = pickle.load(f)\n",
    "    preds = pickle.load(f)\n",
    "  bams[model] = BinaryAvgMetrics(targs, preds, [prob[:, 1] for prob in probs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T00:14:38.013717Z",
     "start_time": "2019-11-26T00:14:36.810643Z"
    }
   },
   "outputs": [],
   "source": [
    "transfer = 'mlh_mimic'\n",
    "bams = pickle.load(open(workdir/f'{transfer}_bams.pkl', 'rb'))\n",
    "final_metrics = pd.read_csv(workdir/f'{transfer}_final_metrics.csv', index_col=0)\n",
    "best_models = pd.read_csv(workdir/f'{transfer}_best_models.csv', index_col=0)\n",
    "ttests = pd.read_csv(workdir/f'{transfer}_ttests.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T00:14:40.807689Z",
     "start_time": "2019-11-26T00:14:40.759766Z"
    }
   },
   "outputs": [],
   "source": [
    "itr = iter(bams.keys())\n",
    "bams.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T00:28:10.331909Z",
     "start_time": "2019-11-26T00:28:09.272844Z"
    }
   },
   "outputs": [],
   "source": [
    "model = next(itr)\n",
    "print(model)\n",
    "bams[model].get_avg_metrics(conf=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T16:56:35.569503Z",
     "start_time": "2019-11-25T16:56:35.538122Z"
    }
   },
   "outputs": [],
   "source": [
    "final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T16:56:37.547629Z",
     "start_time": "2019-11-25T16:56:37.497420Z"
    }
   },
   "outputs": [],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T23:26:57.007415Z",
     "start_time": "2019-11-17T23:26:56.965555Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(ttests.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:08:24.402774Z",
     "start_time": "2019-11-25T17:08:24.372448Z"
    }
   },
   "outputs": [],
   "source": [
    "save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:11:16.872830Z",
     "start_time": "2019-11-25T17:11:15.759926Z"
    }
   },
   "outputs": [],
   "source": [
    "transfer = 'mlh_mimic'\n",
    "bams = pickle.load(open(workdir/f'{transfer}_bams.pkl', 'rb'))\n",
    "final_metrics = pd.read_csv(workdir/f'{transfer}_final_metrics.csv', index_col=0)\n",
    "best_models = pd.read_csv(workdir/f'{transfer}_best_models.csv', index_col=0)\n",
    "ttests = pd.read_csv(workdir/f'{transfer}_ttests.csv', index_col=0)\n",
    "\n",
    "for k in bams.keys():\n",
    "  bams[k.upper()] = bams.pop(k)\n",
    "\n",
    "bams['AVG-ALL'] = bams.pop('AVG-LR-RF-GBM')\n",
    "bams['MAX-ALL'] = bams.pop('MAX-LR-RF-GBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:11:26.780556Z",
     "start_time": "2019-11-25T17:11:16.874876Z"
    }
   },
   "outputs": [],
   "source": [
    "itr = iter(bams.keys())\n",
    "bams.keys()\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "for md in itr:\n",
    "  df = pd.DataFrame()\n",
    "  for k, m in bams[md].yield_metrics():\n",
    "    df[k] = m\n",
    "  df['model'] = md\n",
    "  cols = list(df.columns)\n",
    "  cols = [cols[-1]] + cols[:-1]\n",
    "  df = df[cols]\n",
    "  metrics[md] = df\n",
    "\n",
    "plot_df = pd.concat(metrics.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:11:51.403357Z",
     "start_time": "2019-11-25T17:11:50.893934Z"
    }
   },
   "outputs": [],
   "source": [
    "met = 'AUC'\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(15,8))\n",
    "sns.boxplot(x='model', y=met, data=plot_df, ax=ax)\n",
    "ax.set_xlabel('')\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{transfer}_{met.lower()}_box_plot.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:08:28.117328Z",
     "start_time": "2019-11-25T17:08:28.088209Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_mean_tprs(bams, base_fpr):\n",
    "  mean_tprs = {}  \n",
    "  for model, bam in bams.items():\n",
    "    tprs = []  \n",
    "    for i, (targs, probs) in enumerate(zip(bam.targs, bam.pos_probs)):\n",
    "      fpr, tpr, _ = roc_curve(targs, probs)\n",
    "      tpr = interp(base_fpr, fpr, tpr)\n",
    "      tpr[0] = 0.0\n",
    "      tprs.append(tpr)\n",
    "\n",
    "    tprs = np.array(tprs)\n",
    "    mean_tprs[model] = tprs.mean(axis=0)\n",
    "    \n",
    "  return mean_tprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:12:47.667467Z",
     "start_time": "2019-11-25T17:12:37.326640Z"
    }
   },
   "outputs": [],
   "source": [
    "des = 'all_'\n",
    "\n",
    "if not des:\n",
    "  plot_bams = {k: bams[k] for k in bams.keys() if '-' not in k}\n",
    "  des = ''  \n",
    "  names = plot_bams.keys()\n",
    "  aucs = [model.auroc_avg() for _, model in plot_bams.items()]\n",
    "  legends = [f'{model} ({auc})' for model, auc in zip(names, aucs)]\n",
    "elif des == 'avg_':\n",
    "  plot_bams = {k: bams[k] for k in bams.keys() if 'AVG' in k}\n",
    "  names = [name[4:] for name in plot_bams.keys()]\n",
    "  aucs = [model.auroc_avg() for _, model in plot_bams.items()]\n",
    "  legends = [f'{model} ({auc})' for model, auc in zip(names, aucs)]  \n",
    "elif des == 'max_':\n",
    "  plot_bams = {k: bams[k] for k in bams.keys() if 'MAX' in k}\n",
    "  names = [name[4:] for name in plot_bams.keys()]\n",
    "  aucs = [model.auroc_avg() for _, model in plot_bams.items()]\n",
    "  legends = [f'{model} ({auc})' for model, auc in zip(names, aucs)]  \n",
    "elif des == 'all_':\n",
    "  plot_bams = bams\n",
    "  names = plot_bams.keys()\n",
    "  aucs = [model.auroc_avg() for _, model in plot_bams.items()]\n",
    "  legends = [f'{model} ({auc})' for model, auc in zip(names, aucs)]\n",
    "  \n",
    "legends  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:12:54.625690Z",
     "start_time": "2019-11-25T17:12:47.669188Z"
    }
   },
   "outputs": [],
   "source": [
    "base_fpr = np.linspace(0, 1, 100)\n",
    "mean_tprs = get_mean_tprs(plot_bams, base_fpr)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(11, 8))\n",
    "for i, (model, mean_tpr) in enumerate(mean_tprs.items()):\n",
    "  ax.plot(base_fpr, mean_tpr)\n",
    "ax.plot([0, 1], [0, 1], linestyle=':')  \n",
    "ax.grid(b=True, which='major', color='#d3d3d3', linewidth=1.0)\n",
    "ax.grid(b=True, which='minor', color='#d3d3d3', linewidth=0.5)\n",
    "ax.set_ylabel('Sensitivity')\n",
    "ax.set_xlabel('1 - Specificity')\n",
    "ax.legend(legends)\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{transfer}_{des}mean_auc.pdf', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
