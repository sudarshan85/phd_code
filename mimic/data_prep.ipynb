{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC Notes and Structured Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:37:21.305438Z",
     "start_time": "2019-12-17T13:37:21.175268Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:37:22.128748Z",
     "start_time": "2019-12-17T13:37:21.308411Z"
    }
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from ast import literal_eval\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:37:22.148451Z",
     "start_time": "2019-12-17T13:37:22.130707Z"
    }
   },
   "outputs": [],
   "source": [
    "path = Path('data')\n",
    "workdir = path/'workdir'\n",
    "figdir = workdir/'figures'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:37:22.166423Z",
     "start_time": "2019-12-17T13:37:22.149941Z"
    }
   },
   "outputs": [],
   "source": [
    "def change_name(col_name):\n",
    "  if '(' not in col_name:\n",
    "    return col_name\n",
    "  cols = literal_eval(col_name)\n",
    "  return f'{cols[0]}_{cols[1]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:37:22.215920Z",
     "start_time": "2019-12-17T13:37:22.169025Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_interval(x):\n",
    "  if pd.isnull(x):\n",
    "    return -1\n",
    "  \n",
    "  if 0 < x <= 1:\n",
    "    return 0\n",
    "  elif 1 < x <= 2:\n",
    "    return 1\n",
    "  elif 2 < x <= 3:\n",
    "    return 2\n",
    "  elif 3 < x <= 4:\n",
    "    return 3\n",
    "  elif 4 < x <= 5:\n",
    "    return 4\n",
    "  elif 5 < x <= 6:\n",
    "    return 5\n",
    "  elif 6 < x <= 7:\n",
    "    return 6\n",
    "  elif 7 < x <= 8:\n",
    "    return 7\n",
    "  elif 8 < x <= 9:\n",
    "    return 8\n",
    "  elif 9 < x <= 10:\n",
    "    return 9\n",
    "  elif 10 < x <= 11:\n",
    "    return 10\n",
    "  elif 11 < x <= 12:\n",
    "    return 11\n",
    "  elif 12 < x <= 13:\n",
    "    return 12\n",
    "  elif 13 < x <= 14:\n",
    "    return 13\n",
    "  elif 14 < x <= 15:\n",
    "    return 14\n",
    "  else:\n",
    "    return 15\n",
    "\n",
    "def icu_adm_label(x):\n",
    "  if 0 <= x <= 1:\n",
    "    return -1 # unused notes due to data leakage\n",
    "  elif 1 < x <= 3:\n",
    "    return 1 # imminent ICU admission\n",
    "  elif 3 < x <= 5:\n",
    "    return -1 # unused notes due to data leakage\n",
    "  else:\n",
    "    return 0 # delayed ICU admission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:37:23.829667Z",
     "start_time": "2019-12-17T13:37:22.218034Z"
    }
   },
   "outputs": [],
   "source": [
    "notes_df = pd.read_csv(path/'unstructured.csv', parse_dates=['intime', 'admittime', 'charttime'])\n",
    "notes_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:37:25.563290Z",
     "start_time": "2019-12-17T13:37:23.830963Z"
    }
   },
   "outputs": [],
   "source": [
    "vitals_df = pd.read_csv(path/'structured.csv', parse_dates=['charttime'])\n",
    "vitals_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:37:39.731812Z",
     "start_time": "2019-12-17T13:37:39.660739Z"
    }
   },
   "outputs": [],
   "source": [
    "notes_hadms = notes_df['hadm_id'].unique()\n",
    "vitals_hadms = vitals_df['hadm_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:39:04.250573Z",
     "start_time": "2019-12-17T13:39:04.178392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of encounters that definitely have structured vitals data: 14150\n",
      "Number of encounters that definitely have clinical notes: 12932\n",
      "Number of encounters that have both vitals and clinical notes: 8254\n"
     ]
    }
   ],
   "source": [
    "# Extract common `hadm_id` and filter out those that do not appear in both dfs\n",
    "common_hadms = set(vitals_df['hadm_id'].unique()).intersection(notes_df['hadm_id'].unique())\n",
    "\n",
    "print(f\"Number of encounters that definitely have structured vitals data: {len(vitals_hadms)}\")\n",
    "print(f\"Number of encounters that definitely have clinical notes: {len(notes_hadms)}\")\n",
    "print(f\"Number of encounters that have both vitals and clinical notes: {len(common_hadms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:40:01.079067Z",
     "start_time": "2019-12-17T13:40:00.706520Z"
    }
   },
   "outputs": [],
   "source": [
    "notes_common = notes_df[notes_df['hadm_id'].isin(common_hadms)].reset_index(drop=True)\n",
    "vitals_common = vitals_df[vitals_df['hadm_id'].isin(common_hadms)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:41:05.467187Z",
     "start_time": "2019-12-17T13:41:05.242531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1463808, 10), (64457, 7))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "s, n = set(vitals_common['hadm_id'].unique()), set(notes_common['hadm_id'].unique())\n",
    "assert(s.symmetric_difference(n) == set())\n",
    "\n",
    "vitals_common.shape, notes_common.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:41:50.810555Z",
     "start_time": "2019-12-17T13:41:46.234879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53270, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes_common['note'] = notes_common['category'].str.cat(notes_common['description'], sep='\\n')\n",
    "notes_common['note'] = notes_common['note'].str.cat(notes_common['text'], sep='\\n')\n",
    "notes_common.drop(columns=['category', 'description', 'text'], inplace=True) \n",
    "\n",
    "notes_common = pd.DataFrame(notes_common.groupby(['hadm_id', 'intime', 'admittime', 'charttime'])['note'].apply('\\n'.join)).reset_index()\n",
    "notes_common['category'] = notes_common['note'].apply(lambda x: x.split('\\n')[0])\n",
    "notes_common.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:42:46.058792Z",
     "start_time": "2019-12-17T13:42:33.957311Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270288, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove redundant info by filling in each time column with the value of the var\n",
    "vitals_common = vitals_common.groupby(['hadm_id','charttime']).sum(min_count = 1).reset_index()\n",
    "# Groupby ffill \n",
    "vitals_common = vitals_common.groupby(['hadm_id'], as_index=False).apply(lambda group: group.ffill())\n",
    "# Groupby bfill \n",
    "vitals_common = vitals_common.groupby(['hadm_id'], as_index=False).apply(lambda group: group.bfill())\n",
    "vitals_common = vitals_common.fillna(vitals_common.median())\n",
    "vitals_common.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:43:30.057481Z",
     "start_time": "2019-12-17T13:43:25.232175Z"
    }
   },
   "outputs": [],
   "source": [
    "notes_common.to_csv(path/'unstructured_notes_proc.csv', index=False)\n",
    "vitals_common.to_csv(path/'structured_vitals_proc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Statistics Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:50:10.527843Z",
     "start_time": "2019-12-17T13:50:09.980073Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270288, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vitals_common = pd.read_csv(path/'structured_vitals_proc.csv', parse_dates=['charttime'])\n",
    "vitals_common.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:50:22.197284Z",
     "start_time": "2019-12-17T13:50:22.142687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>134899</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3128</th>\n",
       "      <td>137495</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5098</th>\n",
       "      <td>161246</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5976</th>\n",
       "      <td>171847</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7272</th>\n",
       "      <td>187987</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      hadm_id  size\n",
       "2899   134899    10\n",
       "3128   137495    11\n",
       "5098   161246    11\n",
       "5976   171847    11\n",
       "7272   187987    15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.DataFrame(vitals_common.groupby('hadm_id').size(), columns=['size']).reset_index()\n",
    "hadms = x.loc[(x['size'] >= 10) & (x['size'] <= 20)].sample(5)['hadm_id'].tolist()\n",
    "x.loc[x['hadm_id'].isin(hadms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:50:48.461282Z",
     "start_time": "2019-12-17T13:50:48.405558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 10)\n",
      "Index(['hadm_id', 'charttime', 'hr', 'sbp', 'dbp', 'map', 'resp', 'temp',\n",
      "       'spo2', 'glucose'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dev_subset = vitals_common.loc[(vitals_common['hadm_id'].isin(hadms))].reset_index(drop=True)\n",
    "print(dev_subset.shape)\n",
    "print(dev_subset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:50:56.174086Z",
     "start_time": "2019-12-17T13:50:56.126688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "var_cols = dev_subset.columns[2:]\n",
    "print(len(var_cols))\n",
    "running_stats = ['min', 'mean', 'median', 'std', 'max']\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:51:09.647985Z",
     "start_time": "2019-12-17T13:51:09.356150Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/vaosl01/opt/NFS/su0/anaconda3/envs/phd/lib/python3.7/site-packages/ipykernel_launcher.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88715604f881434abc8be82551678ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Encounters', max=5.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for hadm_id, group_df in tqdm(dev_subset.groupby('hadm_id'), desc='Encounters'):\n",
    "  df = group_df.copy()\n",
    "  var_df = df[var_cols].reset_index(drop=True) # save the original vals for later\n",
    "  \n",
    "  df.set_index('charttime', inplace=True) # set charttime as index for rolling 24h\n",
    "  stats_df = df[var_cols].rolling('24h').agg(running_stats)\n",
    "  \n",
    "  df = pd.DataFrame(stats_df.to_records()) # flatten the resulting dataframe\n",
    "  df.insert(loc=1, column='hadm_id', value=hadm_id)\n",
    "  \n",
    "  df.rename(columns=change_name, inplace=True) # rename columns\n",
    "  df = pd.concat([df, var_df], axis=1) # add the original vals back\n",
    "  \n",
    "  # reorder vars such that the columns are var, var_stat...\n",
    "  stats_cols = df.columns[2:]\n",
    "  all_cols = []\n",
    "  for var in var_cols:\n",
    "    all_cols.append(var)\n",
    "    for stat in stats_cols:\n",
    "      if f'{var}_' in stat:\n",
    "        all_cols.append(stat)\n",
    "        \n",
    "  order = list(df.columns[:2]) + all_cols\n",
    "  df = df[order]\n",
    "  dfs.append(df)\n",
    "\n",
    "dev_subset_stats = pd.concat(dfs)\n",
    "dev_subset_stats.reset_index(drop=True, inplace=True)\n",
    "dev_subset_stats['charttime'] = pd.to_datetime(dev_subset_stats['charttime'])\n",
    "\n",
    "std_cols = [col for col in dev_subset_stats.columns if 'std' in col]\n",
    "dev_subset_stats[std_cols] = dev_subset_stats[std_cols].fillna(0)\n",
    "\n",
    "dev_subset_stats = dev_subset_stats[['hadm_id', 'charttime'] + list(dev_subset_stats.columns[2:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:51:29.625240Z",
     "start_time": "2019-12-17T13:51:29.367630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['hadm_id', 'charttime', 'hr', 'hr_min', 'hr_mean', 'hr_median',\n",
       "       'hr_std', 'hr_max', 'sbp', 'sbp_min', 'sbp_mean', 'sbp_median',\n",
       "       'sbp_std', 'sbp_max', 'dbp', 'dbp_min', 'dbp_mean', 'dbp_median',\n",
       "       'dbp_std', 'dbp_max', 'map', 'map_min', 'map_mean', 'map_median',\n",
       "       'map_std', 'map_max', 'resp', 'resp_min', 'resp_mean', 'resp_median',\n",
       "       'resp_std', 'resp_max', 'temp', 'temp_min', 'temp_mean', 'temp_median',\n",
       "       'temp_std', 'temp_max', 'spo2', 'spo2_min', 'spo2_mean', 'spo2_median',\n",
       "       'spo2_std', 'spo2_max', 'glucose', 'glucose_min', 'glucose_mean',\n",
       "       'glucose_median', 'glucose_std', 'glucose_max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dev_subset_stats.shape)\n",
    "dev_subset_stats.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:17:38.310709Z",
     "start_time": "2019-11-19T02:17:38.082677Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = pd.DataFrame(str_common_all.groupby('hadm_id').size(), columns=['size']).reset_index()\n",
    "hadms = x.loc[(x['size'] >= 10) & (x['size'] <= 20)].sample(5)['hadm_id'].tolist()\n",
    "x.loc[x['hadm_id'].isin(hadms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:17:38.341987Z",
     "start_time": "2019-11-19T02:17:38.312762Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "str_subset = str_common_all.loc[(str_common_all['hadm_id'].isin(hadms))].reset_index(drop=True)\n",
    "print(str_subset.shape)\n",
    "str_subset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:17:38.365601Z",
     "start_time": "2019-11-19T02:17:38.343747Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var_cols = str_subset.columns[2:]\n",
    "print(len(var_cols))\n",
    "running_stats = ['min', 'mean', 'median', 'std', 'max']\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:17:38.947520Z",
     "start_time": "2019-11-19T02:17:38.367153Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for hadm_id, group_df in tqdm(str_subset.groupby('hadm_id'), desc='Encounters'):\n",
    "  df = group_df.copy()\n",
    "  var_df = df[var_cols].reset_index(drop=True) # save the original vals for later\n",
    "  \n",
    "  df.set_index('charttime', inplace=True) # set charttime as index for rolling 24h\n",
    "  stats_df = df[var_cols].rolling('24h').agg(running_stats)\n",
    "  \n",
    "  df = pd.DataFrame(stats_df.to_records()) # flatten the resulting dataframe\n",
    "  df.insert(loc=1, column='hadm_id', value=hadm_id)\n",
    "  \n",
    "  df.rename(columns=change_name, inplace=True) # rename columns\n",
    "  df = pd.concat([df, var_df], axis=1) # add the original vals back\n",
    "  \n",
    "  # reorder vars such that the columns are var, var_stat...\n",
    "  stats_cols = df.columns[2:]\n",
    "  all_cols = []\n",
    "  for var in var_cols:\n",
    "    all_cols.append(var)\n",
    "    for stat in stats_cols:\n",
    "      if f'{var}_' in stat:\n",
    "        all_cols.append(stat)\n",
    "        \n",
    "  order = list(df.columns[:2]) + all_cols\n",
    "  df = df[order]\n",
    "  dfs.append(df)\n",
    "\n",
    "str_subset_stats = pd.concat(dfs)\n",
    "str_subset_stats.reset_index(drop=True, inplace=True)\n",
    "str_subset_stats['charttime'] = pd.to_datetime(str_subset_stats['charttime'])\n",
    "\n",
    "std_cols = [col for col in str_subset_stats.columns if 'std' in col]\n",
    "str_subset_stats[std_cols] = str_subset_stats[std_cols].fillna(0)\n",
    "\n",
    "cols = ['hadm_id', 'charttime'] + list(str_subset_stats.columns[2:])\n",
    "str_subset_stats = str_subset_stats[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:17:38.966439Z",
     "start_time": "2019-11-19T02:17:38.948726Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(str_subset_stats.shape)\n",
    "str_subset_stats.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:47:35.683611Z",
     "start_time": "2019-11-19T02:47:33.015077Z"
    }
   },
   "outputs": [],
   "source": [
    "notes_common_vital = pd.read_csv(path/'notes_common_vital.csv', parse_dates=['intime', 'admittime', 'charttime'])\n",
    "notes_common_vital.drop(columns=['category'], inplace=True)\n",
    "\n",
    "str_common_vital_stats = pd.read_csv(path/'str_common_vital_stats.csv', parse_dates=['charttime'])\n",
    "str_vital_cols = str_common_vital_stats.columns[2:]\n",
    "pickle.dump(list(str_vital_cols), open(path/'str_vital_cols.pkl', 'wb'))\n",
    "\n",
    "print(str_common_vital_stats.shape, notes_common_vital.shape, str_common_vital_stats['hadm_id'].nunique(), notes_common_vital['hadm_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:47:42.084608Z",
     "start_time": "2019-11-19T02:47:35.687082Z"
    }
   },
   "outputs": [],
   "source": [
    "notes_common_all = pd.read_csv(path/'notes_common_all.csv', parse_dates=['intime', 'admittime', 'charttime'])\n",
    "notes_common_all.drop(columns=['category'], inplace=True)\n",
    "\n",
    "str_common_all_stats = pd.read_csv(path/'str_common_all_stats.csv', parse_dates=['charttime'])\n",
    "str_all_cols = str_common_all_stats.columns[2:]\n",
    "pickle.dump(list(str_all_cols), open(path/'str_all_cols.pkl', 'wb'))\n",
    "\n",
    "print(str_common_all_stats.shape, notes_common_all.shape, str_common_all_stats['hadm_id'].nunique(), notes_common_all['hadm_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Vital Merge Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:22:46.523423Z",
     "start_time": "2019-11-19T02:22:46.469257Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "main = ['hadm_id', 'charttime']\n",
    "sub1 = ['hr', 'hr_max', 'temp', 'temp_min', 'glucose', 'glucose_std', 'map', 'map_median']\n",
    "sub2 = ['admittime', 'intime', 'note']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:22:47.905405Z",
     "start_time": "2019-11-19T02:22:47.825052Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = pd.DataFrame(notes_common_vital.groupby('hadm_id').size(), columns=['size']).reset_index()\n",
    "hadms = x.loc[(x['size'] >= 2) & (x['size'] <= 15)].sample(5)['hadm_id'].tolist()\n",
    "\n",
    "str_subset_stats = str_common_vital_stats.loc[(str_common_vital_stats['hadm_id'].isin(hadms))][main + sub1].copy().reset_index(drop=True)\n",
    "\n",
    "notes_subset = notes_common_vital.loc[(notes_common_vital['hadm_id'].isin(hadms))][main + sub2].copy().reset_index(drop=True)\n",
    "str_subset_stats.shape, str_subset_stats['hadm_id'].nunique(), notes_subset.shape, notes_subset['hadm_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:22:56.945808Z",
     "start_time": "2019-11-19T02:22:56.728047Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(str_subset_stats.groupby('hadm_id').size(), columns=['size']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:22:58.472929Z",
     "start_time": "2019-11-19T02:22:58.420848Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(notes_subset.groupby('hadm_id').size(), columns=['size']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:24:07.902589Z",
     "start_time": "2019-11-19T02:24:07.676928Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([str_subset_stats, notes_subset], axis=0, sort=True)\n",
    "df.sort_values(by=['charttime'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "pd.DataFrame(df.groupby('hadm_id').size(), columns=['size']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:24:07.991927Z",
     "start_time": "2019-11-19T02:24:07.949412Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:25:10.662407Z",
     "start_time": "2019-11-19T02:25:10.442442Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i += 1\n",
    "print(hadms[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:25:10.711130Z",
     "start_time": "2019-11-19T02:25:10.665085Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "str_subset_stats[str_subset_stats['hadm_id'] == hadms[i]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:25:11.970741Z",
     "start_time": "2019-11-19T02:25:11.914754Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_subset[notes_subset['hadm_id'] == hadms[i]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:25:13.088994Z",
     "start_time": "2019-11-19T02:25:13.022349Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[df['hadm_id'] == hadms[i]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:24:34.606809Z",
     "start_time": "2019-11-19T02:24:34.536650Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols = ['hr', 'hr_max', 'temp', 'temp_min', 'glucose', 'glucose_std', 'map', 'map_median', 'admittime', 'intime']\n",
    "df[cols] = df.groupby('hadm_id')[cols].ffill()\n",
    "df[cols] = df[cols].fillna(df[cols].median())\n",
    "df['note'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### All Merge Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T18:11:54.912448Z",
     "start_time": "2019-11-18T18:11:54.713108Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "main = ['hadm_id', 'charttime']\n",
    "sub1 = ['fibrinogen', 'fibrinogen_min', 'wbc', 'wbc_mean', 'hemoglobin', 'hemoglobin_median', 'potassium', 'potassium_std', 'bilirubin_total', 'bilirubin_total_median']\n",
    "sub2 = ['admittime', 'intime', 'note']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T18:11:54.969685Z",
     "start_time": "2019-11-18T18:11:54.915188Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = pd.DataFrame(notes_common_all.groupby('hadm_id').size(), columns=['size']).reset_index()\n",
    "hadms = x.loc[(x['size'] >= 2) & (x['size'] <= 15)].sample(5)['hadm_id'].tolist()\n",
    "\n",
    "str_subset_stats = str_common_all_stats.loc[(str_common_all_stats['hadm_id'].isin(hadms))][main + sub1].copy().reset_index(drop=True)\n",
    "\n",
    "notes_subset = notes_common_all.loc[(notes_common_all['hadm_id'].isin(hadms))][main + sub2].copy().reset_index(drop=True)\n",
    "str_subset_stats.shape, str_subset_stats['hadm_id'].nunique(), notes_subset.shape, notes_subset['hadm_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T18:11:55.004635Z",
     "start_time": "2019-11-18T18:11:54.971699Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(str_subset_stats.groupby('hadm_id').size(), columns=['size']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T18:11:55.061968Z",
     "start_time": "2019-11-18T18:11:55.008074Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(notes_subset.groupby('hadm_id').size(), columns=['size']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T18:14:15.079605Z",
     "start_time": "2019-11-18T18:14:14.862796Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([str_subset_stats, notes_subset], axis=0, sort=True)\n",
    "df.sort_values(by=['charttime'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "pd.DataFrame(df.groupby('hadm_id').size(), columns=['size']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T18:15:19.851392Z",
     "start_time": "2019-11-18T18:15:19.660607Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T18:16:35.930853Z",
     "start_time": "2019-11-18T18:16:35.739103Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i += 1\n",
    "print(hadms[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T18:16:35.981640Z",
     "start_time": "2019-11-18T18:16:35.937314Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "str_subset_stats[str_subset_stats['hadm_id'] == hadms[i]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T18:16:36.016072Z",
     "start_time": "2019-11-18T18:16:35.983820Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_subset[notes_subset['hadm_id'] == hadms[i]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T18:16:36.054102Z",
     "start_time": "2019-11-18T18:16:36.017845Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[df['hadm_id'] == hadms[i]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T18:15:13.298504Z",
     "start_time": "2019-11-18T18:15:13.242748Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df[['admittime', 'intime']] = df.groupby('hadm_id')[['admittime', 'intime']].ffill()\n",
    "df[['admittime', 'intime']] = df.groupby('hadm_id')[['admittime', 'intime']].bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T18:16:13.813225Z",
     "start_time": "2019-11-18T18:16:13.738764Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols = ['fibrinogen', 'fibrinogen_min', 'wbc', 'wbc_mean', 'hemoglobin', 'hemoglobin_median', 'potassium', 'potassium_std', 'bilirubin_total', 'bilirubin_total_median']\n",
    "df[cols] = df.groupby('hadm_id')[cols].ffill()\n",
    "df[cols] = df[cols].fillna(df[cols].median())\n",
    "df['note'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T18:16:51.068141Z",
     "start_time": "2019-11-18T18:16:51.018234Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Concat the notes and structured data such that `charttime` will list the time for _all_ charted data points such that if note was charted, structured variables are `NaN` and vice versa\n",
    "2. Forward fill and backward fill `admittime` and `intime` coming from the notes grouping by `hadm_id` so that `NaT`s get replaced\n",
    "3. Forward fill the structured variable values grouping by `hadm_id`\n",
    "4. For those records where note were charted before structured data, replace the `NaN` with population median\n",
    "5. Replace `note` `NaN`s with empty string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:47:42.455678Z",
     "start_time": "2019-11-19T02:47:42.087586Z"
    }
   },
   "outputs": [],
   "source": [
    "full_common_vital = pd.concat([str_common_vital_stats, notes_common_vital], axis=0, sort=True)\n",
    "full_common_vital.sort_values(by=['charttime'], inplace=True)\n",
    "full_common_vital.reset_index(drop=True, inplace=True)\n",
    "\n",
    "full_common_vital[['admittime', 'intime']] = full_common_vital.groupby('hadm_id')[['admittime', 'intime']].ffill()\n",
    "full_common_vital[['admittime', 'intime']] = full_common_vital.groupby('hadm_id')[['admittime', 'intime']].bfill()\n",
    "\n",
    "full_common_vital.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:47:44.627455Z",
     "start_time": "2019-11-19T02:47:43.133346Z"
    }
   },
   "outputs": [],
   "source": [
    "full_common_vital[str_vital_cols] = full_common_vital.groupby('hadm_id')[str_vital_cols].ffill()\n",
    "full_common_vital[str_vital_cols] = full_common_vital[str_vital_cols].fillna(full_common_vital[str_vital_cols].median())\n",
    "full_common_vital['note'].fillna('', inplace=True)\n",
    "\n",
    "x = pd.DataFrame(full_common_vital.isna().sum(), columns=['sum']).reset_index()\n",
    "assert(x['sum'].sum() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:47:45.378813Z",
     "start_time": "2019-11-19T02:47:45.266415Z"
    }
   },
   "outputs": [],
   "source": [
    "order = ['hadm_id', 'admittime', 'charttime', 'intime'] + list(str_vital_cols) + ['note']\n",
    "full_common_vital = full_common_vital[order]\n",
    "full_common_vital.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:47:48.781117Z",
     "start_time": "2019-11-19T02:47:47.968665Z"
    }
   },
   "outputs": [],
   "source": [
    "full_common_vital['admit_to_icu'] = (full_common_vital['intime'] - full_common_vital['admittime'])/np.timedelta64(1, 'D')\n",
    "full_common_vital['chart_to_icu'] = (full_common_vital['intime'] - full_common_vital['charttime'])/np.timedelta64(1, 'D')\n",
    "full_common_vital['note_len'] = full_common_vital['note'].apply(len)\n",
    "\n",
    "full_common_vital['interval'] = full_common_vital['chart_to_icu'].apply(data_interval)\n",
    "full_common_vital['imi_adm_label'] = full_common_vital['interval'].apply(icu_adm_label)\n",
    "full_common_vital.shape, full_common_vital['hadm_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:48:06.770754Z",
     "start_time": "2019-11-19T02:48:05.703289Z"
    }
   },
   "outputs": [],
   "source": [
    "full_common_all = pd.concat([str_common_all_stats, notes_common_all], axis=0, sort=True)\n",
    "full_common_all.sort_values(by=['charttime'], inplace=True)\n",
    "full_common_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "full_common_all[['admittime', 'intime']] = full_common_all.groupby('hadm_id')[['admittime', 'intime']].ffill()\n",
    "full_common_all[['admittime', 'intime']] = full_common_all.groupby('hadm_id')[['admittime', 'intime']].bfill()\n",
    "\n",
    "full_common_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:48:13.224108Z",
     "start_time": "2019-11-19T02:48:07.431530Z"
    }
   },
   "outputs": [],
   "source": [
    "full_common_all[str_all_cols] = full_common_all.groupby('hadm_id')[str_all_cols].ffill()\n",
    "full_common_all[str_all_cols] = full_common_all[str_all_cols].fillna(full_common_all[str_all_cols].median())\n",
    "full_common_all['note'].fillna('', inplace=True)\n",
    "\n",
    "x = pd.DataFrame(full_common_all.isna().sum(), columns=['sum']).reset_index()\n",
    "assert(x['sum'].sum() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:48:13.472210Z",
     "start_time": "2019-11-19T02:48:13.226663Z"
    }
   },
   "outputs": [],
   "source": [
    "order = ['hadm_id', 'admittime', 'charttime', 'intime'] + list(str_all_cols) + ['note']\n",
    "full_common_all = full_common_all[order]\n",
    "full_common_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:48:15.199687Z",
     "start_time": "2019-11-19T02:48:14.462267Z"
    }
   },
   "outputs": [],
   "source": [
    "full_common_all['admit_to_icu'] = (full_common_all['intime'] - full_common_all['admittime'])/np.timedelta64(1, 'D')\n",
    "full_common_all['chart_to_icu'] = (full_common_all['intime'] - full_common_all['charttime'])/np.timedelta64(1, 'D')\n",
    "full_common_all['note_len'] = full_common_all['note'].apply(len)\n",
    "\n",
    "\n",
    "full_common_all['interval'] = full_common_all['chart_to_icu'].apply(data_interval)\n",
    "full_common_all['imi_adm_label'] = full_common_all['interval'].apply(icu_adm_label)\n",
    "full_common_all['hadm_id'].nunique(), len(full_common_all)\n",
    "\n",
    "full_common_all.shape, full_common_all['hadm_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T20:05:36.331139Z",
     "start_time": "2019-11-18T20:05:35.898083Z"
    }
   },
   "outputs": [],
   "source": [
    "# g = full_common_vital.loc[full_common_vital['imi_adm_label'] != -1].groupby(['imi_adm_label']).size().to_numpy()\n",
    "# print(f\"Prevalence of positive class with only vitals and notes:{(g[1]/g.sum())*100:0.1f}%\")\n",
    "\n",
    "# g = full_common_all.loc[full_common_all['imi_adm_label'] != -1].groupby(['imi_adm_label']).size().to_numpy()\n",
    "# print(f\"Prevalence of positive class with only vitals, labs, and notes:{(g[1]/g.sum())*100:0.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:48:21.571015Z",
     "start_time": "2019-11-19T02:48:21.348909Z"
    }
   },
   "outputs": [],
   "source": [
    "full_common_vital.groupby(['imi_adm_label']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:48:21.737371Z",
     "start_time": "2019-11-19T02:48:21.573838Z"
    }
   },
   "outputs": [],
   "source": [
    "full_common_all.groupby(['imi_adm_label']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:49:41.881530Z",
     "start_time": "2019-11-19T02:48:28.311680Z"
    }
   },
   "outputs": [],
   "source": [
    "full_common_vital.to_csv(path/'full_common_vital.csv', index=False)\n",
    "full_common_all.to_csv(path/'full_common_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new table with just the common `hadm_id` by running the following code:\n",
    "```\n",
    "create table common_adms (hadm_id integer);\n",
    "\\copy common_adms(hadm_id) from '/path/to/common_hadm_ids.csv' delimiter ',' csv header;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T02:49:42.141774Z",
     "start_time": "2019-11-19T02:49:41.884126Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(path/'full_common_vital_hadm_ids.csv', 'w') as f:\n",
    "  f.write('hadm_id\\n')\n",
    "  f.write('\\n'.join([str(i) for i in full_common_vital['hadm_id'].unique()]))\n",
    "\n",
    "with open(path/'full_common_all_hadm_ids.csv', 'w') as f:\n",
    "  f.write('hadm_id\\n')\n",
    "  f.write('\\n'.join([str(i) for i in full_common_all['hadm_id'].unique()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Cohort: **notes_all**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Read in all **notes_all** and subset it to get all the data with label not equal to -1 (only data used for modeling). Then get the unique ``hadm_id``'s within that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:34:57.639816Z",
     "start_time": "2019-11-08T11:34:55.210139Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_df = pd.read_csv(path/'notes_all_proc.csv', parse_dates=['intime', 'admittime', 'ne_charttime'])\n",
    "model_notes_df = notes_df[notes_df['imi_adm_label'] != -1].reset_index(drop=True)\n",
    "hadms = model_notes_df['hadm_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Subset the **notes_cohort** to get details of only those encountered that are used for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:34:57.701506Z",
     "start_time": "2019-11-08T11:34:57.641856Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_cohort = pd.read_csv(path/'notes_all_cohort.csv')\n",
    "notes_cohort = notes_cohort[notes_cohort['hadm_id'].isin(hadms)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:34:57.723224Z",
     "start_time": "2019-11-08T11:34:57.703132Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def group_eth(eth):\n",
    "  eth = eth.lower()\n",
    "  if 'white' in eth:\n",
    "    return 'white'\n",
    "  elif 'black' in eth:\n",
    "    return 'black'\n",
    "  elif 'hispanic' in eth:\n",
    "    return 'hispanic'\n",
    "  elif 'asian' in eth:\n",
    "    return 'asian'\n",
    "  else:\n",
    "    return 'other'\n",
    "\n",
    "notes_cohort['ethnicity'] = notes_cohort['ethnicity'].apply(group_eth)\n",
    "notes_cohort.loc[notes_cohort['admission_age'] > 100, 'admission_age'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:34:57.755577Z",
     "start_time": "2019-11-08T11:34:57.724741Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Number of patients in notes cohort: {notes_cohort['subject_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:34:57.777448Z",
     "start_time": "2019-11-08T11:34:57.757106Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = notes_cohort.groupby('expire_flag')['subject_id'].nunique().to_numpy()\n",
    "print(f\"Mortality in notes cohort: {g[1]} ({(g[1]/g.sum())*100:0.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:34:57.796887Z",
     "start_time": "2019-11-08T11:34:57.778932Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = notes_cohort.groupby('gender')['subject_id'].nunique().to_numpy()\n",
    "print(f\"Males in notes cohort: {g[1]} ({(g[1]/g.sum())*100:0.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:34:57.845698Z",
     "start_time": "2019-11-08T11:34:57.798152Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Mean:{notes_cohort.groupby('subject_id')['admission_age'].first().mean():0.1f}\")\n",
    "print(f\"STD:{notes_cohort.groupby('subject_id')['admission_age'].first().std():0.1f}\")\n",
    "print(f\"25th percentile:{notes_cohort.groupby('subject_id')['admission_age'].first().quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{notes_cohort.groupby('subject_id')['admission_age'].first().quantile(0.75):0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:34:57.886585Z",
     "start_time": "2019-11-08T11:34:57.847471Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = pd.DataFrame(notes_cohort.groupby('admission_type')['hadm_id'].nunique()).reset_index()\n",
    "g.columns = ['encounter_type', 'count']\n",
    "g['pct'] = np.round((g['count']/g['count'].sum() * 100), 1)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:34:57.911379Z",
     "start_time": "2019-11-08T11:34:57.888169Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = pd.DataFrame(notes_cohort.groupby('ethnicity')['subject_id'].nunique()).reset_index()\n",
    "g.columns = ['ethnicity', 'count']\n",
    "g['pct'] = np.round((g['count']/g['count'].sum() * 100), 1)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Notes Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:35:02.943214Z",
     "start_time": "2019-11-08T11:35:01.777594Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_df = pd.read_csv(path/'notes_all_proc.csv', parse_dates=['intime', 'admittime', 'ne_charttime'])\n",
    "model_notes_df = notes_df[notes_df['imi_adm_label'] != -1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:35:04.907623Z",
     "start_time": "2019-11-08T11:35:04.729899Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Encounter time to ICU Admission for model cohort:\")\n",
    "print(f\"Mean:{model_notes_df['admit_to_icu'].mean():0.1f}\")\n",
    "print(f\"STD:{model_notes_df['admit_to_icu'].std():0.1f}\")\n",
    "print(f\"25th percentile:{model_notes_df['admit_to_icu'].quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{model_notes_df['admit_to_icu'].quantile(0.75):0.1f}\")\n",
    "print(\"Encounter time to ICU Admission for notes cohort:\")\n",
    "print(f\"Mean:{notes_df['admit_to_icu'].mean():0.1f}\")\n",
    "print(f\"STD:{notes_df['admit_to_icu'].std():0.1f}\")\n",
    "print(f\"25th percentile:{notes_df['admit_to_icu'].quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{notes_df['admit_to_icu'].quantile(0.75):0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:35:04.927043Z",
     "start_time": "2019-11-08T11:35:04.909595Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Average Number of clinical notes per encounter for model cohort: {(len(model_notes_df)/model_notes_df['hadm_id'].nunique()):0.1f}\")\n",
    "print(f\"Average Number of clinical notes per encounter for notes cohort: {(len(notes_df)/notes_df['hadm_id'].nunique()):0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:35:05.090050Z",
     "start_time": "2019-11-08T11:35:05.065585Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Clinical Note Length for model cohort:\")\n",
    "print(f\"Mean:{model_notes_df['note_len'].mean():0.1f}\")\n",
    "print(f\"STD:{model_notes_df['note_len'].std():0.1f}\")\n",
    "print(f\"25th percentile:{model_notes_df['note_len'].quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{model_notes_df['note_len'].quantile(0.75):0.1f}\")\n",
    "print()\n",
    "print(\"Clinical Note Length for notes cohort:\")\n",
    "print(f\"Mean:{notes_df['note_len'].mean():0.1f}\")\n",
    "print(f\"STD:{notes_df['note_len'].std():0.1f}\")\n",
    "print(f\"25th percentile:{notes_df['note_len'].quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{notes_df['note_len'].quantile(0.75):0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:35:06.479779Z",
     "start_time": "2019-11-08T11:35:06.449176Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Note distribution by category in model cohort:\")\n",
    "g = pd.DataFrame(model_notes_df.groupby('category').size()).reset_index()\n",
    "g.columns = ['category', 'count']\n",
    "g['pct'] = np.round((g['count']/g['count'].sum() * 100), 1)\n",
    "print(g)\n",
    "print()\n",
    "print(\"Note distribution by category in notes cohort:\")\n",
    "g = pd.DataFrame(notes_df.groupby('category').size()).reset_index()\n",
    "g.columns = ['category', 'count']\n",
    "g['pct'] = np.round((g['count']/g['count'].sum() * 100), 1)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Notes Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:49:29.411016Z",
     "start_time": "2019-11-08T11:49:28.286327Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cohort = 'notes_all'\n",
    "notes_df = pd.read_csv(path/f'{cohort}_proc.csv', parse_dates=['intime', 'admittime', 'ne_charttime'])\n",
    "\n",
    "save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:49:30.079479Z",
     "start_time": "2019-11-08T11:49:29.412776Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Note length distribution\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "sns.distplot(notes_df['note_len'], kde=False, ax=ax, bins=100)\n",
    "ax.set_xlim(0, 10000)\n",
    "ax.set_xlabel('Length of Note (characters)')\n",
    "ax.set_ylabel('# notes')\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_note_len_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:49:30.709041Z",
     "start_time": "2019-11-08T11:49:30.081827Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Note distribution over days before ICU admission binned to 15 days\n",
    "plot_df = notes_df[['admit_to_icu']]\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.distplot(plot_df, kde=False, ax=ax, bins=80)\n",
    "ax.set_xlabel('Time to ICU admission (days)')\n",
    "ax.set_ylabel('# notes')\n",
    "ax.set_xlim(0, 70)\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_admit_to_icu_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:49:31.260714Z",
     "start_time": "2019-11-08T11:49:30.711313Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Note distribution over days before ICU admission binned to 15 days\n",
    "intervals = ['-1 ≤ t ≤ 0']\n",
    "intervals += [f'-{i+1} ≤ t ≤ -{i}' for i in range(1, notes_df['interval'].max())]\n",
    "intervals.append(f\"t ≥ -{notes_df['interval'].max()}\")\n",
    "\n",
    "plot_df = pd.DataFrame(notes_df.loc[notes_df['interval'] != -1].groupby('interval').size(), columns=['n_notes']).reset_index(drop=True)\n",
    "plot_df['days'] = intervals\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "sns.barplot(x='days', y='n_notes', data=plot_df, ax=ax)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=45, ha='right')\n",
    "ax.set_xlabel('Time to ICU admission (days)')\n",
    "ax.set_ylabel('# notes')\n",
    "for index, row in plot_df.iterrows():\n",
    "    ax.text(index, row['n_notes'], str(row['n_notes']), color='black', ha='center', va='bottom')\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_admit_to_icu_binned_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T12:47:22.475327Z",
     "start_time": "2019-11-08T12:47:17.467891Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note distribution over days before ICU admission by Category  binned to 15 days\n",
    "def plot_intervals(ax, df, cat):\n",
    "  sns.barplot(x='days', y='n_notes', data=df, ax=ax)\n",
    "  ax.set_xticklabels(ax.get_xticklabels(),rotation=45, ha='right')\n",
    "  ax.set_xlabel('')\n",
    "  ax.set_ylabel('')\n",
    "  ax.set_title(f\"Note Category: {cat}\\n# notes: {df['n_notes'].sum()}\")   \n",
    "\n",
    "  for index, (_, row) in enumerate(df.iterrows()):\n",
    "      ax.text(index, row['n_notes'], str(row['n_notes']), color='black', ha='center', va='bottom') \n",
    "\n",
    "plot_df = pd.DataFrame(notes_df.groupby(['category', 'interval']).size(), columns=['n_notes'])\n",
    "plot_df.reset_index(inplace=True)\n",
    "plot_df['days'] = plot_df['interval'].apply(lambda x: intervals[x])\n",
    "plot_df.drop(['interval'], inplace=True, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(4, 3, figsize=(20, 25))\n",
    "plot_intervals(ax[0][0], plot_df.loc[plot_df['category'] == 'Case Management ', ['n_notes', 'days']], 'Case Management')\n",
    "plot_intervals(ax[0][1], plot_df.loc[plot_df['category'] == 'Consult', ['n_notes', 'days']], 'Consult')\n",
    "plot_intervals(ax[0][2], plot_df.loc[plot_df['category'] == 'General', ['n_notes', 'days']], 'General')\n",
    "               \n",
    "plot_intervals(ax[1][0], plot_df.loc[plot_df['category'] == 'Nursing', ['n_notes', 'days']], 'Nursing')\n",
    "plot_intervals(ax[1][1], plot_df.loc[plot_df['category'] == 'Nursing/other', ['n_notes', 'days']], 'Nursing/other')\n",
    "plot_intervals(ax[1][2], plot_df.loc[plot_df['category'] == 'Nutrition', ['n_notes', 'days']], 'Nutrition')\n",
    "\n",
    "plot_intervals(ax[2][0], plot_df.loc[plot_df['category'] == 'Pharmacy', ['n_notes', 'days']], 'Pharmacy')\n",
    "plot_intervals(ax[2][1], plot_df.loc[plot_df['category'] == 'Physician ', ['n_notes', 'days',]], 'Physician')\n",
    "plot_intervals(ax[2][2], plot_df.loc[plot_df['category'] == 'Radiology', ['n_notes', 'days']], 'Radiology')\n",
    "               \n",
    "plot_intervals(ax[3][0], plot_df.loc[plot_df['category'] == 'Rehab Services', ['n_notes', 'days']], 'Rehab Services')\n",
    "plot_intervals(ax[3][1], plot_df.loc[plot_df['category'] == 'Respiratory ', ['n_notes', 'days']], 'Respiratory')\n",
    "plot_intervals(ax[3][2], plot_df.loc[plot_df['category'] == 'Social Work', ['n_notes', 'days']], 'Social Work')\n",
    "\n",
    "fig.text(0.5, 0.09, 'Time to ICU admission (days)', ha='center')\n",
    "fig.text(0.08, 0.5, '# notes', va='center', rotation='vertical')\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.3)\n",
    "               \n",
    "if save:               \n",
    "  fig.savefig(figdir/f'{cohort}_admit_to_icu_cat_binned_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T14:25:52.503649Z",
     "start_time": "2019-11-08T14:25:51.885341Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Histogram of time between note charttime and ICU admittime\n",
    "plot_df = notes_df[['category', 'note_to_icu']]\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.distplot(plot_df['note_to_icu'], kde=False, ax=ax, bins=80)\n",
    "ax.set_xlabel('Note Charttime to ICU Admittime (days)')\n",
    "ax.set_ylabel('# notes')\n",
    "ax.set_xlim(0, 60)\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_note_to_icu_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T14:26:18.244019Z",
     "start_time": "2019-11-08T14:26:13.786692Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Histogram of time between note charttime and ICU admittime by Category\n",
    "def plot_period(ax, df, cat):\n",
    "  sns.distplot(df, kde=False, ax=ax, bins=10)\n",
    "  ax.set_xlabel('')\n",
    "  ax.set_ylabel('')\n",
    "  ax.set_title(f\"Note Category: {cat}\")\n",
    "\n",
    "fig, ax = plt.subplots(4, 3, figsize=(20, 25))\n",
    "plot_period(ax[0][0], plot_df.loc[plot_df['category'] == 'Case Management ', ['note_to_icu']], 'Case Management')\n",
    "plot_period(ax[0][1], plot_df.loc[plot_df['category'] == 'Consult', ['note_to_icu']], 'Consult')\n",
    "plot_period(ax[0][2], plot_df.loc[plot_df['category'] == 'General', ['note_to_icu']], 'General')\n",
    "\n",
    "plot_period(ax[1][0], plot_df.loc[plot_df['category'] == 'Nursing', ['note_to_icu']], 'Nursing')\n",
    "plot_period(ax[1][1], plot_df.loc[plot_df['category'] == 'Nursing/other', ['note_to_icu']], 'Nursing/other')\n",
    "plot_period(ax[1][2], plot_df.loc[plot_df['category'] == 'Nutrition', ['note_to_icu']], 'Nutrition')\n",
    "\n",
    "plot_period(ax[2][0], plot_df.loc[plot_df['category'] == 'Pharmacy', ['note_to_icu']], 'Pharmacy')\n",
    "plot_period(ax[2][1], plot_df.loc[plot_df['category'] == 'Physician ', ['note_to_icu',]], 'Physician')\n",
    "plot_period(ax[2][2], plot_df.loc[plot_df['category'] == 'Radiology', ['note_to_icu']], 'Radiology')\n",
    "\n",
    "plot_period(ax[3][0], plot_df.loc[plot_df['category'] == 'Rehab Services', ['note_to_icu']], 'Rehab Services')\n",
    "plot_period(ax[3][1], plot_df.loc[plot_df['category'] == 'Respiratory ', ['note_to_icu']], 'Respiratory')\n",
    "plot_period(ax[3][2], plot_df.loc[plot_df['category'] == 'Social Work', ['note_to_icu']], 'Social Work')\n",
    "\n",
    "fig.text(0.5, 0.1, 'Note Charttime to ICU Admittime (days)', ha='center')\n",
    "fig.text(0.08, 0.5, '# notes', va='center', rotation='vertical')\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.1)\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_note_to_icu_cat_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:03:10.440104Z",
     "start_time": "2019-11-08T15:03:10.159215Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "desc = ['Unused', 'Delayed ICU Admission', 'Imminent ICU Admission']\n",
    "\n",
    "p = pd.DataFrame(notes_df.groupby(['imi_adm_label']).size(), columns=['n_notes']).reset_index()\n",
    "# p1 = pd.DataFrame(notes_df.groupby(['imi_adm_label']).size(), columns=['n_notes']).reset_index()\n",
    "# p2 = notes_df.groupby(['imi_adm_label'])['hadm_id'].nunique().reset_index()\n",
    "\n",
    "# p = p1.merge(p2, on=['imi_adm_label'])\n",
    "p['imi_adm_label'] = desc\n",
    "p = p.reindex([2, 1, 0])\n",
    "# p.reset_index(inplace=True, drop=True)\n",
    "\n",
    "plot_df = p.copy()\n",
    "plot_df.rename(columns={'hadm_id':'# Encounters', 'n_notes':'# Notes'}, inplace=True)\n",
    "plot_df = pd.melt(plot_df, id_vars='imi_adm_label', var_name='Legend', value_name='counts')\n",
    "\n",
    "plot_df\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "sns.barplot(x='imi_adm_label', y='counts', data=plot_df, ax=ax)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), ha='center')\n",
    "ax.set_xlabel('Class Label')\n",
    "ax.set_ylabel('# notes')\n",
    "\n",
    "for index, row in plot_df.iterrows():\n",
    "#     if index < len(plot_df)//2:\n",
    "        ax.text(index+0.06, row['counts'], str(row['counts']), color='black', ha='right', va='bottom')\n",
    "#     else:\n",
    "#         ax.text(index % (len(plot_df)//2), row['counts'], str(row['counts']), color='black', ha='right', va='bottom')\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_note_class_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:49:16.198800Z",
     "start_time": "2019-10-20T11:49:08.788077Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 1. Read in the data and drop duplicates\n",
    "notes_df = pd.read_csv(mimic_path/'imi_notes_mimic.csv', parse_dates=['intime', 'admittime', 'ne_charttime'])\n",
    "notes_df.drop_duplicates(inplace=True)\n",
    "\n",
    "str_df = pd.read_csv(mimic_path/'imi_str_mimic.csv', parse_dates=['ce_charttime'])\n",
    "str_df.drop_duplicates(inplace=True)\n",
    "\n",
    "str_df.shape, notes_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Get the `hadm_id`s for just the notes and create table for those. This is to get the characterstics only those `hadm_ids` with notes (and possibly structured) for extracting cohort characterstics in case just notes results suck.\n",
    "\n",
    "```\n",
    "create table notes_adms (hadm_id integer);\n",
    "\\copy notes_adms(hadm_id) from '/path/to/notes_hadm_ids.csv' delimiter ',' csv header;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:39:16.964414Z",
     "start_time": "2019-10-29T19:39:16.925120Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(mimic_path/'hadm_ids_with_notes.csv', 'w') as f:\n",
    "  f.write('hadm_id\\n')\n",
    "  f.write('\\n'.join([str(i) for i in notes_df['hadm_id'].unique()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:49:16.570786Z",
     "start_time": "2019-10-20T11:49:16.200530Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 2. Extract common `hadm_id` and filter out those that do not appear in both dfs\n",
    "hadms = set(str_df['hadm_id'].unique()).intersection(notes_df['hadm_id'].unique())\n",
    "print(f\"Number of encounters having both forms of data: {len(hadms)}\")\n",
    "\n",
    "common_notes = notes_df[notes_df['hadm_id'].isin(hadms)].reset_index(drop=True)\n",
    "common_str = str_df[str_df['hadm_id'].isin(hadms)].reset_index(drop=True)\n",
    "\n",
    "common_str.shape, str_df.shape, common_notes.shape, notes_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a new table with just the common `hadm_id` by running the following code:\n",
    "```\n",
    "create table common_adms (hadm_id integer);\n",
    "\\copy common_adms(hadm_id) from '/path/to/common_hadm_ids.csv' delimiter ',' csv header;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:49:22.355557Z",
     "start_time": "2019-10-20T11:49:22.323641Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sanity check and write the common `hadm_id`s to disk for extracting cohort characterstics\n",
    "s, n = set(common_str['hadm_id'].unique()), set(common_notes['hadm_id'].unique())\n",
    "assert(s.symmetric_difference(n) == set())\n",
    "\n",
    "with open(mimic_path/'common_hadm_ids.csv', 'w') as f:\n",
    "  f.write('hadm_id\\n')\n",
    "  f.write('\\n'.join([str(i) for i in s]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After this we run `cohort.sql` from the MIMIC extraction repo, to create the cohort table which contains details about the cohort. We will have two tables: `common_cohort` with `hadm_id`s that have both structured and notes data, and `notes_cohort` which with `hadm_id`s that have notes data (and maybe sturctured data that we don't care about)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:50:06.003783Z",
     "start_time": "2019-10-20T11:49:58.226274Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 3. remove redundant info by filling in each time column with the value of the var\n",
    "common_str = common_str.groupby(['hadm_id','ce_charttime']).sum(min_count = 1).reset_index()\n",
    "# 4. groupby ffill \n",
    "common_str = common_str.groupby(['hadm_id'], as_index=False).apply(lambda group: group.ffill())\n",
    "common_str.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Steps 5 & 6 for both common_notes and notes_df\n",
    "# 5. cat the category, description, text into a new note col and remove them\n",
    "notes_df['note'] = notes_df['category'].str.cat(notes_df['description'], sep='\\n')\n",
    "notes_df['note'] = notes_df['note'].str.cat(notes_df['text'], sep='\\n')\n",
    "notes_df['note_len'] = notes_df['note'].apply(len)\n",
    "notes_df.drop(columns=['category', 'description', 'text'], inplace=True)  \n",
    "\n",
    "# 6. cat notes charted at the same time\n",
    "notes_df = pd.DataFrame(notes_df.groupby(['hadm_id', 'intime', 'admittime', 'ne_charttime'])['note'].apply('\\n'.join)).reset_index()\n",
    "\n",
    "notes_df['category'] = notes_df['note'].apply(lambda x: x.split('\\n')[0])\n",
    "notes_df['admit_to_icu'] = (notes_df['intime'] - notes_df['admittime'])/np.timedelta64(1, 'D')\n",
    "notes_df['note_to_icu'] = (notes_df['intime'] - notes_df['ne_charttime'])/np.timedelta64(1, 'D')\n",
    "notes_df['note_len'] = notes_df['note'].apply(len)\n",
    "\n",
    "# 5. cat the category, description, text into a new note col and remove them\n",
    "common_notes['note'] = common_notes['category'].str.cat(common_notes['description'], sep='\\n')\n",
    "common_notes['note'] = common_notes['note'].str.cat(common_notes['text'], sep='\\n')\n",
    "common_notes['note_len'] = common_notes['note'].apply(len)\n",
    "common_notes.drop(columns=['category', 'description', 'text'], inplace=True)  \n",
    "\n",
    "# 6. cat notes charted at the same time\n",
    "common_notes = pd.DataFrame(common_notes.groupby(['hadm_id', 'intime', 'admittime', 'ne_charttime'])['note'].apply('\\n'.join)).reset_index()\n",
    "\n",
    "common_notes['category'] = common_notes['note'].apply(lambda x: x.split('\\n')[0])\n",
    "common_notes['admit_to_icu'] = (common_notes['intime'] - common_notes['admittime'])/np.timedelta64(1, 'D')\n",
    "common_notes['note_to_icu'] = (common_notes['intime'] - common_notes['ne_charttime'])/np.timedelta64(1, 'D')\n",
    "common_notes['note_len'] = common_notes['note'].apply(len)\n",
    "\n",
    "common_notes.shape, notes_df.shape, notes_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can drop those rows which don't have basic vitals of HR, MAP, and RESP. But for now, we go with everything we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# common_str.dropna(subset=['hr', 'map', 'resp'], how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T13:30:50.329682Z",
     "start_time": "2019-10-17T13:30:40.405909Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "common_str.to_csv(mimic_path/'proc1_str_mimic.csv', index=False)\n",
    "common_notes.to_csv(mimic_path/'proc1_notes_mimic_common.csv', index=False)\n",
    "notes_df.to_csv(mimic_path/'proc1_notes_mimic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Dev for Structured Data Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This code is only development on a subset of encounters to calculate change statistics. This will be run for all the encounters separately using the `stats.py` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:51:38.683282Z",
     "start_time": "2019-10-16T16:51:38.660983Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def percentile(n):\n",
    "  def percentile_(x):\n",
    "    return x.quantile(n)\n",
    "  percentile_.__name__ = f'percentile_{n*100:2.0f}'\n",
    "  return percentile_\n",
    "\n",
    "def change_name(col_name):\n",
    "  if '(' not in col_name:\n",
    "    return col_name\n",
    "  cols = literal_eval(col_name)\n",
    "  return f'{cols[0]}_{cols[1]}'\n",
    "\n",
    "statistics = ['min', 'mean', 'median', 'std', 'var', 'kurt', 'skew', percentile(0.25), percentile(0.75), stats.iqr, 'max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:51:39.443285Z",
     "start_time": "2019-10-16T16:51:38.684845Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "common_str = pd.read_csv(mimic_path/'proc1_str_mimic.csv', parse_dates=['ce_charttime'])\n",
    "var_cols = common_str.columns[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:51:39.474480Z",
     "start_time": "2019-10-16T16:51:39.444800Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hadms = [100104, 100975, 101511, 111073]\n",
    "subset_df = common_str.loc[(common_str['hadm_id'].isin(hadms))].reset_index(drop=True)\n",
    "subset_df.groupby('hadm_id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:52:35.484546Z",
     "start_time": "2019-10-16T16:52:27.419630Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for hadm_id, group_df in tqdm(subset_df.groupby('hadm_id'), desc='Encounters'):\n",
    "  df = group_df.copy()\n",
    "  var_df = df[var_cols].reset_index(drop=True) # save the original vals for later\n",
    "  \n",
    "  df.set_index('ce_charttime', inplace=True) # set charttime as index for rolling 24h\n",
    "  stats_df = df[var_cols].rolling('24h').agg(statistics)\n",
    "  \n",
    "  df = pd.DataFrame(stats_df.to_records()) # flatten the resulting dataframe\n",
    "  df.insert(loc=1, column='hadm_id', value=hadm_id)\n",
    "  \n",
    "  df.rename(columns=change_name, inplace=True) # rename columns\n",
    "  df = pd.concat([df, var_df], axis=1) # add the original vals back\n",
    "  \n",
    "  # reorder vars such that the columns are var, var_stat...\n",
    "  stats_cols = df.columns[2:]\n",
    "  all_cols = []\n",
    "  for var in var_cols:\n",
    "    all_cols.append(var)\n",
    "    for stat in stats_cols:\n",
    "      if f'{var}_' in stat:\n",
    "        all_cols.append(stat)\n",
    "        \n",
    "  order = list(df.columns[:2]) + all_cols\n",
    "  df = df[order]\n",
    "  \n",
    "  df.to_csv(mimic_path/f'stats_dir/{hadm_id}.csv', index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-19T22:06:42.381630Z",
     "start_time": "2019-10-19T22:02:42.935524Z"
    },
    "code_folding": [
     6
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "stats_dir = mimic_path/'stats_dir'\n",
    "n_files = 0\n",
    "for _ in stats_dir.glob('*.csv'):\n",
    "  n_files += 1\n",
    "\n",
    "for i, f in enumerate(tqdm(stats_dir.glob('*.csv'), total=n_files, desc='Stats CSV')):\n",
    "    dfs.append(pd.read_csv(f))\n",
    "\n",
    "common_str = pd.concat(dfs)\n",
    "common_str.reset_index(drop=True, inplace=True)\n",
    "common_str['ce_charttime'] =  pd.to_datetime(common_str['ce_charttime'])\n",
    "common_str.to_csv(mimic_path/'proc2_str_mimic_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We are going to label only the notes df. Since we are basing our time element on `ne_charttime` and attaching structured data to `common_notes` to get final merged df, we don't need to label structured data separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:30:17.407651Z",
     "start_time": "2019-10-20T11:30:17.386422Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def note_interval(x):\n",
    "  if pd.isnull(x):\n",
    "    return -1\n",
    "  \n",
    "  if 0 < x <= 1:\n",
    "    return 0\n",
    "  elif 1 < x <= 2:\n",
    "    return 1\n",
    "  elif 2 < x <= 3:\n",
    "    return 2\n",
    "  elif 3 < x <= 4:\n",
    "    return 3\n",
    "  elif 4 < x <= 5:\n",
    "    return 4\n",
    "  elif 5 < x <= 6:\n",
    "    return 5\n",
    "  elif 6 < x <= 7:\n",
    "    return 6\n",
    "  elif 7 < x <= 8:\n",
    "    return 7\n",
    "  elif 8 < x <= 9:\n",
    "    return 8\n",
    "  elif 9 < x <= 10:\n",
    "    return 9\n",
    "  elif 10 < x <= 11:\n",
    "    return 10\n",
    "  elif 11 < x <= 12:\n",
    "    return 11\n",
    "  elif 12 < x <= 13:\n",
    "    return 12\n",
    "  elif 13 < x <= 14:\n",
    "    return 13\n",
    "  elif 14 < x <= 15:\n",
    "    return 14\n",
    "  else:\n",
    "    return 15\n",
    "\n",
    "def icu_adm_label(x):\n",
    "  if 0 <= x <= 1:\n",
    "    return -1 # unused notes due to data leakage\n",
    "  elif 1 < x <= 3:\n",
    "    return 1 # imminent ICU admission\n",
    "  elif 3 < x <= 5:\n",
    "    return -1 # unused notes due to data leakage\n",
    "  else:\n",
    "    return 0 # delayed ICU admission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:30:23.432460Z",
     "start_time": "2019-10-20T11:30:18.851464Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_df = pd.read_csv(mimic_path/'proc1_notes_mimic.csv', parse_dates=['intime', 'admittime', 'ne_charttime'])\n",
    "common_notes = pd.read_csv(mimic_path/'proc1_notes_mimic_common.csv', parse_dates=['intime', 'admittime', 'ne_charttime'])\n",
    "common_notes.shape, notes_df.shape, notes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:31:27.495604Z",
     "start_time": "2019-10-20T11:31:27.288402Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_df['interval'] = notes_df['note_to_icu'].apply(note_interval)\n",
    "common_notes['interval'] = common_notes['note_to_icu'].apply(note_interval)\n",
    "notes_df['imi_adm_label'] = notes_df['interval'].apply(icu_adm_label)\n",
    "common_notes['imi_adm_label'] = common_notes['interval'].apply(icu_adm_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:31:28.596552Z",
     "start_time": "2019-10-20T11:31:28.567980Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = common_notes.loc[common_notes['imi_adm_label'] != -1].groupby(['imi_adm_label']).size().to_numpy()\n",
    "print(f\"Prevalence in notes cohort:{(g[1]/g.sum())*100:0.1f}\")\n",
    "g = notes_df.loc[notes_df['imi_adm_label'] != -1].groupby(['imi_adm_label']).size().to_numpy()\n",
    "print(f\"Prevalence in notes cohort:{(g[1]/g.sum())*100:0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:31:42.621042Z",
     "start_time": "2019-10-20T11:31:34.511905Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_df.to_csv(mimic_path/'proc3_notes_mimic_labeled.csv', index=False)\n",
    "common_notes.to_csv(mimic_path/'proc3_notes_mimic_common_labeled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Cohort Characterstics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:32:03.739988Z",
     "start_time": "2019-10-20T11:32:03.683440Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "common_cohort = pd.read_csv(mimic_path/'common_cohort.csv')\n",
    "notes_cohort = pd.read_csv(mimic_path/'notes_cohort.csv')\n",
    "common_cohort.shape, notes_cohort.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:32:04.076918Z",
     "start_time": "2019-10-20T11:32:04.048416Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def group_eth(eth):\n",
    "  eth = eth.lower()\n",
    "  if 'white' in eth:\n",
    "    return 'white'\n",
    "  elif 'black' in eth:\n",
    "    return 'black'\n",
    "  elif 'hispanic' in eth:\n",
    "    return 'hispanic'\n",
    "  elif 'asian' in eth:\n",
    "    return 'asian'\n",
    "  else:\n",
    "    return 'other'\n",
    "\n",
    "common_cohort['ethnicity'] = common_cohort['ethnicity'].apply(group_eth)\n",
    "notes_cohort['ethnicity'] = notes_cohort['ethnicity'].apply(group_eth)\n",
    "common_cohort.loc[common_cohort['admission_age'] > 100, 'admission_age'] = 100\n",
    "notes_cohort.loc[notes_cohort['admission_age'] > 100, 'admission_age'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:32:05.171807Z",
     "start_time": "2019-10-20T11:32:05.155850Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Number of patients in common cohort: {common_cohort['subject_id'].nunique()}\")\n",
    "print(f\"Number of patients in notes cohort: {notes_cohort['subject_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:32:12.527681Z",
     "start_time": "2019-10-20T11:32:12.461949Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = common_cohort.groupby('expire_flag')['subject_id'].nunique().to_numpy()\n",
    "print(f\"Mortality in common cohort: {g[1]} ({(g[1]/g.sum())*100:0.1f}%)\")\n",
    "\n",
    "g = notes_cohort.groupby('expire_flag')['subject_id'].nunique().to_numpy()\n",
    "print(f\"Mortality in notes cohort: {g[1]} ({(g[1]/g.sum())*100:0.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:32:41.925404Z",
     "start_time": "2019-10-20T11:32:41.904094Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = common_cohort.groupby('gender')['subject_id'].nunique().to_numpy()\n",
    "print(f\"Males in common cohort: {g[1]} ({(g[1]/g.sum())*100:0.1f}%)\")\n",
    "g = notes_cohort.groupby('gender')['subject_id'].nunique().to_numpy()\n",
    "print(f\"Males in notes cohort: {g[1]} ({(g[1]/g.sum())*100:0.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:32:56.650984Z",
     "start_time": "2019-10-20T11:32:56.520232Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Age details in common cohort:\")\n",
    "print(f\"Mean:{common_cohort.groupby('subject_id')['admission_age'].first().mean():0.1f}\")\n",
    "print(f\"STD:{common_cohort.groupby('subject_id')['admission_age'].first().std():0.1f}\")\n",
    "print(f\"25th percentile:{common_cohort.groupby('subject_id')['admission_age'].first().quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{common_cohort.groupby('subject_id')['admission_age'].first().quantile(0.75):0.1f}\")\n",
    "print()\n",
    "print(\"Age details in notes cohort:\")\n",
    "print(f\"Mean:{notes_cohort.groupby('subject_id')['admission_age'].first().mean():0.1f}\")\n",
    "print(f\"STD:{notes_cohort.groupby('subject_id')['admission_age'].first().std():0.1f}\")\n",
    "print(f\"25th percentile:{notes_cohort.groupby('subject_id')['admission_age'].first().quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{notes_cohort.groupby('subject_id')['admission_age'].first().quantile(0.75):0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:33:12.631502Z",
     "start_time": "2019-10-20T11:33:12.560122Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Admission types for common cohort:\")\n",
    "g = pd.DataFrame(common_cohort.groupby('admission_type')['hadm_id'].nunique()).reset_index()\n",
    "g.columns = ['encounter_type', 'count']\n",
    "g['pct'] = np.round((g['count']/g['count'].sum() * 100), 1)\n",
    "print(g)\n",
    "print()\n",
    "print(\"Admission types for notes cohort:\")\n",
    "g = pd.DataFrame(notes_cohort.groupby('admission_type')['hadm_id'].nunique()).reset_index()\n",
    "g.columns = ['encounter_type', 'count']\n",
    "g['pct'] = np.round((g['count']/g['count'].sum() * 100), 1)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:33:21.997140Z",
     "start_time": "2019-10-20T11:33:21.952578Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Ethnicty for common cohort:\")\n",
    "g = pd.DataFrame(common_cohort.groupby('ethnicity')['subject_id'].nunique()).reset_index()\n",
    "g.columns = ['ethnicity', 'count']\n",
    "g['pct'] = np.round((g['count']/g['count'].sum() * 100), 1)\n",
    "print(g)\n",
    "print()\n",
    "print(\"Ethnicty for notes cohort:\")\n",
    "g = pd.DataFrame(notes_cohort.groupby('ethnicity')['subject_id'].nunique()).reset_index()\n",
    "g.columns = ['ethnicity', 'count']\n",
    "g['pct'] = np.round((g['count']/g['count'].sum() * 100), 1)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Notes Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:33:29.153755Z",
     "start_time": "2019-10-20T11:33:27.030467Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_df = pd.read_csv(mimic_path/'proc3_notes_mimic_labeled.csv', parse_dates=['intime', 'admittime', 'ne_charttime'])\n",
    "common_notes = pd.read_csv(mimic_path/'proc3_notes_mimic_common_labeled.csv', parse_dates=['intime', 'admittime', 'ne_charttime'])\n",
    "common_notes.shape, notes_df.shape, notes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:33:29.747237Z",
     "start_time": "2019-10-20T11:33:29.721261Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Encounter time to ICU Admission for common cohort:\")\n",
    "print(f\"Mean:{common_notes['admit_to_icu'].mean():0.1f}\")\n",
    "print(f\"STD:{common_notes['admit_to_icu'].std():0.1f}\")\n",
    "print(f\"25th percentile:{common_notes['admit_to_icu'].quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{common_notes['admit_to_icu'].quantile(0.75):0.1f}\")\n",
    "print()\n",
    "print(\"Encounter time to ICU Admission for notes cohort:\")\n",
    "print(f\"Mean:{notes_df['admit_to_icu'].mean():0.1f}\")\n",
    "print(f\"STD:{notes_df['admit_to_icu'].std():0.1f}\")\n",
    "print(f\"25th percentile:{notes_df['admit_to_icu'].quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{notes_df['admit_to_icu'].quantile(0.75):0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:33:56.162358Z",
     "start_time": "2019-10-20T11:33:56.145514Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Average Number of clinical notes per encounter for common cohort: {(len(common_notes)/common_notes['hadm_id'].nunique()):0.1f}\")\n",
    "print(f\"Average Number of clinical notes per encounter for notes cohort: {(len(notes_df)/notes_df['hadm_id'].nunique()):0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:34:01.172178Z",
     "start_time": "2019-10-20T11:34:01.145983Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Clinical Note Length for common cohort:\")\n",
    "print(f\"Mean:{common_notes['note_len'].mean():0.1f}\")\n",
    "print(f\"STD:{common_notes['note_len'].std():0.1f}\")\n",
    "print(f\"25th percentile:{common_notes['note_len'].quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{common_notes['note_len'].quantile(0.75):0.1f}\")\n",
    "print()\n",
    "print(\"Clinical Note Length for notes cohort:\")\n",
    "print(f\"Mean:{notes_df['note_len'].mean():0.1f}\")\n",
    "print(f\"STD:{notes_df['note_len'].std():0.1f}\")\n",
    "print(f\"25th percentile:{notes_df['note_len'].quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{notes_df['note_len'].quantile(0.75):0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:34:04.778529Z",
     "start_time": "2019-10-20T11:34:04.744011Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Note distribution by category in common cohort:\")\n",
    "g = pd.DataFrame(common_notes.groupby('category').size()).reset_index()\n",
    "g.columns = ['category', 'count']\n",
    "g['pct'] = np.round((g['count']/g['count'].sum() * 100), 1)\n",
    "print(g)\n",
    "print()\n",
    "print(\"Note distribution by category in notes cohort:\")\n",
    "g = pd.DataFrame(notes_df.groupby('category').size()).reset_index()\n",
    "g.columns = ['category', 'count']\n",
    "g['pct'] = np.round((g['count']/g['count'].sum() * 100), 1)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Notes Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:35:45.166115Z",
     "start_time": "2019-10-20T11:35:45.141477Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save = True\n",
    "cohort = 'common'\n",
    "figdir = mimic_path/'figures'\n",
    "\n",
    "if cohort == 'common':\n",
    "  df = common_notes\n",
    "elif cohort == 'notes':\n",
    "  df = notes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:35:45.728222Z",
     "start_time": "2019-10-20T11:35:45.168254Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Note length distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.distplot(df['note_len'], kde=False, ax=ax, bins=100)\n",
    "ax.set_xlim(0, 12500)\n",
    "ax.set_xlabel('Length of Note (characters)')\n",
    "ax.set_ylabel('# notes')\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_note_len_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:35:46.248382Z",
     "start_time": "2019-10-20T11:35:45.730552Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Note distribution over days before ICU admission binned to 15 days\n",
    "plot_df = df[['admit_to_icu']]\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.distplot(plot_df, kde=False, ax=ax, bins=80)\n",
    "ax.set_xlabel('Time to ICU admission (days)')\n",
    "ax.set_ylabel('# notes')\n",
    "ax.set_xlim(0, 70)\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_to_icu_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:35:46.765312Z",
     "start_time": "2019-10-20T11:35:46.250254Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Note distribution over days before ICU admission binned to 15 days\n",
    "intervals = ['-1 ≤ t ≤ 0']\n",
    "intervals += [f'-{i+1} ≤ t ≤ -{i}' for i in range(1, notes_df['interval'].max())]\n",
    "intervals.append(f\"t ≥ -{df['interval'].max()}\")\n",
    "\n",
    "plot_df = pd.DataFrame(df.loc[notes_df['interval'] != -1].groupby('interval').size(), columns=['n_notes']).reset_index(drop=True)\n",
    "plot_df['days'] = intervals\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "sns.barplot(x='days', y='n_notes', data=plot_df, ax=ax)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=45, ha='right')\n",
    "ax.set_xlabel('Time to ICU admission (days)')\n",
    "ax.set_ylabel('# notes')\n",
    "for index, row in plot_df.iterrows():\n",
    "    ax.text(index, row['n_notes'], str(row['n_notes']), color='black', ha='center', va='bottom')\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_admit_to_icu_binned_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:35:51.482585Z",
     "start_time": "2019-10-20T11:35:46.767468Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note distribution over days before ICU admission by Category  binned to 15 days\n",
    "def plot_intervals(ax, df, cat):\n",
    "  sns.barplot(x='days', y='n_notes', data=df, ax=ax)\n",
    "  ax.set_xticklabels(ax.get_xticklabels(),rotation=45, ha='right')\n",
    "  ax.set_xlabel('')\n",
    "  ax.set_ylabel('')\n",
    "  ax.set_title(f\"Note Category: {cat}\\n# notes: {df['n_notes'].sum()}\")   \n",
    "\n",
    "  for index, (_, row) in enumerate(df.iterrows()):\n",
    "      ax.text(index, row['n_notes'], str(row['n_notes']), color='black', ha='center', va='bottom') \n",
    "\n",
    "plot_df = pd.DataFrame(df.groupby(['category', 'interval']).size(), columns=['n_notes'])\n",
    "plot_df.reset_index(inplace=True)\n",
    "plot_df['days'] = plot_df['interval'].apply(lambda x: intervals[x])\n",
    "plot_df.drop(['interval'], inplace=True, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(6, 2, figsize=(20, 50))\n",
    "plot_intervals(ax[0][0], plot_df.loc[plot_df['category'] == 'Case Management ', ['n_notes', 'days']], 'Case Management')\n",
    "plot_intervals(ax[0][1], plot_df.loc[plot_df['category'] == 'Consult', ['n_notes', 'days']], 'Consult')\n",
    "\n",
    "plot_intervals(ax[1][0], plot_df.loc[plot_df['category'] == 'General', ['n_notes', 'days']], 'General')\n",
    "plot_intervals(ax[1][1], plot_df.loc[plot_df['category'] == 'Nursing', ['n_notes', 'days']], 'Nursing')\n",
    "\n",
    "plot_intervals(ax[2][0], plot_df.loc[plot_df['category'] == 'Nursing/other', ['n_notes', 'days']], 'Nursing/other')\n",
    "plot_intervals(ax[2][1], plot_df.loc[plot_df['category'] == 'Nutrition', ['n_notes', 'days']], 'Nutrition')\n",
    "\n",
    "plot_intervals(ax[3][0], plot_df.loc[plot_df['category'] == 'Pharmacy', ['n_notes', 'days']], 'Pharmacy')\n",
    "plot_intervals(ax[3][1], plot_df.loc[plot_df['category'] == 'Physician ', ['n_notes', 'days',]], 'Physician')\n",
    "\n",
    "plot_intervals(ax[4][0], plot_df.loc[plot_df['category'] == 'Radiology', ['n_notes', 'days']], 'Radiology')\n",
    "plot_intervals(ax[4][1], plot_df.loc[plot_df['category'] == 'Rehab Services', ['n_notes', 'days']], 'Rehab Services')\n",
    "\n",
    "plot_intervals(ax[5][0], plot_df.loc[plot_df['category'] == 'Respiratory ', ['n_notes', 'days']], 'Respiratory')\n",
    "plot_intervals(ax[5][1], plot_df.loc[plot_df['category'] == 'Social Work', ['n_notes', 'days']], 'Social Work')\n",
    "\n",
    "fig.text(0.5, 0.1, 'Time to ICU admission (days)', ha='center')\n",
    "fig.text(0.08, 0.5, '# notes', va='center', rotation='vertical')\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.3)\n",
    "               \n",
    "if save:               \n",
    "  fig.savefig(figdir/f'{cohort}_admit_to_icu_cat_binned_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:35:52.090382Z",
     "start_time": "2019-10-20T11:35:51.484548Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Histogram of time between note charttime and ICU admittime\n",
    "plot_df = df[['category', 'note_to_icu']]\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.distplot(plot_df['note_to_icu'], kde=False, ax=ax, bins=80)\n",
    "ax.set_xlabel('Note Charttime to ICU Admittime (days)')\n",
    "ax.set_ylabel('# notes')\n",
    "ax.set_xlim(0, 60)\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_note_to_icu_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:35:56.029771Z",
     "start_time": "2019-10-20T11:35:52.091956Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Histogram of time between note charttime and ICU admittime by Category\n",
    "def plot_period(ax, df, cat):\n",
    "  sns.distplot(df, kde=False, ax=ax, bins=10)\n",
    "  ax.set_xlabel('')\n",
    "  ax.set_ylabel('')\n",
    "  ax.set_title(f\"Note Category: {cat}\")\n",
    "\n",
    "fig, ax = plt.subplots(6, 2, figsize=(20, 50))\n",
    "plot_period(ax[0][0], plot_df.loc[plot_df['category'] == 'Case Management ', ['note_to_icu']], 'Case Management')\n",
    "plot_period(ax[0][1], plot_df.loc[plot_df['category'] == 'Consult', ['note_to_icu']], 'Consult')\n",
    "\n",
    "plot_period(ax[1][0], plot_df.loc[plot_df['category'] == 'General', ['note_to_icu']], 'General')\n",
    "plot_period(ax[1][1], plot_df.loc[plot_df['category'] == 'Nursing', ['note_to_icu']], 'Nursing')\n",
    "\n",
    "plot_period(ax[2][0], plot_df.loc[plot_df['category'] == 'Nursing/other', ['note_to_icu']], 'Nursing/other')\n",
    "plot_period(ax[2][1], plot_df.loc[plot_df['category'] == 'Nutrition', ['note_to_icu']], 'Nutrition')\n",
    "\n",
    "plot_period(ax[3][0], plot_df.loc[plot_df['category'] == 'Pharmacy', ['note_to_icu']], 'Pharmacy')\n",
    "plot_period(ax[3][1], plot_df.loc[plot_df['category'] == 'Physician ', ['note_to_icu',]], 'Physician')\n",
    "\n",
    "plot_period(ax[4][0], plot_df.loc[plot_df['category'] == 'Radiology', ['note_to_icu']], 'Radiology')\n",
    "plot_period(ax[4][1], plot_df.loc[plot_df['category'] == 'Rehab Services', ['note_to_icu']], 'Rehab Services')\n",
    "\n",
    "plot_period(ax[5][0], plot_df.loc[plot_df['category'] == 'Respiratory ', ['note_to_icu']], 'Respiratory')\n",
    "plot_period(ax[5][1], plot_df.loc[plot_df['category'] == 'Social Work', ['note_to_icu']], 'Social Work')\n",
    "\n",
    "fig.text(0.5, 0.11, 'Note Charttime to ICU Admittime (days)', ha='center')\n",
    "fig.text(0.08, 0.5, '# notes', va='center', rotation='vertical')\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.1)\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_note_to_icu_cat_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:35:56.299289Z",
     "start_time": "2019-10-20T11:35:56.031733Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "desc = ['Delayed ICU Admission', 'Imminent ICU Admission']\n",
    "\n",
    "p1 = pd.DataFrame(df.loc[df['imi_adm_label'] != -1].groupby(['imi_adm_label']).size(), columns=['n_notes']).reset_index()\n",
    "p2 = df.loc[df['imi_adm_label'] != -1].groupby(['imi_adm_label'])['hadm_id'].nunique().reset_index()\n",
    "\n",
    "p = p1.merge(p2, on=['imi_adm_label'])\n",
    "p['imi_adm_label'] = desc\n",
    "p = p.reindex([1, 0])\n",
    "p.reset_index(inplace=True, drop=True)\n",
    "\n",
    "plot_df = p.copy()\n",
    "plot_df.rename(columns={'hadm_id':'# Encounters', 'n_notes':'# Notes'}, inplace=True)\n",
    "plot_df = pd.melt(plot_df, id_vars='imi_adm_label', var_name='Legend', value_name='counts')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.barplot(x='imi_adm_label', y='counts', hue='Legend', data=plot_df, ax=ax)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), ha='center')\n",
    "ax.set_xlabel('Class Label')\n",
    "ax.set_ylabel('# notes')\n",
    "\n",
    "for index, row in plot_df.iterrows():\n",
    "    if index < len(plot_df)//2:\n",
    "        ax.text(index-0.13, row['counts']+50, str(row['counts']), color='black', ha='right', va='bottom')\n",
    "    else:\n",
    "        ax.text(index % (len(plot_df)//2)+0.25, row['counts']+50, str(row['counts']), color='black', ha='right', va='bottom')\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_note_class_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Merge Structured and Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:53:17.000526Z",
     "start_time": "2019-10-20T11:53:05.531184Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "common_notes = pd.read_csv(mimic_path/'proc3_notes_mimic_common_labeled.csv', parse_dates=['intime', 'admittime', 'ne_charttime'])\n",
    "common_str = pd.read_csv(mimic_path/'proc2_str_mimic_stats.csv', parse_dates=['ce_charttime'])\n",
    "\n",
    "s, n = set(common_str['hadm_id'].unique()), set(common_notes['hadm_id'].unique())\n",
    "assert(s.symmetric_difference(n) == set())\n",
    "col_order = ['hadm_id', 'intime', 'admittime', 'admit_to_icu'] + list(common_str.columns[2:]) + ['ne_charttime', 'note_to_icu', 'interval', 'category', 'note', 'note_len', 'imi_adm_label']\n",
    "\n",
    "common_str.shape, common_notes.shape, len(col_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:53:17.538410Z",
     "start_time": "2019-10-20T11:53:17.002654Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "common_str.sort_values(by='ce_charttime', inplace=True)\n",
    "common_str.reset_index(inplace=True, drop=True)\n",
    "\n",
    "common_notes.sort_values(by='ne_charttime', inplace=True)\n",
    "common_notes.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:53:18.465205Z",
     "start_time": "2019-10-20T11:53:17.540152Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mimic_df = pd.merge_asof(common_notes, common_str, left_on='ne_charttime', right_on='ce_charttime', by='hadm_id')\n",
    "mimic_df.drop(columns='ce_charttime', inplace=True)\n",
    "mimic_df = mimic_df[col_order]\n",
    "\n",
    "mimic_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:54:04.279286Z",
     "start_time": "2019-10-20T11:53:46.400686Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mimic_df.to_csv(mimic_path/'merged_labeled_mimic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:23:52.754411Z",
     "start_time": "2019-10-16T16:23:52.059911Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_str_df = pd.read_csv(mimic_path/'proc1_str_mimic.csv', parse_dates=['ce_charttime'])\n",
    "all_var_cols = all_str_df.columns[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:24:02.956282Z",
     "start_time": "2019-10-16T16:24:02.927238Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hadms = [100104, 100975, 101511, 111073]\n",
    "dev_str = all_str_df.loc[(all_str_df['hadm_id'].isin(hadms))].reset_index(drop=True)\n",
    "dev_str.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:24:11.382293Z",
     "start_time": "2019-10-16T16:24:11.364838Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def percentile(n):\n",
    "  def percentile_(x):\n",
    "    return x.quantile(n)\n",
    "  percentile_.__name__ = f'percentile_{n*100:2.0f}'\n",
    "  return percentile_\n",
    "\n",
    "def all_change_name(col_name):\n",
    "  if '(' not in col_name:\n",
    "    return col_name\n",
    "  cols = literal_eval(col_name)\n",
    "  return f'{cols[2]}_{cols[1]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:24:14.022784Z",
     "start_time": "2019-10-16T16:24:14.007286Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "statistics = ['min', 'mean', 'median', 'std', 'var', 'kurt', 'skew', percentile(0.25), percentile(0.75), stats.iqr, 'max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:24:22.720203Z",
     "start_time": "2019-10-16T16:24:22.701204Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_var_df = dev_str[all_var_cols] # save the original vals for later\n",
    "dev_str.set_index('ce_charttime', inplace=True) # set charttime index for 24h rolling\n",
    "dev_str.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:28:32.855893Z",
     "start_time": "2019-10-16T16:24:34.750709Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_stats_df = dev_str.groupby('hadm_id')[all_var_cols].rolling('24h').agg(statistics)\n",
    "all_stats_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:28:35.993269Z",
     "start_time": "2019-10-16T16:28:32.857503Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dev_str = pd.DataFrame(all_stats_df.to_records()) # flatten the resulting dataframe\n",
    "dev_str.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:28:45.917969Z",
     "start_time": "2019-10-16T16:28:45.889693Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dev_str = dev_str.iloc[:, :(2 + (len(statistics) * len(all_var_cols)))] # drop duplicate columns resulting from rolling\n",
    "dev_str.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:28:57.912219Z",
     "start_time": "2019-10-16T16:28:57.888078Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dev_str.rename(columns=all_change_name, inplace=True) # rename columns\n",
    "dev_str.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:29:04.178961Z",
     "start_time": "2019-10-16T16:29:04.161131Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dev_str = pd.concat([dev_str, all_var_df], axis=1) # add the original vals back\n",
    "dev_str.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:29:13.163391Z",
     "start_time": "2019-10-16T16:29:13.144337Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# reorder vars such that the columns are var, var_stat...\n",
    "stats_cols = dev_str.columns[2:]\n",
    "all_cols = []\n",
    "for var in all_var_cols:\n",
    "  all_cols.append(var)\n",
    "  for stat in stats_cols:\n",
    "    if f'{var}_' in stat:\n",
    "      all_cols.append(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:29:18.173555Z",
     "start_time": "2019-10-16T16:29:18.151100Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "order = list(dev_str.columns[:2]) + all_cols\n",
    "dev_str = dev_str[order]\n",
    "dev_str.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:29:19.679110Z",
     "start_time": "2019-10-16T16:29:19.633453Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dev_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T02:06:14.232466Z",
     "start_time": "2019-10-09T02:06:14.055129Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dev_str.to_csv(mimic_path/'sample_str.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T17:10:51.165288Z",
     "start_time": "2019-10-08T17:08:51.719096Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols = list(str_df.columns[1:]) # get the cols to merge (everything except hadm_id)\n",
    "final_dfs = [] \n",
    "\n",
    "grouped = notes_df.groupby('hadm_id') # get groups of encounter ids\n",
    "for name, group in grouped:\n",
    "  final_df = group.copy().reset_index(drop=True) # make a copy of notes for that encounter\n",
    "  for col in cols:\n",
    "    final_df[col] = np.nan # set the values to nan\n",
    "\n",
    "  idx = 0 # index to track the final row in the given encounter\n",
    "  for i, note_row in final_df.iterrows():\n",
    "    ne = note_row['ne_charttime']\n",
    "    sub = str_df.loc[(str_df['hadm_id'] == name)].reset_index(drop=True) # get the df corresponding to the ecounter\n",
    "    for j, str_row in sub.iterrows():\n",
    "      ce = str_row['ce_charttime']\n",
    "      if ne < ce: # if the variable charttime < note charttime\n",
    "        idx += 1\n",
    "        \n",
    "        # grab the previous values for the variables and break\n",
    "        for col in cols:\n",
    "          final_df.iloc[i, final_df.columns.get_loc(col)] = sub.iloc[j-1][col]          \n",
    "        break               \n",
    "  pdb.set_trace()\n",
    "  # get the last value in the df for the variables\n",
    "  for col in cols:\n",
    "    final_df.iloc[idx, final_df.columns.get_loc(col)] = sub.iloc[-1][col]\n",
    "  \n",
    "  final_dfs.append(final_df) # append the df to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T17:03:04.193531Z",
     "start_time": "2019-10-08T17:03:03.997303Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# cat the list to get final df and reset index\n",
    "mimic_df = pd.concat(final_dfs)\n",
    "mimic_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T16:48:33.701761Z",
     "start_time": "2019-10-08T16:48:32.404207Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dev = True\n",
    "if dev:\n",
    "  columns = ['hadm_id', 'ce_charttime', 'hr', 'resp', 'magnesium']\n",
    "  var_cols = columns[2:]\n",
    "  working_hadms = [196673, 197006]\n",
    "  str_df = pd.read_csv(mimic_path/'proc1_str_mimic.csv', usecols=columns, parse_dates=['ce_charttime'])\n",
    "else:\n",
    "  working_hadms = str_df['hadm_id'].unique()  \n",
    "  str_df = pd.read_csv(mimic_path/'proc1_str_mimic.csv', parse_dates=['ce_charttime'])\n",
    "  var_cols = str_df.columns[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T16:48:43.151995Z",
     "start_time": "2019-10-08T16:48:43.066502Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dev_str = str_df.loc[(str_df['hadm_id'].isin(working_hadms))].reset_index(drop=True)\n",
    "dev_notes = notes_df.loc[(notes_df['hadm_id'].isin(working_hadms))][['hadm_id', 'ne_charttime', 'note']].reset_index(drop=True)\n",
    "dev_notes.sort_values('ne_charttime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T14:55:31.428892Z",
     "start_time": "2019-10-08T14:55:25.755474Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove redundant info by filling in each time column with the value of the var\n",
    "dev_str = dev_str.groupby(['hadm_id','ce_charttime']).sum(min_count = 1).reset_index()\n",
    "\n",
    "# groupby ffill \n",
    "dev_str = dev_str.groupby(['hadm_id'], as_index=False).apply(lambda group: group.ffill())\n",
    "var_cols = dev_str.columns[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T14:55:37.061817Z",
     "start_time": "2019-10-08T14:55:37.041319Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def percentile(n):\n",
    "  def percentile_(x):\n",
    "    return x.quantile(n)\n",
    "  percentile_.__name__ = f'percentile_{n*100:2.0f}'\n",
    "  return percentile_\n",
    "\n",
    "def get_stats(df, var_cols, statistics):\n",
    "  df.set_index('ce_charttime', inplace=True)\n",
    "  stats_dfs = []\n",
    "\n",
    "  for var in var_cols:\n",
    "    stats_df = df.groupby('hadm_id')[var].rolling('24h').agg(statistics).reset_index(drop=True)\n",
    "    stats_df.columns = [f'{var}_{col}' for col in stats_df.columns]\n",
    "    stats_df = pd.concat([df[var].reset_index(drop=True), stats_df], axis=1)\n",
    "    stats_dfs.append(stats_df)\n",
    "    \n",
    "  df.reset_index(inplace=True)\n",
    "  df.drop(var_cols, inplace=True, axis=1)\n",
    "  return pd.concat([df, *stats_dfs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T14:57:19.619907Z",
     "start_time": "2019-10-08T14:55:37.698805Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "statistics = ['min', 'mean', 'median', 'std', 'var', 'kurt', 'skew', percentile(0.25), percentile(0.75), stats.iqr, 'max']\n",
    "\n",
    "dev_str = get_stats(dev_str.copy(), var_cols, statistics)\n",
    "move = ['hadm_id', 'ce_charttime']\n",
    "order = move + (dev_str.columns.drop(move).tolist())\n",
    "dev_str = dev_str[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T14:49:37.198972Z",
     "start_time": "2019-10-08T14:49:37.177071Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_str = dev_str.loc[(dev_str['hadm_id'] == 196673)].reset_index(drop=True)\n",
    "df_notes = dev_notes.loc[(dev_notes['hadm_id'] == 196673)].reset_index(drop=True)\n",
    "df_str.shape, df_notes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T14:49:37.853122Z",
     "start_time": "2019-10-08T14:49:37.808969Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T14:49:50.726279Z",
     "start_time": "2019-10-08T14:49:50.705128Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T14:42:15.338593Z",
     "start_time": "2019-10-08T14:42:14.838625Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols = list(dev_str.columns[1:]) # get the cols to merge (everything except hadm_id)\n",
    "final_dfs = [] \n",
    "\n",
    "grouped = dev_notes.groupby('hadm_id') # get groups of encounter ids\n",
    "for name, group in grouped:\n",
    "  final_df = group.copy().reset_index(drop=True) # make a copy of notes for that encounter\n",
    "  for col in cols:\n",
    "    final_df[col] = np.nan # set the values to nan\n",
    "\n",
    "  idx = 0 # index to track the final row in the given encounter\n",
    "  for i, note_row in final_df.iterrows():\n",
    "    ne = note_row['ne_charttime']\n",
    "    sub = dev_str.loc[(dev_str['hadm_id'] == name)].reset_index(drop=True) # get the df corresponding to the ecounter\n",
    "    for j, str_row in sub.iterrows():\n",
    "      ce = str_row['ce_charttime']\n",
    "      if ne < ce: # if the variable charttime < note charttime\n",
    "        idx += 1\n",
    "        \n",
    "        # grab the previous values for the variables and break\n",
    "        for col in cols:\n",
    "          final_df.iloc[i, final_df.columns.get_loc(col)] = sub.iloc[j-1][col]          \n",
    "        break               \n",
    "\n",
    "  # get the last value in the df for the variables\n",
    "  for col in cols:\n",
    "    final_df.iloc[idx, final_df.columns.get_loc(col)] = sub.iloc[-1][col]\n",
    "  \n",
    "  final_dfs.append(final_df) # append the df to the list\n",
    "\n",
    "# cat the list to get final df and reset index\n",
    "final_df = pd.concat(final_dfs)\n",
    "final_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T14:46:33.644063Z",
     "start_time": "2019-10-08T14:46:33.617803Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "move = ['ne_charttime', 'note']\n",
    "order = (final_df.columns.drop(move).tolist()) + move \n",
    "final_df = final_df[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T14:46:34.640605Z",
     "start_time": "2019-10-08T14:46:34.601151Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "final_df.loc[(final_df['hadm_id'] == 196673)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T02:46:13.523738Z",
     "start_time": "2019-11-17T02:46:13.263615Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = {\n",
    "  'hadm_id': [140694, 140694, 140694, 171544, 171544, 171544, 171544, 171544,],\n",
    "  'charttime': [pd.to_datetime('2121-08-12 19:00:00'), pd.to_datetime('2121-08-12 19:45:00'), pd.to_datetime('2121-08-12 20:00:00'), pd.to_datetime('2153-09-06 14:11:00'), pd.to_datetime('2153-09-06 17:30:00'), pd.to_datetime('2153-09-06 17:35:00'), pd.to_datetime('2153-09-06 17:40:00'), pd.to_datetime('2153-09-06 17:45:00')],\n",
    "  'hr': [67.0, 68.0, 70.0, 80.0, 80.0, 80.0, 76.0, 79.0],\n",
    "  'sbp': [102.0, 135.0, 153.0, 114.0, 114.0, 114.0, 115.0, 117.0],\n",
    "  'dbp': [75.0, 68.0, 94.0, 50.0, 50.0, 50.0, 51.0, 53.0],\n",
    "}\n",
    "\n",
    "dn = {\n",
    "  'hadm_id': [140694, 140694, 140694, 171544, 171544, 171544, 171544, 171544,],\n",
    "  'charttime': [pd.to_datetime('2121-08-10 20:32:00'), pd.to_datetime('2121-08-11 12:57:00'), pd.to_datetime('2121-08-11 15:18:00'), pd.to_datetime('2153-09-05 15:09:00'), pd.to_datetime('2153-09-05 17:43:00'), pd.to_datetime('2153-09-06 10:36:00'), pd.to_datetime('2153-09-06 15:55:00'), pd.to_datetime('2153-09-06 17:12:00')],\n",
    "  'note': ['some text1', 'some text2', 'some text3', 'some text4', 'some text5', 'some text6', 'some text7', 'some text8']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T02:46:14.655376Z",
     "start_time": "2019-11-17T02:46:14.609790Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = pd.DataFrame(ds)\n",
    "dn = pd.DataFrame(dn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T02:46:15.643380Z",
     "start_time": "2019-11-17T02:46:15.591571Z"
    }
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T02:46:21.822160Z",
     "start_time": "2019-11-17T02:46:21.773524Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T00:53:54.257448Z",
     "start_time": "2019-11-17T00:53:54.187321Z"
    }
   },
   "outputs": [],
   "source": [
    "final = {\n",
    "  'hadm_id': [140694, 140694, 140694, 140694, 140694, 140694, 171544, 171544, 171544, 171544, 171544, 171544, 171544, 171544, 171544, 171544],\n",
    "  'charttime': [pd.to_datetime('2121-08-10 20:32:00'), pd.to_datetime('2121-08-11 12:57:00'), pd.to_datetime('2121-08-11 15:18:00'), pd.to_datetime('2121-08-12 19:00:00'), pd.to_datetime('2121-08-12 19:45:00'), pd.to_datetime('2121-08-12 20:00:00'), pd.to_datetime('2153-09-05 15:09:00'), pd.to_datetime('2153-09-05 17:43:00'), pd.to_datetime('2153-09-06 10:36:00'), pd.to_datetime('2153-09-06 14:11:00'), pd.to_datetime('2153-09-06 15:55:00'), pd.to_datetime('2153-09-06 17:12:00'), pd.to_datetime('2153-09-06 17:30:00'), pd.to_datetime('2153-09-06 17:35:00'), pd.to_datetime('2153-09-06 17:40:00'), pd.to_datetime('2153-09-06 17:45:00')],\n",
    "  'ce_charttime': [pd.NaT, pd.NaT, pd.NaT, pd.to_datetime('2121-08-12 19:00:00'), pd.to_datetime('2121-08-12 19:45:00'), pd.to_datetime('2121-08-12 20:00:00'), pd.NaT, pd.NaT, pd.NaT, pd.to_datetime('2153-09-06 14:11:00'), pd.NaT, pd.NaT, pd.to_datetime('2153-09-06 17:30:00'), pd.to_datetime('2153-09-06 17:35:00'), pd.to_datetime('2153-09-06 17:40:00'), pd.to_datetime('2153-09-06 17:45:00')],\n",
    "  'hr': [np.nan, np.nan, np.nan, 67.0, 68.0, 70.0, np.nan, np.nan, np.nan, 80.0, np.nan, np.nan, 80.0, 80.0, 76.0, 76.0],\n",
    "  'sbp': [np.nan, np.nan, np.nan, 102.0, 135.0, 153.0, np.nan, np.nan, np.nan, 114.0, np.nan, np.nan, 114.0, 114.0, 115.0, 117.0],\n",
    "  'dbp': [np.nan, np.nan, np.nan, 75.0, 68.0, 94.0, np.nan, np.nan, np.nan, 50.0, np.nan, np.nan, 50.0, 50.0, 51.0, 53.0],\n",
    "  'ne_charttime': [pd.to_datetime('2121-08-10 20:32:00'), pd.to_datetime('2121-08-11 12:57:00'), pd.to_datetime('2121-08-11 15:18:00'), pd.NaT, pd.NaT, pd.NaT, pd.to_datetime('2153-09-05 15:09:00'), pd.to_datetime('2153-09-05 17:43:00'), pd.to_datetime('2153-09-06 10:36:00'), pd.NaT, pd.to_datetime('2153-09-06 15:55:00'), pd.to_datetime('2153-09-06 17:12:00'), pd.NaT, pd.NaT, pd.NaT, pd.NaT],\n",
    "  'note': ['some text1', 'some text2', 'some text3', np.nan, np.nan, np.nan, 'some text4', 'some text5', 'some text6', np.nan, 'some text7', 'some text8', np.nan, np.nan, np.nan, np.nan] \n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
