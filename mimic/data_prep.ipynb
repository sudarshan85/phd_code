{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC Notes and Structured Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:25:18.397037Z",
     "start_time": "2019-12-18T19:25:18.363610Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:25:20.376094Z",
     "start_time": "2019-12-18T19:25:18.413382Z"
    }
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from ast import literal_eval\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:25:20.408464Z",
     "start_time": "2019-12-18T19:25:20.379035Z"
    }
   },
   "outputs": [],
   "source": [
    "path = Path('data')\n",
    "workdir = path/'workdir'\n",
    "figdir = workdir/'figures'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:25:20.436316Z",
     "start_time": "2019-12-18T19:25:20.410945Z"
    }
   },
   "outputs": [],
   "source": [
    "def change_name(col_name):\n",
    "  if '(' not in col_name:\n",
    "    return col_name\n",
    "  cols = literal_eval(col_name)\n",
    "  return f'{cols[0]}_{cols[1]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:25:20.465838Z",
     "start_time": "2019-12-18T19:25:20.438198Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_interval(x):\n",
    "  if pd.isnull(x):\n",
    "    return -1\n",
    "  \n",
    "  if 0 < x <= 1:\n",
    "    return 0\n",
    "  elif 1 < x <= 2:\n",
    "    return 1\n",
    "  elif 2 < x <= 3:\n",
    "    return 2\n",
    "  elif 3 < x <= 4:\n",
    "    return 3\n",
    "  elif 4 < x <= 5:\n",
    "    return 4\n",
    "  elif 5 < x <= 6:\n",
    "    return 5\n",
    "  elif 6 < x <= 7:\n",
    "    return 6\n",
    "  elif 7 < x <= 8:\n",
    "    return 7\n",
    "  elif 8 < x <= 9:\n",
    "    return 8\n",
    "  elif 9 < x <= 10:\n",
    "    return 9\n",
    "  elif 10 < x <= 11:\n",
    "    return 10\n",
    "  elif 11 < x <= 12:\n",
    "    return 11\n",
    "  elif 12 < x <= 13:\n",
    "    return 12\n",
    "  elif 13 < x <= 14:\n",
    "    return 13\n",
    "  elif 14 < x <= 15:\n",
    "    return 14\n",
    "  else:\n",
    "    return 15\n",
    "\n",
    "def icu_adm_label(x):\n",
    "  if 0 <= x <= 1:\n",
    "    return -1 # unused notes due to data leakage\n",
    "  elif 1 < x <= 3:\n",
    "    return 1 # imminent ICU admission\n",
    "  elif 3 < x <= 5:\n",
    "    return -1 # unused notes due to data leakage\n",
    "  else:\n",
    "    return 0 # delayed ICU admission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:04:03.050651Z",
     "start_time": "2019-12-18T18:03:59.024222Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_df = pd.read_csv(path/'unstructured_raw.csv', parse_dates=['intime', 'admittime', 'charttime'])\n",
    "notes_df.drop_duplicates(inplace=True)\n",
    "\n",
    "vitals_df = pd.read_csv(path/'structured_raw_with_meta.csv', parse_dates=['charttime'])\n",
    "vitals_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:04:03.654975Z",
     "start_time": "2019-12-18T18:04:03.576465Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_hadms = notes_df['hadm_id'].unique()\n",
    "vitals_hadms = vitals_df['hadm_id'].unique()\n",
    "\n",
    "# Extract common `hadm_id` and filter out those that do not appear in both dfs\n",
    "common_hadms = set(vitals_df['hadm_id'].unique()).intersection(notes_df['hadm_id'].unique())\n",
    "\n",
    "print(f\"Number of encounters that definitely have structured vitals data: {len(vitals_hadms)}\")\n",
    "print(f\"Number of encounters that definitely have clinical notes: {len(notes_hadms)}\")\n",
    "print(f\"Number of encounters that have both vitals and clinical notes: {len(common_hadms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:04:04.039910Z",
     "start_time": "2019-12-18T18:04:03.656919Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_common = notes_df[notes_df['hadm_id'].isin(common_hadms)].reset_index(drop=True)\n",
    "vitals_common = vitals_df[vitals_df['hadm_id'].isin(common_hadms)].reset_index(drop=True)\n",
    "\n",
    "# sanity check\n",
    "s, n = set(vitals_common['hadm_id'].unique()), set(notes_common['hadm_id'].unique())\n",
    "assert(s.symmetric_difference(n) == set())\n",
    "\n",
    "vitals_common.shape, notes_common.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:04:08.660251Z",
     "start_time": "2019-12-18T18:04:04.041693Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_common['note'] = notes_common['category'].str.cat(notes_common['description'], sep='\\n')\n",
    "notes_common['note'] = notes_common['note'].str.cat(notes_common['text'], sep='\\n')\n",
    "notes_common.drop(columns=['category', 'description', 'text'], inplace=True) \n",
    "\n",
    "notes_common = pd.DataFrame(notes_common.groupby(['hadm_id', 'intime', 'admittime', 'charttime'])['note'].apply('\\n'.join)).reset_index()\n",
    "notes_common['category'] = notes_common['note'].apply(lambda x: x.split('\\n')[0])\n",
    "notes_common.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:04:27.404871Z",
     "start_time": "2019-12-18T18:04:08.706291Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove redundant info by filling in each time column with the value of the var\n",
    "vitals_common = vitals_common.groupby(['hadm_id','charttime','intime','admittime']).sum(min_count = 1).reset_index()\n",
    "# Groupby ffill \n",
    "vitals_common = vitals_common.groupby(['hadm_id'], as_index=False).apply(lambda group: group.ffill())\n",
    "# Groupby bfill \n",
    "vitals_common = vitals_common.groupby(['hadm_id'], as_index=False).apply(lambda group: group.bfill())\n",
    "vitals_common = vitals_common.fillna(vitals_common.median())\n",
    "vitals_common.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:04:32.349074Z",
     "start_time": "2019-12-18T18:04:27.407150Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_common.to_csv(path/'unstructured_notes_proc.csv', index=False)\n",
    "vitals_common.to_csv(path/'structured_vitals_proc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Compute Statistics Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:04:32.769944Z",
     "start_time": "2019-12-18T18:04:32.351580Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vitals_common = pd.read_csv(path/'structured_vitals_proc.csv', parse_dates=['charttime'])\n",
    "vitals_common.drop(['intime', 'admittime'], inplace=True, axis=1)\n",
    "vitals_common.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:04:32.810159Z",
     "start_time": "2019-12-18T18:04:32.771701Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = pd.DataFrame(vitals_common.groupby('hadm_id').size(), columns=['size']).reset_index()\n",
    "hadms = x.loc[(x['size'] >= 10) & (x['size'] <= 20)].sample(5)['hadm_id'].tolist()\n",
    "x.loc[x['hadm_id'].isin(hadms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:04:32.835800Z",
     "start_time": "2019-12-18T18:04:32.811459Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dev_subset = vitals_common.loc[(vitals_common['hadm_id'].isin(hadms))].reset_index(drop=True)\n",
    "print(dev_subset.shape)\n",
    "print(dev_subset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:04:32.856700Z",
     "start_time": "2019-12-18T18:04:32.837143Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var_cols = dev_subset.columns[2:]\n",
    "print(len(var_cols))\n",
    "running_stats = ['min', 'mean', 'median', 'std', 'max']\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:04:33.077347Z",
     "start_time": "2019-12-18T18:04:32.858208Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for hadm_id, group_df in tqdm(dev_subset.groupby('hadm_id'), desc='Encounters'):\n",
    "  df = group_df.copy()\n",
    "  var_df = df[var_cols].reset_index(drop=True) # save the original vals for later\n",
    "  \n",
    "  df.set_index('charttime', inplace=True) # set charttime as index for rolling 24h\n",
    "  stats_df = df[var_cols].rolling('24h').agg(running_stats)\n",
    "  \n",
    "  df = pd.DataFrame(stats_df.to_records()) # flatten the resulting dataframe\n",
    "  df.insert(loc=1, column='hadm_id', value=hadm_id)\n",
    "  \n",
    "  df.rename(columns=change_name, inplace=True) # rename columns\n",
    "  df = pd.concat([df, var_df], axis=1) # add the original vals back\n",
    "  \n",
    "  # reorder vars such that the columns are var, var_stat...\n",
    "  stats_cols = df.columns[2:]\n",
    "  all_cols = []\n",
    "  for var in var_cols:\n",
    "    all_cols.append(var)\n",
    "    for stat in stats_cols:\n",
    "      if f'{var}_' in stat:\n",
    "        all_cols.append(stat)\n",
    "        \n",
    "  order = list(df.columns[:2]) + all_cols\n",
    "  df = df[order]\n",
    "  dfs.append(df)\n",
    "\n",
    "dev_subset_stats = pd.concat(dfs)\n",
    "dev_subset_stats.reset_index(drop=True, inplace=True)\n",
    "dev_subset_stats['charttime'] = pd.to_datetime(dev_subset_stats['charttime'])\n",
    "\n",
    "std_cols = [col for col in dev_subset_stats.columns if 'std' in col]\n",
    "dev_subset_stats[std_cols] = dev_subset_stats[std_cols].fillna(0)\n",
    "\n",
    "dev_subset_stats = dev_subset_stats[['hadm_id', 'charttime'] + list(dev_subset_stats.columns[2:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:04:33.096415Z",
     "start_time": "2019-12-18T18:04:33.078908Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(dev_subset_stats.shape)\n",
    "dev_subset_stats.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep data for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:09:11.442886Z",
     "start_time": "2019-12-18T18:09:08.324642Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_common = pd.read_csv(path/'unstructured_notes_proc.csv', parse_dates=['intime', 'admittime', 'charttime'])\n",
    "notes_common.drop(columns=['category'], inplace=True)\n",
    "\n",
    "vitals_common_solo = pd.read_csv(path/'structured_vitals_proc.csv', parse_dates=['intime', 'admittime', 'charttime'])\n",
    "vitals_common_stats = pd.read_csv(path/'structured_vitals_stats.csv', parse_dates=['charttime'])\n",
    "\n",
    "# pickle.dump(list(vitals_common_solo.columns[2:]), open(path/'vitals_solo_cols.pkl', 'wb'))\n",
    "# pickle.dump(list(vitals_common_stats.columns[2:]), open(path/'vitals_stats_cols.pkl', 'wb'))\n",
    "\n",
    "print(vitals_common_solo.shape, vitals_common_stats.shape, notes_common.shape, vitals_common_solo['hadm_id'].nunique(), vitals_common_stats['hadm_id'].nunique(), notes_common['hadm_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Merge Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:09:29.087057Z",
     "start_time": "2019-12-18T18:09:29.038299Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "main = ['hadm_id', 'charttime']\n",
    "sub1 = ['hr', 'hr_max', 'temp', 'temp_min', 'glucose', 'glucose_std', 'map', 'map_median']\n",
    "sub2 = ['admittime', 'intime', 'note']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:09:29.150789Z",
     "start_time": "2019-12-18T18:09:29.090231Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = pd.DataFrame(notes_common.groupby('hadm_id').size(), columns=['size']).reset_index()\n",
    "hadms = x.loc[(x['size'] >= 2) & (x['size'] <= 15)].sample(5)['hadm_id'].tolist()\n",
    "\n",
    "subset_stats = vitals_common_stats.loc[(vitals_common_stats['hadm_id'].isin(hadms))][main + sub1].copy().reset_index(drop=True)\n",
    "\n",
    "subset_notes = notes_common.loc[(notes_common['hadm_id'].isin(hadms))][main + sub2].copy().reset_index(drop=True)\n",
    "subset_stats.shape, subset_stats['hadm_id'].nunique(), subset_notes.shape, subset_notes['hadm_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:09:34.743855Z",
     "start_time": "2019-12-18T18:09:34.691265Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(subset_stats.groupby('hadm_id').size(), columns=['size']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:09:35.527282Z",
     "start_time": "2019-12-18T18:09:35.475428Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(subset_notes.groupby('hadm_id').size(), columns=['size']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:09:38.178485Z",
     "start_time": "2019-12-18T18:09:38.101294Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "subset_stats.sort_values(by='charttime', inplace=True)\n",
    "subset_stats.reset_index(inplace=True, drop=True)\n",
    "\n",
    "subset_notes.sort_values(by='charttime', inplace=True)\n",
    "subset_notes.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df = pd.merge_asof(subset_notes, subset_stats, left_on='charttime', right_on='charttime', by='hadm_id')\n",
    "\n",
    "cols = ['hr', 'hr_max', 'temp', 'temp_min', 'glucose', 'glucose_std', 'map', 'map_median']\n",
    "\n",
    "df = df.groupby(['hadm_id'], as_index=False).apply(lambda group: group.bfill())\n",
    "df[cols] = df[cols].fillna(df[cols].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:09:42.315466Z",
     "start_time": "2019-12-18T18:09:42.273024Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:09:42.351831Z",
     "start_time": "2019-12-18T18:09:42.318325Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i += 1\n",
    "print(hadms[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:09:42.393490Z",
     "start_time": "2019-12-18T18:09:42.354469Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "subset_stats[subset_stats['hadm_id'] == hadms[i]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:09:42.425974Z",
     "start_time": "2019-12-18T18:09:42.395391Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "subset_notes[subset_notes['hadm_id'] == hadms[i]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:09:42.460401Z",
     "start_time": "2019-12-18T18:09:42.427623Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[df['hadm_id'] == hadms[i]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:09:54.081397Z",
     "start_time": "2019-12-18T18:09:54.033903Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Final Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:10:26.267894Z",
     "start_time": "2019-12-18T18:10:18.862315Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vitals_common_stats.sort_values(by='charttime', inplace=True)\n",
    "vitals_common_stats.reset_index(inplace=True, drop=True)\n",
    "\n",
    "notes_common.sort_values(by='charttime', inplace=True)\n",
    "notes_common.reset_index(inplace=True, drop=True)\n",
    "\n",
    "mm_notes_vitals = pd.merge_asof(notes_common, vitals_common_stats, left_on='charttime', right_on='charttime', by='hadm_id')\n",
    "\n",
    "str_cols = pickle.load(open(path/'vitals_stats_cols.pkl', 'rb'))\n",
    "\n",
    "mm_notes_vitals = mm_notes_vitals.groupby(['hadm_id'], as_index=False).apply(lambda group: group.bfill())\n",
    "mm_notes_vitals[str_cols] = mm_notes_vitals[str_cols].fillna(mm_notes_vitals[str_cols].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:10:27.197158Z",
     "start_time": "2019-12-18T18:10:27.128209Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = pd.DataFrame(mm_notes_vitals.isna().sum(), columns=['sum']).reset_index()\n",
    "assert(x['sum'].sum() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T18:16:21.513160Z",
     "start_time": "2019-12-18T18:16:16.501188Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mm_notes_vitals.to_csv(path/'mm_notes_vitals_proc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:25:27.717563Z",
     "start_time": "2019-12-18T19:25:24.705793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270288, 12) (53270, 5) (53270, 53)\n"
     ]
    }
   ],
   "source": [
    "notes_common = pd.read_csv(path/'unstructured_notes_proc.csv', parse_dates=['intime', 'admittime', 'charttime'])\n",
    "notes_common.drop(columns=['category'], inplace=True)\n",
    "\n",
    "vitals_common_solo = pd.read_csv(path/'structured_vitals_proc.csv', parse_dates=['intime', 'admittime', 'charttime'])\n",
    "mm_notes_vitals = pd.read_csv(path/'mm_notes_vitals_proc.csv', parse_dates=['intime', 'admittime', 'charttime'])\n",
    "\n",
    "drop_cols = ['charttime', 'intime', 'admittime', 'admit_to_icu', 'chart_to_icu', 'interval']\n",
    "print(vitals_common_solo.shape, notes_common.shape, mm_notes_vitals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:25:27.747237Z",
     "start_time": "2019-12-18T19:25:27.719869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['hadm_id', 'charttime', 'intime', 'admittime', 'hr', 'sbp', 'dbp',\n",
      "       'map', 'resp', 'temp', 'spo2', 'glucose'],\n",
      "      dtype='object')\n",
      "Index(['hadm_id', 'intime', 'admittime', 'charttime', 'note'], dtype='object')\n",
      "Index(['hadm_id', 'intime', 'admittime', 'charttime', 'note', 'hr', 'hr_min',\n",
      "       'hr_mean', 'hr_median', 'hr_std', 'hr_max', 'sbp', 'sbp_min',\n",
      "       'sbp_mean', 'sbp_median', 'sbp_std', 'sbp_max', 'dbp', 'dbp_min',\n",
      "       'dbp_mean', 'dbp_median', 'dbp_std', 'dbp_max', 'map', 'map_min',\n",
      "       'map_mean', 'map_median', 'map_std', 'map_max', 'resp', 'resp_min',\n",
      "       'resp_mean', 'resp_median', 'resp_std', 'resp_max', 'temp', 'temp_min',\n",
      "       'temp_mean', 'temp_median', 'temp_std', 'temp_max', 'spo2', 'spo2_min',\n",
      "       'spo2_mean', 'spo2_median', 'spo2_std', 'spo2_max', 'glucose',\n",
      "       'glucose_min', 'glucose_mean', 'glucose_median', 'glucose_std',\n",
      "       'glucose_max'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(vitals_common_solo.columns)\n",
    "print(notes_common.columns)\n",
    "print(mm_notes_vitals.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:25:58.731514Z",
     "start_time": "2019-12-18T19:25:58.548964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((53270, 9), 8254)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes_common['admit_to_icu'] = (notes_common['intime'] - notes_common['admittime'])/np.timedelta64(1, 'D')\n",
    "notes_common['chart_to_icu'] = (notes_common['intime'] - notes_common['charttime'])/np.timedelta64(1, 'D')\n",
    "\n",
    "notes_common['interval'] = notes_common['chart_to_icu'].apply(data_interval)\n",
    "notes_common['imi_adm_label'] = notes_common['interval'].apply(icu_adm_label)\n",
    "notes_common.shape, notes_common['hadm_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:25:59.223358Z",
     "start_time": "2019-12-18T19:25:58.733102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((270288, 16), 8254)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vitals_common_solo['admit_to_icu'] = (vitals_common_solo['intime'] - vitals_common_solo['admittime'])/np.timedelta64(1, 'D')\n",
    "vitals_common_solo['chart_to_icu'] = (vitals_common_solo['intime'] - vitals_common_solo['charttime'])/np.timedelta64(1, 'D')\n",
    "\n",
    "vitals_common_solo['interval'] = vitals_common_solo['chart_to_icu'].apply(data_interval)\n",
    "vitals_common_solo['imi_adm_label'] = vitals_common_solo['interval'].apply(icu_adm_label)\n",
    "vitals_common_solo.shape, vitals_common_solo['hadm_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:25:59.341389Z",
     "start_time": "2019-12-18T19:25:59.225134Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((53270, 57), 8254)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_notes_vitals['admit_to_icu'] = (mm_notes_vitals['intime'] - mm_notes_vitals['admittime'])/np.timedelta64(1, 'D')\n",
    "mm_notes_vitals['chart_to_icu'] = (mm_notes_vitals['intime'] - mm_notes_vitals['charttime'])/np.timedelta64(1, 'D')\n",
    "\n",
    "mm_notes_vitals['interval'] = mm_notes_vitals['chart_to_icu'].apply(data_interval)\n",
    "mm_notes_vitals['imi_adm_label'] = mm_notes_vitals['interval'].apply(icu_adm_label)\n",
    "mm_notes_vitals.shape, mm_notes_vitals['hadm_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:29:49.163582Z",
     "start_time": "2019-12-18T19:29:49.091282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53270, 3) 8254\n",
      "(270288, 10) 8254\n",
      "(53270, 51) 8254\n"
     ]
    }
   ],
   "source": [
    "print(notes_common.shape, notes_common['hadm_id'].nunique())\n",
    "print(vitals_common_solo.shape, vitals_common_solo['hadm_id'].nunique())\n",
    "print(mm_notes_vitals.shape, mm_notes_vitals['hadm_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:26:21.320435Z",
     "start_time": "2019-12-18T19:26:21.042611Z"
    }
   },
   "outputs": [],
   "source": [
    "notes_common.drop(drop_cols, axis=1, inplace=True)\n",
    "vitals_common_solo.drop(drop_cols, axis=1, inplace=True)\n",
    "mm_notes_vitals.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:26:23.081304Z",
     "start_time": "2019-12-18T19:26:23.032875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['hadm_id', 'hr', 'sbp', 'dbp', 'map', 'resp', 'temp', 'spo2', 'glucose',\n",
      "       'imi_adm_label'],\n",
      "      dtype='object')\n",
      "Index(['hadm_id', 'note', 'imi_adm_label'], dtype='object')\n",
      "Index(['hadm_id', 'note', 'hr', 'hr_min', 'hr_mean', 'hr_median', 'hr_std',\n",
      "       'hr_max', 'sbp', 'sbp_min', 'sbp_mean', 'sbp_median', 'sbp_std',\n",
      "       'sbp_max', 'dbp', 'dbp_min', 'dbp_mean', 'dbp_median', 'dbp_std',\n",
      "       'dbp_max', 'map', 'map_min', 'map_mean', 'map_median', 'map_std',\n",
      "       'map_max', 'resp', 'resp_min', 'resp_mean', 'resp_median', 'resp_std',\n",
      "       'resp_max', 'temp', 'temp_min', 'temp_mean', 'temp_median', 'temp_std',\n",
      "       'temp_max', 'spo2', 'spo2_min', 'spo2_mean', 'spo2_median', 'spo2_std',\n",
      "       'spo2_max', 'glucose', 'glucose_min', 'glucose_mean', 'glucose_median',\n",
      "       'glucose_std', 'glucose_max', 'imi_adm_label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(vitals_common_solo.columns)\n",
    "print(notes_common.columns)\n",
    "print(mm_notes_vitals.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T19:29:07.038746Z",
     "start_time": "2019-12-18T19:28:57.949023Z"
    }
   },
   "outputs": [],
   "source": [
    "vitals_common_solo.to_csv(path/'modelready_structured.csv', index=False)\n",
    "notes_common.to_csv(path/'modelready_unstructured.csv', index=False)\n",
    "mm_notes_vitals.to_csv(path/'modelready_mm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Cohort: **notes_all**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Read in all **notes_all** and subset it to get all the data with label not equal to -1 (only data used for modeling). Then get the unique ``hadm_id``'s within that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:34:57.639816Z",
     "start_time": "2019-11-08T11:34:55.210139Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_df = pd.read_csv(path/'notes_all_proc.csv', parse_dates=['intime', 'admittime', 'ne_charttime'])\n",
    "model_notes_df = notes_df[notes_df['imi_adm_label'] != -1].reset_index(drop=True)\n",
    "hadms = model_notes_df['hadm_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Subset the **notes_cohort** to get details of only those encountered that are used for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:34:57.701506Z",
     "start_time": "2019-11-08T11:34:57.641856Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_cohort = pd.read_csv(path/'notes_all_cohort.csv')\n",
    "notes_cohort = notes_cohort[notes_cohort['hadm_id'].isin(hadms)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:34:57.723224Z",
     "start_time": "2019-11-08T11:34:57.703132Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def group_eth(eth):\n",
    "  eth = eth.lower()\n",
    "  if 'white' in eth:\n",
    "    return 'white'\n",
    "  elif 'black' in eth:\n",
    "    return 'black'\n",
    "  elif 'hispanic' in eth:\n",
    "    return 'hispanic'\n",
    "  elif 'asian' in eth:\n",
    "    return 'asian'\n",
    "  else:\n",
    "    return 'other'\n",
    "\n",
    "notes_cohort['ethnicity'] = notes_cohort['ethnicity'].apply(group_eth)\n",
    "notes_cohort.loc[notes_cohort['admission_age'] > 100, 'admission_age'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:34:57.755577Z",
     "start_time": "2019-11-08T11:34:57.724741Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Number of patients in notes cohort: {notes_cohort['subject_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:34:57.777448Z",
     "start_time": "2019-11-08T11:34:57.757106Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = notes_cohort.groupby('expire_flag')['subject_id'].nunique().to_numpy()\n",
    "print(f\"Mortality in notes cohort: {g[1]} ({(g[1]/g.sum())*100:0.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:34:57.796887Z",
     "start_time": "2019-11-08T11:34:57.778932Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = notes_cohort.groupby('gender')['subject_id'].nunique().to_numpy()\n",
    "print(f\"Males in notes cohort: {g[1]} ({(g[1]/g.sum())*100:0.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:34:57.845698Z",
     "start_time": "2019-11-08T11:34:57.798152Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Mean:{notes_cohort.groupby('subject_id')['admission_age'].first().mean():0.1f}\")\n",
    "print(f\"STD:{notes_cohort.groupby('subject_id')['admission_age'].first().std():0.1f}\")\n",
    "print(f\"25th percentile:{notes_cohort.groupby('subject_id')['admission_age'].first().quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{notes_cohort.groupby('subject_id')['admission_age'].first().quantile(0.75):0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:34:57.886585Z",
     "start_time": "2019-11-08T11:34:57.847471Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = pd.DataFrame(notes_cohort.groupby('admission_type')['hadm_id'].nunique()).reset_index()\n",
    "g.columns = ['encounter_type', 'count']\n",
    "g['pct'] = np.round((g['count']/g['count'].sum() * 100), 1)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:34:57.911379Z",
     "start_time": "2019-11-08T11:34:57.888169Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = pd.DataFrame(notes_cohort.groupby('ethnicity')['subject_id'].nunique()).reset_index()\n",
    "g.columns = ['ethnicity', 'count']\n",
    "g['pct'] = np.round((g['count']/g['count'].sum() * 100), 1)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Notes Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:35:02.943214Z",
     "start_time": "2019-11-08T11:35:01.777594Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_df = pd.read_csv(path/'notes_all_proc.csv', parse_dates=['intime', 'admittime', 'ne_charttime'])\n",
    "model_notes_df = notes_df[notes_df['imi_adm_label'] != -1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:35:04.907623Z",
     "start_time": "2019-11-08T11:35:04.729899Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Encounter time to ICU Admission for model cohort:\")\n",
    "print(f\"Mean:{model_notes_df['admit_to_icu'].mean():0.1f}\")\n",
    "print(f\"STD:{model_notes_df['admit_to_icu'].std():0.1f}\")\n",
    "print(f\"25th percentile:{model_notes_df['admit_to_icu'].quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{model_notes_df['admit_to_icu'].quantile(0.75):0.1f}\")\n",
    "print(\"Encounter time to ICU Admission for notes cohort:\")\n",
    "print(f\"Mean:{notes_df['admit_to_icu'].mean():0.1f}\")\n",
    "print(f\"STD:{notes_df['admit_to_icu'].std():0.1f}\")\n",
    "print(f\"25th percentile:{notes_df['admit_to_icu'].quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{notes_df['admit_to_icu'].quantile(0.75):0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:35:04.927043Z",
     "start_time": "2019-11-08T11:35:04.909595Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Average Number of clinical notes per encounter for model cohort: {(len(model_notes_df)/model_notes_df['hadm_id'].nunique()):0.1f}\")\n",
    "print(f\"Average Number of clinical notes per encounter for notes cohort: {(len(notes_df)/notes_df['hadm_id'].nunique()):0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:35:05.090050Z",
     "start_time": "2019-11-08T11:35:05.065585Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Clinical Note Length for model cohort:\")\n",
    "print(f\"Mean:{model_notes_df['note_len'].mean():0.1f}\")\n",
    "print(f\"STD:{model_notes_df['note_len'].std():0.1f}\")\n",
    "print(f\"25th percentile:{model_notes_df['note_len'].quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{model_notes_df['note_len'].quantile(0.75):0.1f}\")\n",
    "print()\n",
    "print(\"Clinical Note Length for notes cohort:\")\n",
    "print(f\"Mean:{notes_df['note_len'].mean():0.1f}\")\n",
    "print(f\"STD:{notes_df['note_len'].std():0.1f}\")\n",
    "print(f\"25th percentile:{notes_df['note_len'].quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{notes_df['note_len'].quantile(0.75):0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:35:06.479779Z",
     "start_time": "2019-11-08T11:35:06.449176Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Note distribution by category in model cohort:\")\n",
    "g = pd.DataFrame(model_notes_df.groupby('category').size()).reset_index()\n",
    "g.columns = ['category', 'count']\n",
    "g['pct'] = np.round((g['count']/g['count'].sum() * 100), 1)\n",
    "print(g)\n",
    "print()\n",
    "print(\"Note distribution by category in notes cohort:\")\n",
    "g = pd.DataFrame(notes_df.groupby('category').size()).reset_index()\n",
    "g.columns = ['category', 'count']\n",
    "g['pct'] = np.round((g['count']/g['count'].sum() * 100), 1)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Notes Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:49:29.411016Z",
     "start_time": "2019-11-08T11:49:28.286327Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cohort = 'notes_all'\n",
    "notes_df = pd.read_csv(path/f'{cohort}_proc.csv', parse_dates=['intime', 'admittime', 'ne_charttime'])\n",
    "\n",
    "save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:49:30.079479Z",
     "start_time": "2019-11-08T11:49:29.412776Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Note length distribution\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "sns.distplot(notes_df['note_len'], kde=False, ax=ax, bins=100)\n",
    "ax.set_xlim(0, 10000)\n",
    "ax.set_xlabel('Length of Note (characters)')\n",
    "ax.set_ylabel('# notes')\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_note_len_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:49:30.709041Z",
     "start_time": "2019-11-08T11:49:30.081827Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Note distribution over days before ICU admission binned to 15 days\n",
    "plot_df = notes_df[['admit_to_icu']]\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.distplot(plot_df, kde=False, ax=ax, bins=80)\n",
    "ax.set_xlabel('Time to ICU admission (days)')\n",
    "ax.set_ylabel('# notes')\n",
    "ax.set_xlim(0, 70)\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_admit_to_icu_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:49:31.260714Z",
     "start_time": "2019-11-08T11:49:30.711313Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Note distribution over days before ICU admission binned to 15 days\n",
    "intervals = ['-1 ≤ t ≤ 0']\n",
    "intervals += [f'-{i+1} ≤ t ≤ -{i}' for i in range(1, notes_df['interval'].max())]\n",
    "intervals.append(f\"t ≥ -{notes_df['interval'].max()}\")\n",
    "\n",
    "plot_df = pd.DataFrame(notes_df.loc[notes_df['interval'] != -1].groupby('interval').size(), columns=['n_notes']).reset_index(drop=True)\n",
    "plot_df['days'] = intervals\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "sns.barplot(x='days', y='n_notes', data=plot_df, ax=ax)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=45, ha='right')\n",
    "ax.set_xlabel('Time to ICU admission (days)')\n",
    "ax.set_ylabel('# notes')\n",
    "for index, row in plot_df.iterrows():\n",
    "    ax.text(index, row['n_notes'], str(row['n_notes']), color='black', ha='center', va='bottom')\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_admit_to_icu_binned_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T12:47:22.475327Z",
     "start_time": "2019-11-08T12:47:17.467891Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note distribution over days before ICU admission by Category  binned to 15 days\n",
    "def plot_intervals(ax, df, cat):\n",
    "  sns.barplot(x='days', y='n_notes', data=df, ax=ax)\n",
    "  ax.set_xticklabels(ax.get_xticklabels(),rotation=45, ha='right')\n",
    "  ax.set_xlabel('')\n",
    "  ax.set_ylabel('')\n",
    "  ax.set_title(f\"Note Category: {cat}\\n# notes: {df['n_notes'].sum()}\")   \n",
    "\n",
    "  for index, (_, row) in enumerate(df.iterrows()):\n",
    "      ax.text(index, row['n_notes'], str(row['n_notes']), color='black', ha='center', va='bottom') \n",
    "\n",
    "plot_df = pd.DataFrame(notes_df.groupby(['category', 'interval']).size(), columns=['n_notes'])\n",
    "plot_df.reset_index(inplace=True)\n",
    "plot_df['days'] = plot_df['interval'].apply(lambda x: intervals[x])\n",
    "plot_df.drop(['interval'], inplace=True, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(4, 3, figsize=(20, 25))\n",
    "plot_intervals(ax[0][0], plot_df.loc[plot_df['category'] == 'Case Management ', ['n_notes', 'days']], 'Case Management')\n",
    "plot_intervals(ax[0][1], plot_df.loc[plot_df['category'] == 'Consult', ['n_notes', 'days']], 'Consult')\n",
    "plot_intervals(ax[0][2], plot_df.loc[plot_df['category'] == 'General', ['n_notes', 'days']], 'General')\n",
    "               \n",
    "plot_intervals(ax[1][0], plot_df.loc[plot_df['category'] == 'Nursing', ['n_notes', 'days']], 'Nursing')\n",
    "plot_intervals(ax[1][1], plot_df.loc[plot_df['category'] == 'Nursing/other', ['n_notes', 'days']], 'Nursing/other')\n",
    "plot_intervals(ax[1][2], plot_df.loc[plot_df['category'] == 'Nutrition', ['n_notes', 'days']], 'Nutrition')\n",
    "\n",
    "plot_intervals(ax[2][0], plot_df.loc[plot_df['category'] == 'Pharmacy', ['n_notes', 'days']], 'Pharmacy')\n",
    "plot_intervals(ax[2][1], plot_df.loc[plot_df['category'] == 'Physician ', ['n_notes', 'days',]], 'Physician')\n",
    "plot_intervals(ax[2][2], plot_df.loc[plot_df['category'] == 'Radiology', ['n_notes', 'days']], 'Radiology')\n",
    "               \n",
    "plot_intervals(ax[3][0], plot_df.loc[plot_df['category'] == 'Rehab Services', ['n_notes', 'days']], 'Rehab Services')\n",
    "plot_intervals(ax[3][1], plot_df.loc[plot_df['category'] == 'Respiratory ', ['n_notes', 'days']], 'Respiratory')\n",
    "plot_intervals(ax[3][2], plot_df.loc[plot_df['category'] == 'Social Work', ['n_notes', 'days']], 'Social Work')\n",
    "\n",
    "fig.text(0.5, 0.09, 'Time to ICU admission (days)', ha='center')\n",
    "fig.text(0.08, 0.5, '# notes', va='center', rotation='vertical')\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.3)\n",
    "               \n",
    "if save:               \n",
    "  fig.savefig(figdir/f'{cohort}_admit_to_icu_cat_binned_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T14:25:52.503649Z",
     "start_time": "2019-11-08T14:25:51.885341Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Histogram of time between note charttime and ICU admittime\n",
    "plot_df = notes_df[['category', 'note_to_icu']]\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.distplot(plot_df['note_to_icu'], kde=False, ax=ax, bins=80)\n",
    "ax.set_xlabel('Note Charttime to ICU Admittime (days)')\n",
    "ax.set_ylabel('# notes')\n",
    "ax.set_xlim(0, 60)\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_note_to_icu_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T14:26:18.244019Z",
     "start_time": "2019-11-08T14:26:13.786692Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Histogram of time between note charttime and ICU admittime by Category\n",
    "def plot_period(ax, df, cat):\n",
    "  sns.distplot(df, kde=False, ax=ax, bins=10)\n",
    "  ax.set_xlabel('')\n",
    "  ax.set_ylabel('')\n",
    "  ax.set_title(f\"Note Category: {cat}\")\n",
    "\n",
    "fig, ax = plt.subplots(4, 3, figsize=(20, 25))\n",
    "plot_period(ax[0][0], plot_df.loc[plot_df['category'] == 'Case Management ', ['note_to_icu']], 'Case Management')\n",
    "plot_period(ax[0][1], plot_df.loc[plot_df['category'] == 'Consult', ['note_to_icu']], 'Consult')\n",
    "plot_period(ax[0][2], plot_df.loc[plot_df['category'] == 'General', ['note_to_icu']], 'General')\n",
    "\n",
    "plot_period(ax[1][0], plot_df.loc[plot_df['category'] == 'Nursing', ['note_to_icu']], 'Nursing')\n",
    "plot_period(ax[1][1], plot_df.loc[plot_df['category'] == 'Nursing/other', ['note_to_icu']], 'Nursing/other')\n",
    "plot_period(ax[1][2], plot_df.loc[plot_df['category'] == 'Nutrition', ['note_to_icu']], 'Nutrition')\n",
    "\n",
    "plot_period(ax[2][0], plot_df.loc[plot_df['category'] == 'Pharmacy', ['note_to_icu']], 'Pharmacy')\n",
    "plot_period(ax[2][1], plot_df.loc[plot_df['category'] == 'Physician ', ['note_to_icu',]], 'Physician')\n",
    "plot_period(ax[2][2], plot_df.loc[plot_df['category'] == 'Radiology', ['note_to_icu']], 'Radiology')\n",
    "\n",
    "plot_period(ax[3][0], plot_df.loc[plot_df['category'] == 'Rehab Services', ['note_to_icu']], 'Rehab Services')\n",
    "plot_period(ax[3][1], plot_df.loc[plot_df['category'] == 'Respiratory ', ['note_to_icu']], 'Respiratory')\n",
    "plot_period(ax[3][2], plot_df.loc[plot_df['category'] == 'Social Work', ['note_to_icu']], 'Social Work')\n",
    "\n",
    "fig.text(0.5, 0.1, 'Note Charttime to ICU Admittime (days)', ha='center')\n",
    "fig.text(0.08, 0.5, '# notes', va='center', rotation='vertical')\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.1)\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_note_to_icu_cat_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:03:10.440104Z",
     "start_time": "2019-11-08T15:03:10.159215Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "desc = ['Unused', 'Delayed ICU Admission', 'Imminent ICU Admission']\n",
    "\n",
    "p = pd.DataFrame(notes_df.groupby(['imi_adm_label']).size(), columns=['n_notes']).reset_index()\n",
    "# p1 = pd.DataFrame(notes_df.groupby(['imi_adm_label']).size(), columns=['n_notes']).reset_index()\n",
    "# p2 = notes_df.groupby(['imi_adm_label'])['hadm_id'].nunique().reset_index()\n",
    "\n",
    "# p = p1.merge(p2, on=['imi_adm_label'])\n",
    "p['imi_adm_label'] = desc\n",
    "p = p.reindex([2, 1, 0])\n",
    "# p.reset_index(inplace=True, drop=True)\n",
    "\n",
    "plot_df = p.copy()\n",
    "plot_df.rename(columns={'hadm_id':'# Encounters', 'n_notes':'# Notes'}, inplace=True)\n",
    "plot_df = pd.melt(plot_df, id_vars='imi_adm_label', var_name='Legend', value_name='counts')\n",
    "\n",
    "plot_df\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "sns.barplot(x='imi_adm_label', y='counts', data=plot_df, ax=ax)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), ha='center')\n",
    "ax.set_xlabel('Class Label')\n",
    "ax.set_ylabel('# notes')\n",
    "\n",
    "for index, row in plot_df.iterrows():\n",
    "#     if index < len(plot_df)//2:\n",
    "        ax.text(index+0.06, row['counts'], str(row['counts']), color='black', ha='right', va='bottom')\n",
    "#     else:\n",
    "#         ax.text(index % (len(plot_df)//2), row['counts'], str(row['counts']), color='black', ha='right', va='bottom')\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_note_class_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:49:16.198800Z",
     "start_time": "2019-10-20T11:49:08.788077Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 1. Read in the data and drop duplicates\n",
    "notes_df = pd.read_csv(mimic_path/'imi_notes_mimic.csv', parse_dates=['intime', 'admittime', 'ne_charttime'])\n",
    "notes_df.drop_duplicates(inplace=True)\n",
    "\n",
    "str_df = pd.read_csv(mimic_path/'imi_str_mimic.csv', parse_dates=['ce_charttime'])\n",
    "str_df.drop_duplicates(inplace=True)\n",
    "\n",
    "str_df.shape, notes_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Get the `hadm_id`s for just the notes and create table for those. This is to get the characterstics only those `hadm_ids` with notes (and possibly structured) for extracting cohort characterstics in case just notes results suck.\n",
    "\n",
    "```\n",
    "create table notes_adms (hadm_id integer);\n",
    "\\copy notes_adms(hadm_id) from '/path/to/notes_hadm_ids.csv' delimiter ',' csv header;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:39:16.964414Z",
     "start_time": "2019-10-29T19:39:16.925120Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(mimic_path/'hadm_ids_with_notes.csv', 'w') as f:\n",
    "  f.write('hadm_id\\n')\n",
    "  f.write('\\n'.join([str(i) for i in notes_df['hadm_id'].unique()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:49:16.570786Z",
     "start_time": "2019-10-20T11:49:16.200530Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 2. Extract common `hadm_id` and filter out those that do not appear in both dfs\n",
    "hadms = set(str_df['hadm_id'].unique()).intersection(notes_df['hadm_id'].unique())\n",
    "print(f\"Number of encounters having both forms of data: {len(hadms)}\")\n",
    "\n",
    "common_notes = notes_df[notes_df['hadm_id'].isin(hadms)].reset_index(drop=True)\n",
    "common_str = str_df[str_df['hadm_id'].isin(hadms)].reset_index(drop=True)\n",
    "\n",
    "common_str.shape, str_df.shape, common_notes.shape, notes_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a new table with just the common `hadm_id` by running the following code:\n",
    "```\n",
    "create table common_adms (hadm_id integer);\n",
    "\\copy common_adms(hadm_id) from '/path/to/common_hadm_ids.csv' delimiter ',' csv header;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:49:22.355557Z",
     "start_time": "2019-10-20T11:49:22.323641Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sanity check and write the common `hadm_id`s to disk for extracting cohort characterstics\n",
    "s, n = set(common_str['hadm_id'].unique()), set(common_notes['hadm_id'].unique())\n",
    "assert(s.symmetric_difference(n) == set())\n",
    "\n",
    "with open(mimic_path/'common_hadm_ids.csv', 'w') as f:\n",
    "  f.write('hadm_id\\n')\n",
    "  f.write('\\n'.join([str(i) for i in s]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After this we run `cohort.sql` from the MIMIC extraction repo, to create the cohort table which contains details about the cohort. We will have two tables: `common_cohort` with `hadm_id`s that have both structured and notes data, and `notes_cohort` which with `hadm_id`s that have notes data (and maybe sturctured data that we don't care about)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:50:06.003783Z",
     "start_time": "2019-10-20T11:49:58.226274Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 3. remove redundant info by filling in each time column with the value of the var\n",
    "common_str = common_str.groupby(['hadm_id','ce_charttime']).sum(min_count = 1).reset_index()\n",
    "# 4. groupby ffill \n",
    "common_str = common_str.groupby(['hadm_id'], as_index=False).apply(lambda group: group.ffill())\n",
    "common_str.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Steps 5 & 6 for both common_notes and notes_df\n",
    "# 5. cat the category, description, text into a new note col and remove them\n",
    "notes_df['note'] = notes_df['category'].str.cat(notes_df['description'], sep='\\n')\n",
    "notes_df['note'] = notes_df['note'].str.cat(notes_df['text'], sep='\\n')\n",
    "notes_df['note_len'] = notes_df['note'].apply(len)\n",
    "notes_df.drop(columns=['category', 'description', 'text'], inplace=True)  \n",
    "\n",
    "# 6. cat notes charted at the same time\n",
    "notes_df = pd.DataFrame(notes_df.groupby(['hadm_id', 'intime', 'admittime', 'ne_charttime'])['note'].apply('\\n'.join)).reset_index()\n",
    "\n",
    "notes_df['category'] = notes_df['note'].apply(lambda x: x.split('\\n')[0])\n",
    "notes_df['admit_to_icu'] = (notes_df['intime'] - notes_df['admittime'])/np.timedelta64(1, 'D')\n",
    "notes_df['note_to_icu'] = (notes_df['intime'] - notes_df['ne_charttime'])/np.timedelta64(1, 'D')\n",
    "notes_df['note_len'] = notes_df['note'].apply(len)\n",
    "\n",
    "# 5. cat the category, description, text into a new note col and remove them\n",
    "common_notes['note'] = common_notes['category'].str.cat(common_notes['description'], sep='\\n')\n",
    "common_notes['note'] = common_notes['note'].str.cat(common_notes['text'], sep='\\n')\n",
    "common_notes['note_len'] = common_notes['note'].apply(len)\n",
    "common_notes.drop(columns=['category', 'description', 'text'], inplace=True)  \n",
    "\n",
    "# 6. cat notes charted at the same time\n",
    "common_notes = pd.DataFrame(common_notes.groupby(['hadm_id', 'intime', 'admittime', 'ne_charttime'])['note'].apply('\\n'.join)).reset_index()\n",
    "\n",
    "common_notes['category'] = common_notes['note'].apply(lambda x: x.split('\\n')[0])\n",
    "common_notes['admit_to_icu'] = (common_notes['intime'] - common_notes['admittime'])/np.timedelta64(1, 'D')\n",
    "common_notes['note_to_icu'] = (common_notes['intime'] - common_notes['ne_charttime'])/np.timedelta64(1, 'D')\n",
    "common_notes['note_len'] = common_notes['note'].apply(len)\n",
    "\n",
    "common_notes.shape, notes_df.shape, notes_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can drop those rows which don't have basic vitals of HR, MAP, and RESP. But for now, we go with everything we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# common_str.dropna(subset=['hr', 'map', 'resp'], how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T13:30:50.329682Z",
     "start_time": "2019-10-17T13:30:40.405909Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "common_str.to_csv(mimic_path/'proc1_str_mimic.csv', index=False)\n",
    "common_notes.to_csv(mimic_path/'proc1_notes_mimic_common.csv', index=False)\n",
    "notes_df.to_csv(mimic_path/'proc1_notes_mimic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Dev for Structured Data Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This code is only development on a subset of encounters to calculate change statistics. This will be run for all the encounters separately using the `stats.py` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:51:38.683282Z",
     "start_time": "2019-10-16T16:51:38.660983Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def percentile(n):\n",
    "  def percentile_(x):\n",
    "    return x.quantile(n)\n",
    "  percentile_.__name__ = f'percentile_{n*100:2.0f}'\n",
    "  return percentile_\n",
    "\n",
    "def change_name(col_name):\n",
    "  if '(' not in col_name:\n",
    "    return col_name\n",
    "  cols = literal_eval(col_name)\n",
    "  return f'{cols[0]}_{cols[1]}'\n",
    "\n",
    "statistics = ['min', 'mean', 'median', 'std', 'var', 'kurt', 'skew', percentile(0.25), percentile(0.75), stats.iqr, 'max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:51:39.443285Z",
     "start_time": "2019-10-16T16:51:38.684845Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "common_str = pd.read_csv(mimic_path/'proc1_str_mimic.csv', parse_dates=['ce_charttime'])\n",
    "var_cols = common_str.columns[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:51:39.474480Z",
     "start_time": "2019-10-16T16:51:39.444800Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hadms = [100104, 100975, 101511, 111073]\n",
    "subset_df = common_str.loc[(common_str['hadm_id'].isin(hadms))].reset_index(drop=True)\n",
    "subset_df.groupby('hadm_id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:52:35.484546Z",
     "start_time": "2019-10-16T16:52:27.419630Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for hadm_id, group_df in tqdm(subset_df.groupby('hadm_id'), desc='Encounters'):\n",
    "  df = group_df.copy()\n",
    "  var_df = df[var_cols].reset_index(drop=True) # save the original vals for later\n",
    "  \n",
    "  df.set_index('ce_charttime', inplace=True) # set charttime as index for rolling 24h\n",
    "  stats_df = df[var_cols].rolling('24h').agg(statistics)\n",
    "  \n",
    "  df = pd.DataFrame(stats_df.to_records()) # flatten the resulting dataframe\n",
    "  df.insert(loc=1, column='hadm_id', value=hadm_id)\n",
    "  \n",
    "  df.rename(columns=change_name, inplace=True) # rename columns\n",
    "  df = pd.concat([df, var_df], axis=1) # add the original vals back\n",
    "  \n",
    "  # reorder vars such that the columns are var, var_stat...\n",
    "  stats_cols = df.columns[2:]\n",
    "  all_cols = []\n",
    "  for var in var_cols:\n",
    "    all_cols.append(var)\n",
    "    for stat in stats_cols:\n",
    "      if f'{var}_' in stat:\n",
    "        all_cols.append(stat)\n",
    "        \n",
    "  order = list(df.columns[:2]) + all_cols\n",
    "  df = df[order]\n",
    "  \n",
    "  df.to_csv(mimic_path/f'stats_dir/{hadm_id}.csv', index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-19T22:06:42.381630Z",
     "start_time": "2019-10-19T22:02:42.935524Z"
    },
    "code_folding": [
     6
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "stats_dir = mimic_path/'stats_dir'\n",
    "n_files = 0\n",
    "for _ in stats_dir.glob('*.csv'):\n",
    "  n_files += 1\n",
    "\n",
    "for i, f in enumerate(tqdm(stats_dir.glob('*.csv'), total=n_files, desc='Stats CSV')):\n",
    "    dfs.append(pd.read_csv(f))\n",
    "\n",
    "common_str = pd.concat(dfs)\n",
    "common_str.reset_index(drop=True, inplace=True)\n",
    "common_str['ce_charttime'] =  pd.to_datetime(common_str['ce_charttime'])\n",
    "common_str.to_csv(mimic_path/'proc2_str_mimic_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We are going to label only the notes df. Since we are basing our time element on `ne_charttime` and attaching structured data to `common_notes` to get final merged df, we don't need to label structured data separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:30:17.407651Z",
     "start_time": "2019-10-20T11:30:17.386422Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def note_interval(x):\n",
    "  if pd.isnull(x):\n",
    "    return -1\n",
    "  \n",
    "  if 0 < x <= 1:\n",
    "    return 0\n",
    "  elif 1 < x <= 2:\n",
    "    return 1\n",
    "  elif 2 < x <= 3:\n",
    "    return 2\n",
    "  elif 3 < x <= 4:\n",
    "    return 3\n",
    "  elif 4 < x <= 5:\n",
    "    return 4\n",
    "  elif 5 < x <= 6:\n",
    "    return 5\n",
    "  elif 6 < x <= 7:\n",
    "    return 6\n",
    "  elif 7 < x <= 8:\n",
    "    return 7\n",
    "  elif 8 < x <= 9:\n",
    "    return 8\n",
    "  elif 9 < x <= 10:\n",
    "    return 9\n",
    "  elif 10 < x <= 11:\n",
    "    return 10\n",
    "  elif 11 < x <= 12:\n",
    "    return 11\n",
    "  elif 12 < x <= 13:\n",
    "    return 12\n",
    "  elif 13 < x <= 14:\n",
    "    return 13\n",
    "  elif 14 < x <= 15:\n",
    "    return 14\n",
    "  else:\n",
    "    return 15\n",
    "\n",
    "def icu_adm_label(x):\n",
    "  if 0 <= x <= 1:\n",
    "    return -1 # unused notes due to data leakage\n",
    "  elif 1 < x <= 3:\n",
    "    return 1 # imminent ICU admission\n",
    "  elif 3 < x <= 5:\n",
    "    return -1 # unused notes due to data leakage\n",
    "  else:\n",
    "    return 0 # delayed ICU admission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:30:23.432460Z",
     "start_time": "2019-10-20T11:30:18.851464Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_df = pd.read_csv(mimic_path/'proc1_notes_mimic.csv', parse_dates=['intime', 'admittime', 'ne_charttime'])\n",
    "common_notes = pd.read_csv(mimic_path/'proc1_notes_mimic_common.csv', parse_dates=['intime', 'admittime', 'ne_charttime'])\n",
    "common_notes.shape, notes_df.shape, notes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:31:27.495604Z",
     "start_time": "2019-10-20T11:31:27.288402Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_df['interval'] = notes_df['note_to_icu'].apply(note_interval)\n",
    "common_notes['interval'] = common_notes['note_to_icu'].apply(note_interval)\n",
    "notes_df['imi_adm_label'] = notes_df['interval'].apply(icu_adm_label)\n",
    "common_notes['imi_adm_label'] = common_notes['interval'].apply(icu_adm_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:31:28.596552Z",
     "start_time": "2019-10-20T11:31:28.567980Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = common_notes.loc[common_notes['imi_adm_label'] != -1].groupby(['imi_adm_label']).size().to_numpy()\n",
    "print(f\"Prevalence in notes cohort:{(g[1]/g.sum())*100:0.1f}\")\n",
    "g = notes_df.loc[notes_df['imi_adm_label'] != -1].groupby(['imi_adm_label']).size().to_numpy()\n",
    "print(f\"Prevalence in notes cohort:{(g[1]/g.sum())*100:0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:31:42.621042Z",
     "start_time": "2019-10-20T11:31:34.511905Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_df.to_csv(mimic_path/'proc3_notes_mimic_labeled.csv', index=False)\n",
    "common_notes.to_csv(mimic_path/'proc3_notes_mimic_common_labeled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Cohort Characterstics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:32:03.739988Z",
     "start_time": "2019-10-20T11:32:03.683440Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "common_cohort = pd.read_csv(mimic_path/'common_cohort.csv')\n",
    "notes_cohort = pd.read_csv(mimic_path/'notes_cohort.csv')\n",
    "common_cohort.shape, notes_cohort.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:32:04.076918Z",
     "start_time": "2019-10-20T11:32:04.048416Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def group_eth(eth):\n",
    "  eth = eth.lower()\n",
    "  if 'white' in eth:\n",
    "    return 'white'\n",
    "  elif 'black' in eth:\n",
    "    return 'black'\n",
    "  elif 'hispanic' in eth:\n",
    "    return 'hispanic'\n",
    "  elif 'asian' in eth:\n",
    "    return 'asian'\n",
    "  else:\n",
    "    return 'other'\n",
    "\n",
    "common_cohort['ethnicity'] = common_cohort['ethnicity'].apply(group_eth)\n",
    "notes_cohort['ethnicity'] = notes_cohort['ethnicity'].apply(group_eth)\n",
    "common_cohort.loc[common_cohort['admission_age'] > 100, 'admission_age'] = 100\n",
    "notes_cohort.loc[notes_cohort['admission_age'] > 100, 'admission_age'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:32:05.171807Z",
     "start_time": "2019-10-20T11:32:05.155850Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Number of patients in common cohort: {common_cohort['subject_id'].nunique()}\")\n",
    "print(f\"Number of patients in notes cohort: {notes_cohort['subject_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:32:12.527681Z",
     "start_time": "2019-10-20T11:32:12.461949Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = common_cohort.groupby('expire_flag')['subject_id'].nunique().to_numpy()\n",
    "print(f\"Mortality in common cohort: {g[1]} ({(g[1]/g.sum())*100:0.1f}%)\")\n",
    "\n",
    "g = notes_cohort.groupby('expire_flag')['subject_id'].nunique().to_numpy()\n",
    "print(f\"Mortality in notes cohort: {g[1]} ({(g[1]/g.sum())*100:0.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:32:41.925404Z",
     "start_time": "2019-10-20T11:32:41.904094Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = common_cohort.groupby('gender')['subject_id'].nunique().to_numpy()\n",
    "print(f\"Males in common cohort: {g[1]} ({(g[1]/g.sum())*100:0.1f}%)\")\n",
    "g = notes_cohort.groupby('gender')['subject_id'].nunique().to_numpy()\n",
    "print(f\"Males in notes cohort: {g[1]} ({(g[1]/g.sum())*100:0.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:32:56.650984Z",
     "start_time": "2019-10-20T11:32:56.520232Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Age details in common cohort:\")\n",
    "print(f\"Mean:{common_cohort.groupby('subject_id')['admission_age'].first().mean():0.1f}\")\n",
    "print(f\"STD:{common_cohort.groupby('subject_id')['admission_age'].first().std():0.1f}\")\n",
    "print(f\"25th percentile:{common_cohort.groupby('subject_id')['admission_age'].first().quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{common_cohort.groupby('subject_id')['admission_age'].first().quantile(0.75):0.1f}\")\n",
    "print()\n",
    "print(\"Age details in notes cohort:\")\n",
    "print(f\"Mean:{notes_cohort.groupby('subject_id')['admission_age'].first().mean():0.1f}\")\n",
    "print(f\"STD:{notes_cohort.groupby('subject_id')['admission_age'].first().std():0.1f}\")\n",
    "print(f\"25th percentile:{notes_cohort.groupby('subject_id')['admission_age'].first().quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{notes_cohort.groupby('subject_id')['admission_age'].first().quantile(0.75):0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:33:12.631502Z",
     "start_time": "2019-10-20T11:33:12.560122Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Admission types for common cohort:\")\n",
    "g = pd.DataFrame(common_cohort.groupby('admission_type')['hadm_id'].nunique()).reset_index()\n",
    "g.columns = ['encounter_type', 'count']\n",
    "g['pct'] = np.round((g['count']/g['count'].sum() * 100), 1)\n",
    "print(g)\n",
    "print()\n",
    "print(\"Admission types for notes cohort:\")\n",
    "g = pd.DataFrame(notes_cohort.groupby('admission_type')['hadm_id'].nunique()).reset_index()\n",
    "g.columns = ['encounter_type', 'count']\n",
    "g['pct'] = np.round((g['count']/g['count'].sum() * 100), 1)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:33:21.997140Z",
     "start_time": "2019-10-20T11:33:21.952578Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Ethnicty for common cohort:\")\n",
    "g = pd.DataFrame(common_cohort.groupby('ethnicity')['subject_id'].nunique()).reset_index()\n",
    "g.columns = ['ethnicity', 'count']\n",
    "g['pct'] = np.round((g['count']/g['count'].sum() * 100), 1)\n",
    "print(g)\n",
    "print()\n",
    "print(\"Ethnicty for notes cohort:\")\n",
    "g = pd.DataFrame(notes_cohort.groupby('ethnicity')['subject_id'].nunique()).reset_index()\n",
    "g.columns = ['ethnicity', 'count']\n",
    "g['pct'] = np.round((g['count']/g['count'].sum() * 100), 1)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Notes Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:33:29.153755Z",
     "start_time": "2019-10-20T11:33:27.030467Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notes_df = pd.read_csv(mimic_path/'proc3_notes_mimic_labeled.csv', parse_dates=['intime', 'admittime', 'ne_charttime'])\n",
    "common_notes = pd.read_csv(mimic_path/'proc3_notes_mimic_common_labeled.csv', parse_dates=['intime', 'admittime', 'ne_charttime'])\n",
    "common_notes.shape, notes_df.shape, notes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:33:29.747237Z",
     "start_time": "2019-10-20T11:33:29.721261Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Encounter time to ICU Admission for common cohort:\")\n",
    "print(f\"Mean:{common_notes['admit_to_icu'].mean():0.1f}\")\n",
    "print(f\"STD:{common_notes['admit_to_icu'].std():0.1f}\")\n",
    "print(f\"25th percentile:{common_notes['admit_to_icu'].quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{common_notes['admit_to_icu'].quantile(0.75):0.1f}\")\n",
    "print()\n",
    "print(\"Encounter time to ICU Admission for notes cohort:\")\n",
    "print(f\"Mean:{notes_df['admit_to_icu'].mean():0.1f}\")\n",
    "print(f\"STD:{notes_df['admit_to_icu'].std():0.1f}\")\n",
    "print(f\"25th percentile:{notes_df['admit_to_icu'].quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{notes_df['admit_to_icu'].quantile(0.75):0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:33:56.162358Z",
     "start_time": "2019-10-20T11:33:56.145514Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Average Number of clinical notes per encounter for common cohort: {(len(common_notes)/common_notes['hadm_id'].nunique()):0.1f}\")\n",
    "print(f\"Average Number of clinical notes per encounter for notes cohort: {(len(notes_df)/notes_df['hadm_id'].nunique()):0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:34:01.172178Z",
     "start_time": "2019-10-20T11:34:01.145983Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Clinical Note Length for common cohort:\")\n",
    "print(f\"Mean:{common_notes['note_len'].mean():0.1f}\")\n",
    "print(f\"STD:{common_notes['note_len'].std():0.1f}\")\n",
    "print(f\"25th percentile:{common_notes['note_len'].quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{common_notes['note_len'].quantile(0.75):0.1f}\")\n",
    "print()\n",
    "print(\"Clinical Note Length for notes cohort:\")\n",
    "print(f\"Mean:{notes_df['note_len'].mean():0.1f}\")\n",
    "print(f\"STD:{notes_df['note_len'].std():0.1f}\")\n",
    "print(f\"25th percentile:{notes_df['note_len'].quantile(0.25):0.1f}\")\n",
    "print(f\"75th percentile:{notes_df['note_len'].quantile(0.75):0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:34:04.778529Z",
     "start_time": "2019-10-20T11:34:04.744011Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Note distribution by category in common cohort:\")\n",
    "g = pd.DataFrame(common_notes.groupby('category').size()).reset_index()\n",
    "g.columns = ['category', 'count']\n",
    "g['pct'] = np.round((g['count']/g['count'].sum() * 100), 1)\n",
    "print(g)\n",
    "print()\n",
    "print(\"Note distribution by category in notes cohort:\")\n",
    "g = pd.DataFrame(notes_df.groupby('category').size()).reset_index()\n",
    "g.columns = ['category', 'count']\n",
    "g['pct'] = np.round((g['count']/g['count'].sum() * 100), 1)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Notes Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:35:45.166115Z",
     "start_time": "2019-10-20T11:35:45.141477Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save = True\n",
    "cohort = 'common'\n",
    "figdir = mimic_path/'figures'\n",
    "\n",
    "if cohort == 'common':\n",
    "  df = common_notes\n",
    "elif cohort == 'notes':\n",
    "  df = notes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:35:45.728222Z",
     "start_time": "2019-10-20T11:35:45.168254Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Note length distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.distplot(df['note_len'], kde=False, ax=ax, bins=100)\n",
    "ax.set_xlim(0, 12500)\n",
    "ax.set_xlabel('Length of Note (characters)')\n",
    "ax.set_ylabel('# notes')\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_note_len_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:35:46.248382Z",
     "start_time": "2019-10-20T11:35:45.730552Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Note distribution over days before ICU admission binned to 15 days\n",
    "plot_df = df[['admit_to_icu']]\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.distplot(plot_df, kde=False, ax=ax, bins=80)\n",
    "ax.set_xlabel('Time to ICU admission (days)')\n",
    "ax.set_ylabel('# notes')\n",
    "ax.set_xlim(0, 70)\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_to_icu_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:35:46.765312Z",
     "start_time": "2019-10-20T11:35:46.250254Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Note distribution over days before ICU admission binned to 15 days\n",
    "intervals = ['-1 ≤ t ≤ 0']\n",
    "intervals += [f'-{i+1} ≤ t ≤ -{i}' for i in range(1, notes_df['interval'].max())]\n",
    "intervals.append(f\"t ≥ -{df['interval'].max()}\")\n",
    "\n",
    "plot_df = pd.DataFrame(df.loc[notes_df['interval'] != -1].groupby('interval').size(), columns=['n_notes']).reset_index(drop=True)\n",
    "plot_df['days'] = intervals\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "sns.barplot(x='days', y='n_notes', data=plot_df, ax=ax)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=45, ha='right')\n",
    "ax.set_xlabel('Time to ICU admission (days)')\n",
    "ax.set_ylabel('# notes')\n",
    "for index, row in plot_df.iterrows():\n",
    "    ax.text(index, row['n_notes'], str(row['n_notes']), color='black', ha='center', va='bottom')\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_admit_to_icu_binned_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:35:51.482585Z",
     "start_time": "2019-10-20T11:35:46.767468Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note distribution over days before ICU admission by Category  binned to 15 days\n",
    "def plot_intervals(ax, df, cat):\n",
    "  sns.barplot(x='days', y='n_notes', data=df, ax=ax)\n",
    "  ax.set_xticklabels(ax.get_xticklabels(),rotation=45, ha='right')\n",
    "  ax.set_xlabel('')\n",
    "  ax.set_ylabel('')\n",
    "  ax.set_title(f\"Note Category: {cat}\\n# notes: {df['n_notes'].sum()}\")   \n",
    "\n",
    "  for index, (_, row) in enumerate(df.iterrows()):\n",
    "      ax.text(index, row['n_notes'], str(row['n_notes']), color='black', ha='center', va='bottom') \n",
    "\n",
    "plot_df = pd.DataFrame(df.groupby(['category', 'interval']).size(), columns=['n_notes'])\n",
    "plot_df.reset_index(inplace=True)\n",
    "plot_df['days'] = plot_df['interval'].apply(lambda x: intervals[x])\n",
    "plot_df.drop(['interval'], inplace=True, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(6, 2, figsize=(20, 50))\n",
    "plot_intervals(ax[0][0], plot_df.loc[plot_df['category'] == 'Case Management ', ['n_notes', 'days']], 'Case Management')\n",
    "plot_intervals(ax[0][1], plot_df.loc[plot_df['category'] == 'Consult', ['n_notes', 'days']], 'Consult')\n",
    "\n",
    "plot_intervals(ax[1][0], plot_df.loc[plot_df['category'] == 'General', ['n_notes', 'days']], 'General')\n",
    "plot_intervals(ax[1][1], plot_df.loc[plot_df['category'] == 'Nursing', ['n_notes', 'days']], 'Nursing')\n",
    "\n",
    "plot_intervals(ax[2][0], plot_df.loc[plot_df['category'] == 'Nursing/other', ['n_notes', 'days']], 'Nursing/other')\n",
    "plot_intervals(ax[2][1], plot_df.loc[plot_df['category'] == 'Nutrition', ['n_notes', 'days']], 'Nutrition')\n",
    "\n",
    "plot_intervals(ax[3][0], plot_df.loc[plot_df['category'] == 'Pharmacy', ['n_notes', 'days']], 'Pharmacy')\n",
    "plot_intervals(ax[3][1], plot_df.loc[plot_df['category'] == 'Physician ', ['n_notes', 'days',]], 'Physician')\n",
    "\n",
    "plot_intervals(ax[4][0], plot_df.loc[plot_df['category'] == 'Radiology', ['n_notes', 'days']], 'Radiology')\n",
    "plot_intervals(ax[4][1], plot_df.loc[plot_df['category'] == 'Rehab Services', ['n_notes', 'days']], 'Rehab Services')\n",
    "\n",
    "plot_intervals(ax[5][0], plot_df.loc[plot_df['category'] == 'Respiratory ', ['n_notes', 'days']], 'Respiratory')\n",
    "plot_intervals(ax[5][1], plot_df.loc[plot_df['category'] == 'Social Work', ['n_notes', 'days']], 'Social Work')\n",
    "\n",
    "fig.text(0.5, 0.1, 'Time to ICU admission (days)', ha='center')\n",
    "fig.text(0.08, 0.5, '# notes', va='center', rotation='vertical')\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.3)\n",
    "               \n",
    "if save:               \n",
    "  fig.savefig(figdir/f'{cohort}_admit_to_icu_cat_binned_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:35:52.090382Z",
     "start_time": "2019-10-20T11:35:51.484548Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Histogram of time between note charttime and ICU admittime\n",
    "plot_df = df[['category', 'note_to_icu']]\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.distplot(plot_df['note_to_icu'], kde=False, ax=ax, bins=80)\n",
    "ax.set_xlabel('Note Charttime to ICU Admittime (days)')\n",
    "ax.set_ylabel('# notes')\n",
    "ax.set_xlim(0, 60)\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_note_to_icu_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:35:56.029771Z",
     "start_time": "2019-10-20T11:35:52.091956Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Histogram of time between note charttime and ICU admittime by Category\n",
    "def plot_period(ax, df, cat):\n",
    "  sns.distplot(df, kde=False, ax=ax, bins=10)\n",
    "  ax.set_xlabel('')\n",
    "  ax.set_ylabel('')\n",
    "  ax.set_title(f\"Note Category: {cat}\")\n",
    "\n",
    "fig, ax = plt.subplots(6, 2, figsize=(20, 50))\n",
    "plot_period(ax[0][0], plot_df.loc[plot_df['category'] == 'Case Management ', ['note_to_icu']], 'Case Management')\n",
    "plot_period(ax[0][1], plot_df.loc[plot_df['category'] == 'Consult', ['note_to_icu']], 'Consult')\n",
    "\n",
    "plot_period(ax[1][0], plot_df.loc[plot_df['category'] == 'General', ['note_to_icu']], 'General')\n",
    "plot_period(ax[1][1], plot_df.loc[plot_df['category'] == 'Nursing', ['note_to_icu']], 'Nursing')\n",
    "\n",
    "plot_period(ax[2][0], plot_df.loc[plot_df['category'] == 'Nursing/other', ['note_to_icu']], 'Nursing/other')\n",
    "plot_period(ax[2][1], plot_df.loc[plot_df['category'] == 'Nutrition', ['note_to_icu']], 'Nutrition')\n",
    "\n",
    "plot_period(ax[3][0], plot_df.loc[plot_df['category'] == 'Pharmacy', ['note_to_icu']], 'Pharmacy')\n",
    "plot_period(ax[3][1], plot_df.loc[plot_df['category'] == 'Physician ', ['note_to_icu',]], 'Physician')\n",
    "\n",
    "plot_period(ax[4][0], plot_df.loc[plot_df['category'] == 'Radiology', ['note_to_icu']], 'Radiology')\n",
    "plot_period(ax[4][1], plot_df.loc[plot_df['category'] == 'Rehab Services', ['note_to_icu']], 'Rehab Services')\n",
    "\n",
    "plot_period(ax[5][0], plot_df.loc[plot_df['category'] == 'Respiratory ', ['note_to_icu']], 'Respiratory')\n",
    "plot_period(ax[5][1], plot_df.loc[plot_df['category'] == 'Social Work', ['note_to_icu']], 'Social Work')\n",
    "\n",
    "fig.text(0.5, 0.11, 'Note Charttime to ICU Admittime (days)', ha='center')\n",
    "fig.text(0.08, 0.5, '# notes', va='center', rotation='vertical')\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.1)\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_note_to_icu_cat_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:35:56.299289Z",
     "start_time": "2019-10-20T11:35:56.031733Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "desc = ['Delayed ICU Admission', 'Imminent ICU Admission']\n",
    "\n",
    "p1 = pd.DataFrame(df.loc[df['imi_adm_label'] != -1].groupby(['imi_adm_label']).size(), columns=['n_notes']).reset_index()\n",
    "p2 = df.loc[df['imi_adm_label'] != -1].groupby(['imi_adm_label'])['hadm_id'].nunique().reset_index()\n",
    "\n",
    "p = p1.merge(p2, on=['imi_adm_label'])\n",
    "p['imi_adm_label'] = desc\n",
    "p = p.reindex([1, 0])\n",
    "p.reset_index(inplace=True, drop=True)\n",
    "\n",
    "plot_df = p.copy()\n",
    "plot_df.rename(columns={'hadm_id':'# Encounters', 'n_notes':'# Notes'}, inplace=True)\n",
    "plot_df = pd.melt(plot_df, id_vars='imi_adm_label', var_name='Legend', value_name='counts')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.barplot(x='imi_adm_label', y='counts', hue='Legend', data=plot_df, ax=ax)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), ha='center')\n",
    "ax.set_xlabel('Class Label')\n",
    "ax.set_ylabel('# notes')\n",
    "\n",
    "for index, row in plot_df.iterrows():\n",
    "    if index < len(plot_df)//2:\n",
    "        ax.text(index-0.13, row['counts']+50, str(row['counts']), color='black', ha='right', va='bottom')\n",
    "    else:\n",
    "        ax.text(index % (len(plot_df)//2)+0.25, row['counts']+50, str(row['counts']), color='black', ha='right', va='bottom')\n",
    "\n",
    "if save:\n",
    "  fig.savefig(figdir/f'{cohort}_note_class_dist.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Merge Structured and Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:53:17.000526Z",
     "start_time": "2019-10-20T11:53:05.531184Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "common_notes = pd.read_csv(mimic_path/'proc3_notes_mimic_common_labeled.csv', parse_dates=['intime', 'admittime', 'ne_charttime'])\n",
    "common_str = pd.read_csv(mimic_path/'proc2_str_mimic_stats.csv', parse_dates=['ce_charttime'])\n",
    "\n",
    "s, n = set(common_str['hadm_id'].unique()), set(common_notes['hadm_id'].unique())\n",
    "assert(s.symmetric_difference(n) == set())\n",
    "col_order = ['hadm_id', 'intime', 'admittime', 'admit_to_icu'] + list(common_str.columns[2:]) + ['ne_charttime', 'note_to_icu', 'interval', 'category', 'note', 'note_len', 'imi_adm_label']\n",
    "\n",
    "common_str.shape, common_notes.shape, len(col_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:53:17.538410Z",
     "start_time": "2019-10-20T11:53:17.002654Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "common_str.sort_values(by='ce_charttime', inplace=True)\n",
    "common_str.reset_index(inplace=True, drop=True)\n",
    "\n",
    "common_notes.sort_values(by='ne_charttime', inplace=True)\n",
    "common_notes.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:53:18.465205Z",
     "start_time": "2019-10-20T11:53:17.540152Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mimic_df = pd.merge_asof(common_notes, common_str, left_on='ne_charttime', right_on='ce_charttime', by='hadm_id')\n",
    "mimic_df.drop(columns='ce_charttime', inplace=True)\n",
    "mimic_df = mimic_df[col_order]\n",
    "\n",
    "mimic_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T11:54:04.279286Z",
     "start_time": "2019-10-20T11:53:46.400686Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mimic_df.to_csv(mimic_path/'merged_labeled_mimic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:23:52.754411Z",
     "start_time": "2019-10-16T16:23:52.059911Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_str_df = pd.read_csv(mimic_path/'proc1_str_mimic.csv', parse_dates=['ce_charttime'])\n",
    "all_var_cols = all_str_df.columns[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:24:02.956282Z",
     "start_time": "2019-10-16T16:24:02.927238Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hadms = [100104, 100975, 101511, 111073]\n",
    "dev_str = all_str_df.loc[(all_str_df['hadm_id'].isin(hadms))].reset_index(drop=True)\n",
    "dev_str.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:24:11.382293Z",
     "start_time": "2019-10-16T16:24:11.364838Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def percentile(n):\n",
    "  def percentile_(x):\n",
    "    return x.quantile(n)\n",
    "  percentile_.__name__ = f'percentile_{n*100:2.0f}'\n",
    "  return percentile_\n",
    "\n",
    "def all_change_name(col_name):\n",
    "  if '(' not in col_name:\n",
    "    return col_name\n",
    "  cols = literal_eval(col_name)\n",
    "  return f'{cols[2]}_{cols[1]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:24:14.022784Z",
     "start_time": "2019-10-16T16:24:14.007286Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "statistics = ['min', 'mean', 'median', 'std', 'var', 'kurt', 'skew', percentile(0.25), percentile(0.75), stats.iqr, 'max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:24:22.720203Z",
     "start_time": "2019-10-16T16:24:22.701204Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_var_df = dev_str[all_var_cols] # save the original vals for later\n",
    "dev_str.set_index('ce_charttime', inplace=True) # set charttime index for 24h rolling\n",
    "dev_str.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:28:32.855893Z",
     "start_time": "2019-10-16T16:24:34.750709Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_stats_df = dev_str.groupby('hadm_id')[all_var_cols].rolling('24h').agg(statistics)\n",
    "all_stats_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:28:35.993269Z",
     "start_time": "2019-10-16T16:28:32.857503Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dev_str = pd.DataFrame(all_stats_df.to_records()) # flatten the resulting dataframe\n",
    "dev_str.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:28:45.917969Z",
     "start_time": "2019-10-16T16:28:45.889693Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dev_str = dev_str.iloc[:, :(2 + (len(statistics) * len(all_var_cols)))] # drop duplicate columns resulting from rolling\n",
    "dev_str.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:28:57.912219Z",
     "start_time": "2019-10-16T16:28:57.888078Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dev_str.rename(columns=all_change_name, inplace=True) # rename columns\n",
    "dev_str.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:29:04.178961Z",
     "start_time": "2019-10-16T16:29:04.161131Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dev_str = pd.concat([dev_str, all_var_df], axis=1) # add the original vals back\n",
    "dev_str.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:29:13.163391Z",
     "start_time": "2019-10-16T16:29:13.144337Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# reorder vars such that the columns are var, var_stat...\n",
    "stats_cols = dev_str.columns[2:]\n",
    "all_cols = []\n",
    "for var in all_var_cols:\n",
    "  all_cols.append(var)\n",
    "  for stat in stats_cols:\n",
    "    if f'{var}_' in stat:\n",
    "      all_cols.append(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:29:18.173555Z",
     "start_time": "2019-10-16T16:29:18.151100Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "order = list(dev_str.columns[:2]) + all_cols\n",
    "dev_str = dev_str[order]\n",
    "dev_str.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T16:29:19.679110Z",
     "start_time": "2019-10-16T16:29:19.633453Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dev_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T02:06:14.232466Z",
     "start_time": "2019-10-09T02:06:14.055129Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dev_str.to_csv(mimic_path/'sample_str.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T17:10:51.165288Z",
     "start_time": "2019-10-08T17:08:51.719096Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols = list(str_df.columns[1:]) # get the cols to merge (everything except hadm_id)\n",
    "final_dfs = [] \n",
    "\n",
    "grouped = notes_df.groupby('hadm_id') # get groups of encounter ids\n",
    "for name, group in grouped:\n",
    "  final_df = group.copy().reset_index(drop=True) # make a copy of notes for that encounter\n",
    "  for col in cols:\n",
    "    final_df[col] = np.nan # set the values to nan\n",
    "\n",
    "  idx = 0 # index to track the final row in the given encounter\n",
    "  for i, note_row in final_df.iterrows():\n",
    "    ne = note_row['ne_charttime']\n",
    "    sub = str_df.loc[(str_df['hadm_id'] == name)].reset_index(drop=True) # get the df corresponding to the ecounter\n",
    "    for j, str_row in sub.iterrows():\n",
    "      ce = str_row['ce_charttime']\n",
    "      if ne < ce: # if the variable charttime < note charttime\n",
    "        idx += 1\n",
    "        \n",
    "        # grab the previous values for the variables and break\n",
    "        for col in cols:\n",
    "          final_df.iloc[i, final_df.columns.get_loc(col)] = sub.iloc[j-1][col]          \n",
    "        break               \n",
    "  pdb.set_trace()\n",
    "  # get the last value in the df for the variables\n",
    "  for col in cols:\n",
    "    final_df.iloc[idx, final_df.columns.get_loc(col)] = sub.iloc[-1][col]\n",
    "  \n",
    "  final_dfs.append(final_df) # append the df to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T17:03:04.193531Z",
     "start_time": "2019-10-08T17:03:03.997303Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# cat the list to get final df and reset index\n",
    "mimic_df = pd.concat(final_dfs)\n",
    "mimic_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T16:48:33.701761Z",
     "start_time": "2019-10-08T16:48:32.404207Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dev = True\n",
    "if dev:\n",
    "  columns = ['hadm_id', 'ce_charttime', 'hr', 'resp', 'magnesium']\n",
    "  var_cols = columns[2:]\n",
    "  working_hadms = [196673, 197006]\n",
    "  str_df = pd.read_csv(mimic_path/'proc1_str_mimic.csv', usecols=columns, parse_dates=['ce_charttime'])\n",
    "else:\n",
    "  working_hadms = str_df['hadm_id'].unique()  \n",
    "  str_df = pd.read_csv(mimic_path/'proc1_str_mimic.csv', parse_dates=['ce_charttime'])\n",
    "  var_cols = str_df.columns[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T16:48:43.151995Z",
     "start_time": "2019-10-08T16:48:43.066502Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dev_str = str_df.loc[(str_df['hadm_id'].isin(working_hadms))].reset_index(drop=True)\n",
    "dev_notes = notes_df.loc[(notes_df['hadm_id'].isin(working_hadms))][['hadm_id', 'ne_charttime', 'note']].reset_index(drop=True)\n",
    "dev_notes.sort_values('ne_charttime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T14:55:31.428892Z",
     "start_time": "2019-10-08T14:55:25.755474Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove redundant info by filling in each time column with the value of the var\n",
    "dev_str = dev_str.groupby(['hadm_id','ce_charttime']).sum(min_count = 1).reset_index()\n",
    "\n",
    "# groupby ffill \n",
    "dev_str = dev_str.groupby(['hadm_id'], as_index=False).apply(lambda group: group.ffill())\n",
    "var_cols = dev_str.columns[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T14:55:37.061817Z",
     "start_time": "2019-10-08T14:55:37.041319Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def percentile(n):\n",
    "  def percentile_(x):\n",
    "    return x.quantile(n)\n",
    "  percentile_.__name__ = f'percentile_{n*100:2.0f}'\n",
    "  return percentile_\n",
    "\n",
    "def get_stats(df, var_cols, statistics):\n",
    "  df.set_index('ce_charttime', inplace=True)\n",
    "  stats_dfs = []\n",
    "\n",
    "  for var in var_cols:\n",
    "    stats_df = df.groupby('hadm_id')[var].rolling('24h').agg(statistics).reset_index(drop=True)\n",
    "    stats_df.columns = [f'{var}_{col}' for col in stats_df.columns]\n",
    "    stats_df = pd.concat([df[var].reset_index(drop=True), stats_df], axis=1)\n",
    "    stats_dfs.append(stats_df)\n",
    "    \n",
    "  df.reset_index(inplace=True)\n",
    "  df.drop(var_cols, inplace=True, axis=1)\n",
    "  return pd.concat([df, *stats_dfs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T14:57:19.619907Z",
     "start_time": "2019-10-08T14:55:37.698805Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "statistics = ['min', 'mean', 'median', 'std', 'var', 'kurt', 'skew', percentile(0.25), percentile(0.75), stats.iqr, 'max']\n",
    "\n",
    "dev_str = get_stats(dev_str.copy(), var_cols, statistics)\n",
    "move = ['hadm_id', 'ce_charttime']\n",
    "order = move + (dev_str.columns.drop(move).tolist())\n",
    "dev_str = dev_str[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T14:49:37.198972Z",
     "start_time": "2019-10-08T14:49:37.177071Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_str = dev_str.loc[(dev_str['hadm_id'] == 196673)].reset_index(drop=True)\n",
    "df_notes = dev_notes.loc[(dev_notes['hadm_id'] == 196673)].reset_index(drop=True)\n",
    "df_str.shape, df_notes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T14:49:37.853122Z",
     "start_time": "2019-10-08T14:49:37.808969Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T14:49:50.726279Z",
     "start_time": "2019-10-08T14:49:50.705128Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T14:42:15.338593Z",
     "start_time": "2019-10-08T14:42:14.838625Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols = list(dev_str.columns[1:]) # get the cols to merge (everything except hadm_id)\n",
    "final_dfs = [] \n",
    "\n",
    "grouped = dev_notes.groupby('hadm_id') # get groups of encounter ids\n",
    "for name, group in grouped:\n",
    "  final_df = group.copy().reset_index(drop=True) # make a copy of notes for that encounter\n",
    "  for col in cols:\n",
    "    final_df[col] = np.nan # set the values to nan\n",
    "\n",
    "  idx = 0 # index to track the final row in the given encounter\n",
    "  for i, note_row in final_df.iterrows():\n",
    "    ne = note_row['ne_charttime']\n",
    "    sub = dev_str.loc[(dev_str['hadm_id'] == name)].reset_index(drop=True) # get the df corresponding to the ecounter\n",
    "    for j, str_row in sub.iterrows():\n",
    "      ce = str_row['ce_charttime']\n",
    "      if ne < ce: # if the variable charttime < note charttime\n",
    "        idx += 1\n",
    "        \n",
    "        # grab the previous values for the variables and break\n",
    "        for col in cols:\n",
    "          final_df.iloc[i, final_df.columns.get_loc(col)] = sub.iloc[j-1][col]          \n",
    "        break               \n",
    "\n",
    "  # get the last value in the df for the variables\n",
    "  for col in cols:\n",
    "    final_df.iloc[idx, final_df.columns.get_loc(col)] = sub.iloc[-1][col]\n",
    "  \n",
    "  final_dfs.append(final_df) # append the df to the list\n",
    "\n",
    "# cat the list to get final df and reset index\n",
    "final_df = pd.concat(final_dfs)\n",
    "final_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T14:46:33.644063Z",
     "start_time": "2019-10-08T14:46:33.617803Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "move = ['ne_charttime', 'note']\n",
    "order = (final_df.columns.drop(move).tolist()) + move \n",
    "final_df = final_df[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T14:46:34.640605Z",
     "start_time": "2019-10-08T14:46:34.601151Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "final_df.loc[(final_df['hadm_id'] == 196673)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T02:46:13.523738Z",
     "start_time": "2019-11-17T02:46:13.263615Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = {\n",
    "  'hadm_id': [140694, 140694, 140694, 171544, 171544, 171544, 171544, 171544,],\n",
    "  'charttime': [pd.to_datetime('2121-08-12 19:00:00'), pd.to_datetime('2121-08-12 19:45:00'), pd.to_datetime('2121-08-12 20:00:00'), pd.to_datetime('2153-09-06 14:11:00'), pd.to_datetime('2153-09-06 17:30:00'), pd.to_datetime('2153-09-06 17:35:00'), pd.to_datetime('2153-09-06 17:40:00'), pd.to_datetime('2153-09-06 17:45:00')],\n",
    "  'hr': [67.0, 68.0, 70.0, 80.0, 80.0, 80.0, 76.0, 79.0],\n",
    "  'sbp': [102.0, 135.0, 153.0, 114.0, 114.0, 114.0, 115.0, 117.0],\n",
    "  'dbp': [75.0, 68.0, 94.0, 50.0, 50.0, 50.0, 51.0, 53.0],\n",
    "}\n",
    "\n",
    "dn = {\n",
    "  'hadm_id': [140694, 140694, 140694, 171544, 171544, 171544, 171544, 171544,],\n",
    "  'charttime': [pd.to_datetime('2121-08-10 20:32:00'), pd.to_datetime('2121-08-11 12:57:00'), pd.to_datetime('2121-08-11 15:18:00'), pd.to_datetime('2153-09-05 15:09:00'), pd.to_datetime('2153-09-05 17:43:00'), pd.to_datetime('2153-09-06 10:36:00'), pd.to_datetime('2153-09-06 15:55:00'), pd.to_datetime('2153-09-06 17:12:00')],\n",
    "  'note': ['some text1', 'some text2', 'some text3', 'some text4', 'some text5', 'some text6', 'some text7', 'some text8']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T02:46:14.655376Z",
     "start_time": "2019-11-17T02:46:14.609790Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = pd.DataFrame(ds)\n",
    "dn = pd.DataFrame(dn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T02:46:15.643380Z",
     "start_time": "2019-11-17T02:46:15.591571Z"
    }
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T02:46:21.822160Z",
     "start_time": "2019-11-17T02:46:21.773524Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T00:53:54.257448Z",
     "start_time": "2019-11-17T00:53:54.187321Z"
    }
   },
   "outputs": [],
   "source": [
    "final = {\n",
    "  'hadm_id': [140694, 140694, 140694, 140694, 140694, 140694, 171544, 171544, 171544, 171544, 171544, 171544, 171544, 171544, 171544, 171544],\n",
    "  'charttime': [pd.to_datetime('2121-08-10 20:32:00'), pd.to_datetime('2121-08-11 12:57:00'), pd.to_datetime('2121-08-11 15:18:00'), pd.to_datetime('2121-08-12 19:00:00'), pd.to_datetime('2121-08-12 19:45:00'), pd.to_datetime('2121-08-12 20:00:00'), pd.to_datetime('2153-09-05 15:09:00'), pd.to_datetime('2153-09-05 17:43:00'), pd.to_datetime('2153-09-06 10:36:00'), pd.to_datetime('2153-09-06 14:11:00'), pd.to_datetime('2153-09-06 15:55:00'), pd.to_datetime('2153-09-06 17:12:00'), pd.to_datetime('2153-09-06 17:30:00'), pd.to_datetime('2153-09-06 17:35:00'), pd.to_datetime('2153-09-06 17:40:00'), pd.to_datetime('2153-09-06 17:45:00')],\n",
    "  'ce_charttime': [pd.NaT, pd.NaT, pd.NaT, pd.to_datetime('2121-08-12 19:00:00'), pd.to_datetime('2121-08-12 19:45:00'), pd.to_datetime('2121-08-12 20:00:00'), pd.NaT, pd.NaT, pd.NaT, pd.to_datetime('2153-09-06 14:11:00'), pd.NaT, pd.NaT, pd.to_datetime('2153-09-06 17:30:00'), pd.to_datetime('2153-09-06 17:35:00'), pd.to_datetime('2153-09-06 17:40:00'), pd.to_datetime('2153-09-06 17:45:00')],\n",
    "  'hr': [np.nan, np.nan, np.nan, 67.0, 68.0, 70.0, np.nan, np.nan, np.nan, 80.0, np.nan, np.nan, 80.0, 80.0, 76.0, 76.0],\n",
    "  'sbp': [np.nan, np.nan, np.nan, 102.0, 135.0, 153.0, np.nan, np.nan, np.nan, 114.0, np.nan, np.nan, 114.0, 114.0, 115.0, 117.0],\n",
    "  'dbp': [np.nan, np.nan, np.nan, 75.0, 68.0, 94.0, np.nan, np.nan, np.nan, 50.0, np.nan, np.nan, 50.0, 50.0, 51.0, 53.0],\n",
    "  'ne_charttime': [pd.to_datetime('2121-08-10 20:32:00'), pd.to_datetime('2121-08-11 12:57:00'), pd.to_datetime('2121-08-11 15:18:00'), pd.NaT, pd.NaT, pd.NaT, pd.to_datetime('2153-09-05 15:09:00'), pd.to_datetime('2153-09-05 17:43:00'), pd.to_datetime('2153-09-06 10:36:00'), pd.NaT, pd.to_datetime('2153-09-06 15:55:00'), pd.to_datetime('2153-09-06 17:12:00'), pd.NaT, pd.NaT, pd.NaT, pd.NaT],\n",
    "  'note': ['some text1', 'some text2', 'some text3', np.nan, np.nan, np.nan, 'some text4', 'some text5', 'some text6', np.nan, 'some text7', 'some text8', np.nan, np.nan, np.nan, np.nan] \n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
